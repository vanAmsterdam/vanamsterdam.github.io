{
  "hash": "e51bcab737239b2c4407e22c850af746",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: The need for speed, performing simulation studies in R, JAX and Julia\nbibliography: references.bib\neval: false\ndraft: false\ndate: 2024-03-08\ncategories:\n- r\n- julia\n- jax\n- python\n- simulation studies\n---\n\n\nSimulation experiments are important when evaluating methods but also for applied work in for example power analyses [e.g. @vanamsterdamAssociationMuscleQuantity2022] or sensitivity analyses [e.g. @vanamsterdamIndividualTreatmentEffect2022].\nWhen using simulations to support scientific claims, the more experiments the better.\nBeing able to perform simulation experiments faster allows researchers to:\n\n- test bigger (or finer) experimental grids\n- attain lower variance by having more repeated experiments\n- test new ideas faster\n\nThe R language has been a popular language among many biostatisticians for a long time, but it is not generally considered the top performing language in terms of speed.\n<!--, especially when it comes to leveraging GPUs for calculations that are amenable to vectorization (such as performing simulation studies across a large grid).-->\nIn recent years, [JAX](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html) (developed by Google) and [Julia](https://julialang.org) have arisen as general scientific computation frameworks.\nJAX and Julia have grown in popularity both in the neural network community as in other scientific communities [e.g. @DifferentiableUniverseInitiativeJax_cosmo2024; @SciMLOpenSource].\nIn this blog post I'll compare R with JAX and Julia for a simple simulation study setup with logistic regression.\n\nWe'll look at the following comparisons:\n\n1. R single thread\n2. R multi-threaded\n3. JAX\n4. Julia single thread\n5. Julia multi-threading\n\n## JAX and Julia vs R: a high level overview\nJAX is a rising star in computer science and natural sciences.\nWithout going in too much details, JAX works by translating python code into an intermediate language that can be run very efficiently on different hardware backends (CPU, GPU, TPU), possibly with just-in-time compilation (JIT).\nJAX prides itself on providing composable transformations for vectorization (`vmap`), paralellization (`pmap`) and automatic differentiation (`grad`), all compatible with `jit`.\nIn R, most of the heavy lifting in terms of computation (such as fitting a logistic regression model) is implemented in high-speed languages such as C++ or Fortran.\nThe usual R-code merely provides an interface to these languages and allows the user to feed in data and analyze results.\nWhereas using JAX and R means working with two languages (one language to write accessible code, another to do fast computation), Julia is a just-in-time compiled language where such translation is not needed.\n\n## The basic setup\n\nWe'll use a simple logistic regression simulation setup, where for each observation:\n\n$$\n\\begin{align}\n\\mathbf{x}_{\\text{full}} &\\sim \\mathcal{N}(0,I) \\in \\mathbb{R}^{10} \\\\\ny &= ||\\mathbf{x}||_0 > 0 \\\\\n\\mathbf{x}_{\\text{obs}} &= [x_0\\ x_i \\ldots x_9]\n\\end{align}\n$$\n\nSo $y$ is the sum of elements of $\\mathbf{x}_{\\text{full}}$ and the observed $\\mathbf{x}_{\\text{obs}}$ contains only the first 9 out of 10 elements of $\\mathbf{x}_{\\text{full}}$.\n\nWe will model this data with logistic regression:\n\n$$\n\\begin{align}\n    \\text{logit}(y) &= \\mathbf{x}_{\\text{obs}} \\boldsymbol{\\beta}'\\\\\n    y &\\sim \\text{Bernoulli} (\\sigma (\\mathbf{x}_{\\text{obs}} \\boldsymbol{\\beta}'))\n\\end{align}\n$$\n\nwhere $\\boldsymbol{\\beta} = [\\beta_1,\\ldots,\\beta_9]$ is a 9-dimensional parameter vector that is to be estimated (we're excluding the usual intercept term).\nWe'll generate `nrep` independent datasets and estimate $\\boldsymbol{\\beta}$ in each one, and finally calculate the average parameter estimates $\\frac{1}{\\text{nrep}}\\sum_{i=1}^{\\text{nrep}}\\boldsymbol{\\beta}^i$.\n\n### Hardware\n\nThe hardware I had available for this comparison is:\n\n- macm1: 2020 macbook air M1, 8Gb RAM, 8 threads\n- linux: linux machine, 64Gb RAM, 12 threads (Intel(R) Xeon(R) W-2135 CPU @ 3.70GH )\n\n\n## The code\n\n### Making data\n\nMaking the data is pretty similar in all cases, except that JAX requires an explicit random key.\n\n::: {.panel-tabset}\n## R\n\n``` {.r}\nmake_data <- function(n=1e3L) {\n    x_vec = rnorm(n*10)\n    X_full = matrix(x_vec, ncol=10)\n    eta = rowSums(X_full)\n    y = eta > 0\n    # return only first 9 column to have some noise\n    X = X_full[,1:9]\n    return(list(X=X,y=y))\n}\n```\n\n## Python\n\n``` {.python}\ndef make_data(k, n=int(1e3)):\n    X_full = random.normal(k, (n,10)) # JAX needs explicit keys for psuedo random number generation\n    eta = jnp.sum(X_full, axis=-1)\n    y = eta > 0\n    # return only first 9 column to have some noise\n    X = X_full[:,:9]\n    return (X, y)\n```\n\n## Julia\n\n``` {.julia}\nfunction make_data(n::Integer=1000)\n    X_full = randn(n,10)\n    eta = vec(sum(X_full, dims=2))\n    y = eta .> 0 # vectorized greater than 0 comparison\n    X = X_full[:,1:9]\n    return X, y\nend\n```\n\n:::\n\n### Run single experiment\n\nNow we'll write the code for a single analysis step, generating data and fitting the logistic regression.\nFor R and Julia we will use the `glm` function to estimate the logistic regression model.\nThe Julia code looks much like the R code.\nAs far as I know there is no equivalent `glm` function implemented in JAX.\nInstead, we need to specify an objective function and will use a general purpose optimizer.\n[JaxOpt](https://jaxopt.github.io/) provides both `binary_logreg` as an objective function and `LBFGS`, a popular general purpose optimizer, which we'll use here.\n\n::: {.panel-tabset}\n## R\n\n``` {.r}\nsolve <- function(...) {\n  data = make_data()\n  fit = glm(data$y~data$X-1, family='binomial')\n  coefs = coef(fit)\n  return(coefs)\n}\n```\n\n## Python\n\n``` {.python}\n# initialize a generic solver with the correct objective function\nsolver = LBFGS(binary_logreg)\nw_init = jnp.zeros((9,))\n\n@jit # jit toggles just-in-time compilation, one of the main features of JAX\ndef solve(k):\n    data = make_data(k)\n    param, state = solver.run(w_init, data)\n    return param\n```\n\n## Julia\n\n``` {.julia}\nfunction solve(i::Int64=1)\n    X, y = make_data()\n    fit = glm(X, y, Bernoulli())\n    coefs = coef(fit)\n    return coefs\nend\n```\n\n:::\n\n### Iterate over runs / settings\n\nFinally we run the experiments `nrep` times and calculate the average coefficient vector.\n\n#### JAX primitive: map versus vmap\nNote that in JAX there are multiple ways to do this, most notably `map` and `vmap`.\nWhereas `map` may offer speedups compared to R due to jit-compiliation, for most purposes `vmap` [is recommended](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.map.html) as it allows JAX to find ways of making the computation more efficient.\nFor example, a vector-vector multiplication *vectorized* over an input of vectors is equivalent to a single matrix-vector multiplication.\nJAX's intermediate language finds these possible optimizations and swaps in the more efficient approach.\nVectorized code runs in parallel and can be much faster.\nNote that in our case, vectorization may not be too beneficial as running `LBFGS` on different datasets may not lend itself to vectorizations (compared e.g. to neural network computations on batches of data).\nA downside of vectorization is that it requires more memory: all the datasets and optimization steps happen in parallel, whereas with loop-based execution, only the coefficients of each time step need to be stored.\n\n::: {.panel-tabset}\n## R\n\n``` {.r}\nif (nthreads == 1) {\n    set.seed(240316)\n    params <- lapply(1:nreps, solve)\n} else {\n    params <- future_map(1:nreps, solve, .options=furrr_options(seed=240316))\n}\noutmat <- do.call(rbind, params)\nmeans <- colMeans(outmat)\nprint(means[1])\n```\n\n## Python\n\n``` {.python}\nk0 = random.PRNGKey(240316)\nks = random.split(k0, args.nreps)\nif args.primitive == 'map':\n    params = lax.map(solve, ks)\nelif args.primitive == 'vmap':\n    params = vmap(solve)(ks)\nelse:\n    raise ValueError(f\"unrecognized primitive: {args.primitive}, choose map or vmap\")\n\nmeans = jnp.mean(params, axis=0)\nprint(means[0])\n```\n\n## Julia\n\n``` {.julia}\nRandom.seed!(240316)\noutmat = zeros(nreps, 9)\n\n@threads for i in 1:nreps # use @threads for multi-threading\n    solution = solve()\n    outmat[i,:] = solution\nend\n\nmeans = mean(outmat, dims=1)\nprint(means[1])\n```\n\n:::\n\n### Bash scripts for speed comparisons\n\nI benchmarked each run with an external `time` command in Bash or ZSH and wrote the results to a file.\n\n::: {.panel-tabset}\n## Bash (linux)\n\n\n::: {.cell}\n\n```{.bash .cell-code  code-fold=\"true\"}\n#!/bin/bash\n\nfor nreps in 1000 10000 100000 1000000 10000000\ndo\n    echo $nreps\n    { time python scripts/jaxspeed.py $nreps map; } 2>&1 | grep real >> bashtimings.txt\n    sed -i '$s/$/ jax '\"${nreps}\"' nreps 12 nthreads map primitive/' bashtimings.txt\n    { time python scripts/jaxspeed.py $nreps vmap; } 2>&1 | grep real >> bashtimings.txt\n    sed -i '$s/$/ jax '\"${nreps}\"' nreps 12 nthreads vmap primitive/' bashtimings.txt\n\n    for nthreads in 1 6 12\n    do\n\t    echo $nthreads\n\t    { time julia -t $nthreads scripts/jlspeed.jl $nreps ; } 2>&1 | grep real >> bashtimings.txt\n\t    sed -i '$s/$/ julia '\"${nreps}\"' nreps '\"${nthreads}\"' nthreads/' bashtimings.txt\n\t    { time Rscript scripts/rspeed.R $nreps $nthreads ; } 2>&1 | grep real >> bashtimings.txt\n\t    sed -i '$s/$/ r '\"${nreps}\"' nreps '\"${nthreads}\"' nthreads/' bashtimings.txt\n    done\ndone\n\n```\n:::\n\n\n## ZSH (macos)\n\n\n::: {.cell}\n\n```{.bash .cell-code  code-fold=\"true\"}\n#!/bin/zsh\n\nfor nreps in 1000 10000 100000 1000000 10000000\ndo\n    echo $nreps\n    { time python scripts/jaxspeed.py $nreps map ; } 2>> timings.txt\n    sed -i '' '$s/$/ '\"${nreps}\"' nreps 8 threads map primitive/' timings.txt\n    { time python scripts/jaxspeed.py $nreps vmap ; } 2>> timings.txt\n    sed -i '' '$s/$/ '\"${nreps}\"' nreps 8 threads vmap primitive/' timings.txt\n\n    for nthreads in 1 8\n    do\n        echo $nthreads\n        { time julia -t $nthreads scripts/jlspeed.jl $nreps ; } 2>> timings.txt\n        sed -i '' '$s/$/ '\"${nreps}\"' nreps '\"${nthreads}\"' nthreads/' timings.txt\n        { time Rscript scripts/rspeed.R $nreps $nthreads ; } 2>> timings.txt\n        sed -i '' '$s/$/ '\"${nreps}\"' nreps '\"${nthreads}\"' nthreads/' timings.txt\n    done\ndone\n\n```\n:::\n\n:::\n\n## The speed\n\n### Running time\n\nFirst, let's see how running time increases with the number of experiments, using all available threads.\nYou cannot easily set number of threads in JAX (see e.g. [this](https://github.com/google/jax/issues/1539) issue on github), so all JAX computations use all threads.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsuppressMessages({\n    library(dplyr)\n    library(data.table)\n    library(purrr)\n    library(stringr)\n    library(ggplot2); theme_set(theme_bw())\n    library(knitr)\n    library(kableExtra)\n})\n\n# get timings from mac\nlns <- readr::read_lines('timings.txt')\n# get timings from linux machine\nblns <- readr::read_lines('bashtimings.txt')\n\n# remove lines with warnings / errors printed to txt file\nlns <- str_subset(lns, \"^Rscript|julia|python\")\nmtimings <- data.table(raw_string=lns)\nbtimings <- data.table(raw_string=blns)\n\n# remove white space and first word (for linux)\ntimings <- rbindlist(list(macm1=mtimings, linux=btimings), idcol='machine')\ntimings[, string:=str_trim(raw_string)] # remove white space\ntimings[, string:=str_replace(string, \"^real\\t\", \"\")] # remove first word linux\n\n# find the language from the string\ntimings[machine=='macm1', command:=word(string)]\ntimings[command=='Rscript', language:='r']\ntimings[command=='python', language:='jax']\ntimings[command=='julia', language:='julia']\ntimings[machine=='linux', language:=str_extract(string, \"(?<=s )[a-z]+\")]\n\n# grab number of threads and reps\ntimings[, nthreads:=as.integer(str_extract(string, \"(\\\\d+)(?= nthreads)\"))]\ntimings[, max_threads:=max(nthreads, na.rm=T), by='machine']\ntimings[is.na(nthreads) & language == 'jax', nthreads:=max_threads]\ntimings[is.na(nthreads) & language %in% c('r', 'julia'), nthreads:=1L]\ntimings[, nreps:=as.integer(str_extract(string, \"(\\\\d+)(?= nreps)\"))]\ntimings[, max_threads:=max(nthreads), by='machine']\n\n# find jax primitive\ntimings[, primitive:=str_extract(string, \"(\\\\w+)(?= primitive)\")]\ntimings[machine=='macm1'&language=='jax'&is.na(primitive), primitive:='map']\n#timings <- timings[!(language=='jax' & primitive!='map')]\n# timings[language=='jax', language:=paste0(language, '-', primitive)]\ntimings[language!='jax', primitive:='map']\ntimings <- timings[!str_ends(primitive, 'nojit')]\n\n# grab the time \ntimings[machine=='macm1', time_str:=str_extract(string, \"(?<=cpu\\\\ )(.*)(?= total)\")]\ntimings[, milliseconds:=as.integer(str_extract(time_str, \"(\\\\d+)$\"))]\ntimings[, seconds     :=as.integer(str_extract(time_str, \"(\\\\d+)(?=.)\"))]\ntimings[, minutes     :=as.integer(str_extract(time_str, \"(\\\\d+)(?=:)\"))]\ntimings[, hours       :=as.integer(str_extract(time_str, \"(\\\\d+)(?=:(\\\\d+:))\"))]\ntimings[machine=='linux', minutes:=as.integer(str_extract(string, \"^(\\\\d+)\"))]\ntimings[machine=='linux', seconds:=as.integer(str_extract(string, \"(?<=m)(\\\\d+)\"))]\ntimings[machine=='linux', milliseconds:=as.integer(str_extract(string, \"(\\\\d+)(?=s)\"))]\ntimings[is.na(minutes), minutes:=0]\ntimings[is.na(hours), hours:=0]\n\n\ntimings[, sec_total:=60*60*hours + 60*minutes + seconds + milliseconds / 1000]\ntimings[, min_total:=sec_total / 60]\n\n# add some vars\ntimings[, n_per_min:=nreps / min_total]\ntimings[, n_per_min3:=n_per_min/1000]\n\n# remove a couple of failed runs where time went down for more experiments (= out of memory)\n#timings <- timings[n_per_min < 1.5e6]\n\n\nfwrite(timings, 'allresults.csv', row.names=F)\n\nggplot(timings[nthreads==max_threads], aes(x=nreps, y=min_total, col=language)) +\n  geom_point() + geom_line(aes(linetype=primitive)) + \n  scale_x_log10() + scale_y_log10() + \n  # facet_grid(machine+nthreads~primitive, labeller='label_both')\n  facet_grid(machine+nthreads~., labeller='label_both')\n```\n\n::: {.cell-output-display}\n![Time to run experiments on the maximum number of threads](index_files/figure-html/fig-times-1.png){#fig-times width=672}\n:::\n:::\n\n\nNote that for JAX `vmap` the clock time actually goes *down* when the number of experiment increases.\nThis is not some magic speedup but the machine running out of memory and thus not completing the experiment, a downside of vectorization.\nWe'll exclude these runs of the further comparisons.\nFor `map` this is not the case.\n\n\n::: {.cell}\n\n:::\n\n\n### Speed\n\nLet's look at the speeds.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Speed: number of repetitions per minute versus of number of experiments](index_files/figure-html/fig-speeds-1.png){#fig-speeds width=672}\n:::\n:::\n\n\nWhy is the speed going down for Julia on the macm1 machine after 1e6 experiments? \nTurns out there is not enough RAM to fit the experiments and the system switches to swap memory which is much slower than using RAM (even on a mac arm64).\nThe speed of R stopped increasing after 1e6 experiments so I didn't run more experiments.\n\n### Threads vs Speed\n\nNow let's check how much extra speed we get from using more threads in R and Julia.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scaling of speed with number of threads](index_files/figure-html/fig-threads-1.png){#fig-threads width=672}\n:::\n:::\n\n::: {.cell tbl-cap='Scaling of speed with number of threads, number of experiments per minute (1000s)'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> language </th>\n   <th style=\"text-align:right;\"> nreps </th>\n   <th style=\"text-align:right;\"> linux_1 </th>\n   <th style=\"text-align:right;\"> linux_6 </th>\n   <th style=\"text-align:right;\"> speedup6 </th>\n   <th style=\"text-align:right;\"> linux_12 </th>\n   <th style=\"text-align:right;\"> speedup12 </th>\n   <th style=\"text-align:right;\"> macm1_1 </th>\n   <th style=\"text-align:right;\"> macm1_8 </th>\n   <th style=\"text-align:right;\"> speedup8 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> julia </td>\n   <td style=\"text-align:right;\"> 1e+03 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 10.7 </td>\n   <td style=\"text-align:right;\"> 11.8 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 1.1 </td>\n   <td style=\"text-align:right;\"> 11.0 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 1.0 </td>\n   <td style=\"text-align:right;border-left:1px solid;border-right:1px solid;\"> 16.2 </td>\n   <td style=\"text-align:right;\"> 16.3 </td>\n   <td style=\"text-align:right;\"> 1.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> julia </td>\n   <td style=\"text-align:right;\"> 1e+04 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 69.5 </td>\n   <td style=\"text-align:right;\"> 108.1 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 1.6 </td>\n   <td style=\"text-align:right;\"> 101.0 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 1.5 </td>\n   <td style=\"text-align:right;border-left:1px solid;border-right:1px solid;\"> 96.9 </td>\n   <td style=\"text-align:right;\"> 147.6 </td>\n   <td style=\"text-align:right;\"> 1.5 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> julia </td>\n   <td style=\"text-align:right;\"> 1e+05 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 146.0 </td>\n   <td style=\"text-align:right;\"> 514.8 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 3.5 </td>\n   <td style=\"text-align:right;\"> 567.1 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 3.9 </td>\n   <td style=\"text-align:right;border-left:1px solid;border-right:1px solid;\"> 151.3 </td>\n   <td style=\"text-align:right;\"> 494.4 </td>\n   <td style=\"text-align:right;\"> 3.3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> julia </td>\n   <td style=\"text-align:right;\"> 1e+06 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 162.8 </td>\n   <td style=\"text-align:right;\"> 857.3 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 5.3 </td>\n   <td style=\"text-align:right;\"> 1001.3 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 6.2 </td>\n   <td style=\"text-align:right;border-left:1px solid;border-right:1px solid;\"> 163.9 </td>\n   <td style=\"text-align:right;\"> 983.6 </td>\n   <td style=\"text-align:right;\"> 6.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> julia </td>\n   <td style=\"text-align:right;\"> 1e+07 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 168.6 </td>\n   <td style=\"text-align:right;\"> 875.7 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 5.2 </td>\n   <td style=\"text-align:right;\"> 1059.3 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 6.3 </td>\n   <td style=\"text-align:right;border-left:1px solid;border-right:1px solid;\"> 163.9 </td>\n   <td style=\"text-align:right;\"> 245.9 </td>\n   <td style=\"text-align:right;\"> 1.5 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> r </td>\n   <td style=\"text-align:right;\"> 1e+03 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 14.8 </td>\n   <td style=\"text-align:right;\"> 14.5 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 1.0 </td>\n   <td style=\"text-align:right;\"> 9.2 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 0.6 </td>\n   <td style=\"text-align:right;border-left:1px solid;border-right:1px solid;\"> 20.8 </td>\n   <td style=\"text-align:right;\"> 23.7 </td>\n   <td style=\"text-align:right;\"> 1.1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> r </td>\n   <td style=\"text-align:right;\"> 1e+04 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 15.3 </td>\n   <td style=\"text-align:right;\"> 53.0 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 3.5 </td>\n   <td style=\"text-align:right;\"> 52.4 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 3.4 </td>\n   <td style=\"text-align:right;border-left:1px solid;border-right:1px solid;\"> 22.5 </td>\n   <td style=\"text-align:right;\"> 72.7 </td>\n   <td style=\"text-align:right;\"> 3.2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> r </td>\n   <td style=\"text-align:right;\"> 1e+05 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 14.6 </td>\n   <td style=\"text-align:right;\"> 74.1 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 5.1 </td>\n   <td style=\"text-align:right;\"> 89.8 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 6.2 </td>\n   <td style=\"text-align:right;border-left:1px solid;border-right:1px solid;\"> 24.6 </td>\n   <td style=\"text-align:right;\"> 98.2 </td>\n   <td style=\"text-align:right;\"> 4.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> r </td>\n   <td style=\"text-align:right;\"> 1e+06 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 13.9 </td>\n   <td style=\"text-align:right;\"> 70.8 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 5.1 </td>\n   <td style=\"text-align:right;\"> 91.0 </td>\n   <td style=\"text-align:right;border-right:1px solid;\"> 6.6 </td>\n   <td style=\"text-align:right;border-left:1px solid;border-right:1px solid;\"> 20.9 </td>\n   <td style=\"text-align:right;\"> 82.0 </td>\n   <td style=\"text-align:right;\"> 3.9 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nSpeed increases with increasing number of threads, though not with a simple linear scaling in the number of threads.\nThe speed increase is similar for R and Julia.\n\n\n### Top speeds per language\n\nLet's see the top speeds per language, also compered to the top R speed on that machine.\n\n\n::: {#tbl-best-per-language .cell tbl-cap='Best speeds per language and machine'}\n::: {.cell-output-display}\n\n\n| n experiments| n threads| running time (minutes)| experiments per minute (x1000)| speed-up vs best R| language| primitive| machine|\n|-------------:|---------:|----------------------:|------------------------------:|------------------:|--------:|---------:|-------:|\n|         1e+05|        12|                    0.6|                          161.9|                1.8|      jax|      vmap|   linux|\n|         1e+07|        12|                    9.4|                         1059.3|               11.6|    julia|       map|   linux|\n|         1e+06|        12|                   11.0|                           91.0|                1.0|        r|       map|   linux|\n|         1e+06|         8|                    6.1|                          163.9|                1.7|      jax|       map|   macm1|\n|         1e+06|         8|                    1.0|                          983.6|               10.0|    julia|       map|   macm1|\n|         1e+05|         8|                    1.0|                           98.2|                1.0|        r|       map|   macm1|\n\n\n:::\n:::\n\n\n### Top speeds overall\n\nTop 10 speeds overall\n\n\n::: {#tbl-topspeeds .cell tbl-cap='top 10 speeds overall'}\n::: {.cell-output-display}\n\n\n| n experiments| n threads| running time (minutes)| experiments per minute (x1000)| speed-up vs best R| language| primitive| machine|\n|-------------:|---------:|----------------------:|------------------------------:|------------------:|--------:|---------:|-------:|\n|         1e+07|        12|                    9.4|                         1059.3|               11.6|    julia|       map|   linux|\n|         1e+06|        12|                    1.0|                         1001.3|               11.0|    julia|       map|   linux|\n|         1e+06|         8|                    1.0|                          983.6|               10.0|    julia|       map|   macm1|\n|         1e+07|         6|                   11.4|                          875.7|                9.6|    julia|       map|   linux|\n|         1e+06|         6|                    1.2|                          857.3|                9.4|    julia|       map|   linux|\n|         1e+05|        12|                    0.2|                          567.1|                6.2|    julia|       map|   linux|\n|         1e+05|         6|                    0.2|                          514.8|                5.7|    julia|       map|   linux|\n|         1e+05|         8|                    0.2|                          494.4|                5.0|    julia|       map|   macm1|\n|         1e+07|         8|                   40.7|                          245.9|                2.5|    julia|       map|   macm1|\n|         1e+07|         1|                   59.3|                          168.6|                1.9|    julia|       map|   linux|\n\n\n:::\n:::\n\n\nAll results are available in a csv file [here](allresults.csv)\n\n## Takeaways\n\n### Speed\n\nIn this setup, both on a Mac M1 and a Linux machine,\n\n1. Julia was 10-11 times faster than R\n2. JAX was 1.7 times faster than R\n\n### Code\n\n1. Julia code seems quite close to R code.\n2. In this simple example, the JAX code doesn't seem too dounting. However, JAX comes with some [sharp bits](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) and may be harder to program efficiently.[^1]\n   \n[^1]: One concrete example with my own previous simulation studies [@vanamsterdamConditionalAverageTreatment2023] is that I used first used `jax.numpy` arrays for different parameters and then a `scipy` function to create all combinations of these parameters. Creating this grid of parameters this way forced copying of `jax.numpy` arrays from the GPU back to CPU and then copying the grid back to GPU. This made the entire process orders of magnitude slower (it was a large grid O(1e12)). Gotchas like these can bite you. Also, JAX relies on pure functions that cannot depend on global variables.\n\n### Caveats\n\n1. JAX needs to recompile when the size of the data changes. When running experiments with e.g. different sizes of data, JAX will become slower because it needs to recompile, or you'll need to find other solutions like padding smaller data with dummy data and giving these dummy data 0 weights in the objective functions.\n2. I didn't have a CUDA-enabled GPU machine for this comparison, `vmap` may be come (much) more performant on a GPU\n3. JAX gives bare bones results. If you want to do e.g. significance testing of coefficients, or model comparisons, you will need to find implementations for this or implement this yourself. R and Julia (specifically the [GLM package](https://juliastats.org/GLM.jl/stable/) provide a much wider suite of methods\n4. In JAX I used general purpose optimizer. There may be more efficient ways of estimating a logistic regression model, whose optimizations are implemented in R and Julia but not JAX. In this sense it may not be a *fair* comparison, though these optimizations would need to be sought or implemented in JAX.\n5. JAX has *autograd*. When writing custom objective functions JAX can automatically calculate gradients and hessians, making it possible to use general purpose first or second order optimizers [e.g. @vanamsterdamConditionalAverageTreatment2023].\n\n### Extensions\n\n#### Optimizing memory usage\n\nSince in this case we only need the avarage of the coefficients we need not store all intermediate results. All languages may beccome much more efficient if we can program this in.\n\n- The [julia domumentation states](https://docs.julialang.org/en/v1/manual/multi-threading/#Atomic-Operations) that certain operations to *atomic* data structures can be done in a safe-way while multithreading. Instead of returning all coefficients of all datasets, we could calculate the average value of the coefficients (and e.g. the avareage of the squares of the values) with less memory overhead by:\n    1. instantiate an atomic vector of 9 coefficients\n    2. let every experiment (which may be in different threads) add its value to this shared atomic vector with `atomic_add!`\n    3. at the end, calculate the mean by dividing by `nreps`.\n- R functions can also overwrite global variables, but a question is whether this can be done in a multi-threading safe way\n- In JAX we may use `scan` to keep track of a running sum of coefficients and then `vmap` a bunch of `scan` computations\n\nIn future posts I plan to dive in to dive in to these optimizations to squeeze more out of these languages.\n\n## Conclusion\n\n::: {.callout-tip}\n## What should you use for glm-like simulation studies?\n\nProbably, Julia\n:::\n\n## Session Info\n\n::: {.panel-tabset}\n## R\n\n``` {.r}\n#| echo: false\n#| eval: false\nlibrary(sessioninfo)\nsession_info(pkgs = \"attached\", to_file=\"_rsession.txt\")\n\n─ Session info ────────────────────────────────────────────────────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       macOS Sonoma 14.3\n system   aarch64, darwin23.0.0\n ui       RStudio\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Amsterdam\n date     2024-03-21\n rstudio  2023.12.1+402 Ocean Storm (desktop)\n pandoc   3.1.12.2 @ /opt/homebrew/bin/ (via rmarkdown)\n\n─ Packages ────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n data.table  * 1.14.8  2023-02-17 [1] CRAN (R 4.3.1)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.1)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.1)\n kableExtra  * 1.4.0   2024-01-24 [1] CRAN (R 4.3.2)\n knitr       * 1.43    2023-05-25 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.1)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.3.2)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.1)\n\n [1] /opt/homebrew/lib/R/4.3/site-library\n [2] /opt/homebrew/Cellar/r/4.3.2/lib/R/library\n\n───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n```\n\n\n\n## Python\n\n``` {.python}\njax==0.4.25\njaxlib==0.4.25\njaxopt==0.8.3\nml-dtypes==0.3.2\nnumpy==1.26.4\nopt-einsum==3.3.0\nscipy==1.12.0\n\n```\n\n## Julia\n\n``` {.julia}\nname = \"jlspeed\"\nuuid = \"a8cd5241-1987-4548-90e5-ac0f39e812f2\"\nauthors = [\"Wouter van Amsterdam\"]\nversion = \"0.1.0\"\n\n[deps]\nArgParse = \"c7e460c6-2fb9-53a9-8c5b-16f535851c63\"\nGLM = \"38e38edf-8417-5370-95a0-9cbb8c7f171a\"\nRandom = \"9a3f8284-a2c9-5f02-9a11-845980a1fd5c\"\nStatsBase = \"2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91\"\n\n```\n:::\n\n## Full scripts\n\n::: {.panel-tabset}\n## R\n\n``` {.r}\n# rspeed\nargs = commandArgs(trailingOnly = T)\nif (length(args) == 0) {\n  nreps = 100\n  nthreads = 1\n} else if (length(args) == 1) {\n  nreps = as.integer(args[1])\n  nthreads = 1\n} else {\n  nreps = as.integer(args[1])\n  nthreads = as.integer(args[2])\n  suppressMessages(library(furrr))\n  plan(multisession, workers=nthreads)\n}\n\nmake_data <- function(n=1e3L) {\n    x_vec = rnorm(n*10)\n    X_full = matrix(x_vec, ncol=10)\n    eta = rowSums(X_full)\n    y = eta > 0\n    # return only first 9 column to have some noise\n    X = X_full[,1:9]\n    return(list(X=X,y=y))\n}\n\nsolve <- function(...) {\n  data = make_data()\n  fit = glm(data$y~data$X-1, family='binomial')\n  coefs = coef(fit)\n  return(coefs)\n}\n\nif (nthreads == 1) {\n    set.seed(240316)\n    params <- lapply(1:nreps, solve)\n} else {\n    params <- future_map(1:nreps, solve, .options=furrr_options(seed=240316))\n}\noutmat <- do.call(rbind, params)\nmeans <- colMeans(outmat)\nprint(means[1])\n\n\n```\n\n## Python\n\nimport jax, jaxopt\njax.config.update('jax_platform_name', 'cpu') # make sure jax doesnt use a gpu if it's available\nfrom jax import numpy as jnp, random, vmap, jit, lax\nfrom jaxopt import LBFGS\nfrom jaxopt.objective import binary_logreg\nfrom argparse import ArgumentParser\n\nparser = ArgumentParser()\nparser.add_argument('nreps', nargs=\"?\", type=int, default=int(10))\nparser.add_argument('primitive', nargs=\"?\", type=str, default=\"vmap\")\n\ndef make_data(k, n=int(1e3)):\n    X_full = random.normal(k, (n,10)) # JAX needs explicit keys for psuedo random number generation\n    eta = jnp.sum(X_full, axis=-1)\n    y = eta > 0\n    # return only first 9 column to have some noise\n    X = X_full[:,:9]\n    return (X, y)\n\n# initialize a generic solver with the correct objective function\nsolver = LBFGS(binary_logreg)\n# need to specify parameter initialization values\nw_init = jnp.zeros((9,))\n\n@jit # jit toggles just-in-time compilation, one of the main features of JAX\ndef solve(k):\n    data = make_data(k)\n    param, state = solver.run(w_init, data)\n    return param\n\nif __name__ == '__main__':\n    args = parser.parse_args()\n    k0 = random.PRNGKey(240316)\n    ks = random.split(k0, args.nreps)\n    if args.primitive == 'map':\n        params = lax.map(solve, ks)\n    elif args.primitive == 'vmap':\n        params = vmap(solve)(ks)\n    else:\n        raise ValueError(f\"unrecognized primitive: {args.primitive}, choose map or vmap\")\n\n    means = jnp.mean(params, axis=0)\n    print(means[0])\n\n\n\n\n## Julia\n\n``` {.julia}\nusing Random, GLM, StatsBase, ArgParse\nimport Base.Threads.@threads\n\nfunction parse_cmdline()\n    parser = ArgParseSettings()\n\n    @add_arg_table parser begin\n        \"nreps\"\n          help = \"number of repetitions\"\n          required = false\n          arg_type = Int\n          default = 10\n    end\n\n    return parse_args(parser)\nend\n\nfunction make_data(n::Integer=1000)\n    X_full = randn(n,10)\n    eta = vec(sum(X_full, dims=2))\n    y = eta .> 0 # vectorized greater than 0 comparison\n    X = X_full[:,1:9]\n    return X, y\nend\n\nfunction solve(i::Int64=1)\n    X, y = make_data()\n    fit = glm(X, y, Bernoulli())\n    coefs = coef(fit)\n    return coefs\nend\n\nfunction main()\n    args = parse_cmdline()\n    nreps = get(args, \"nreps\", 10)\n\n    Random.seed!(240316)\n    outmat = zeros(nreps, 9)\n\n    @threads for i in 1:nreps # use @threads for multi-threading\n        solution = solve()\n        outmat[i,:] = solution\n    end\n\n    means = mean(outmat, dims=1)\n    print(means[1])\nend\n\nmain()\n\n```\n:::\n\n\n\n## References\n\n::: {#refs}\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}