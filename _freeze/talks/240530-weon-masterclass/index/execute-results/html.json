{
  "hash": "fbf22b82a739d2ac4d7f28bf4706b690",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Uses and pitfalls with AI for decision support - harmful self-fulfilling prophecies\"\nsubtitle: \"WEON masterclass - 2024\"\ndate: 2024-05-30\nformat:\n    revealjs:\n        toc: true\n        incremental: true\n        #theme: custom.scss\n        auto-stretch: true\n        center: true\n        fig-align: center\n        width: 1600\n        height: 900\n---\n\n\n# Uses of AI in health care\n\n## AI may have many uses in health care\n\nUse AI to make health care\n\n::: {.columns}\n\n:::: {.column width=\"50%}\n\neasier / more efficient\n\n- administration / documentation\n- translation\n\n::::\n\n:::: {.column width=\"50%}\n\nbetter\n\n- diagnosis (e.g. skin cancer from imaging)\n- prognosis (e.g. survival given medical image)\n- treatment effect (e.g. image biomarker)\n\n::::\n\n:::\n\n. . .\n\n:::{.callout-tip icon=\"false\"}\n\nWhereas treatment effect estimation is typically thought of as a *causal* task requiring *causal* approaches (e.g. randomized controllerd trials), prognosis models are often advertised for making treatment decisions.\n\n:::\n\n## The in-between: using prediction models for (medical) decision making\n\n![](figs/8q8oy7_meme_crossover.jpg){fig-align=\"center\"}\n\n## Using prediction models for decision making is often thought of as a good idea\n\nFor example:\n\n1. give chemotherapy to cancer patients with high predicted risk of recurrence\n2. give statins to patients with a high risk of a heart attack\n\n. . . \n\n::: {.callout-note icon=false}\n## TRIPOD+AI on prediction models [@collinsTRIPODAIStatement2024]\n\n“Their primary use is to support clinical decision making, such as ... **initiate treatment or lifestyle changes.**” \n\n:::\n\n---\n\n:::{.callout-warning}\n\n## This may lead to bad situations when:\n\n1. ignoring the treatments patients may have had during training / validation\n2. only considering measures of predictive accuracy as sufficient evidence for safe deployment\n3. predictive accuracy (AUC) may be measured pre- or post-deployment of the model\n\n:::\n\n# When accurate prediction models yield harmful self-fulfilling prophecies{#selffulfilling}\n\n---\n\n:::{.r-stack}\n\n![](figs/new_overview1a.png){.fragment height=18cm}\n\n![](figs/new_overview1b.png){.fragment height=18cm}\n\n![](figs/new_overview1c.png){.fragment height=18cm}\n\n![](figs/new_overview2a.png){.fragment height=18cm}\n\n![](figs/new_overview2b.png){.fragment height=18cm}\n\n![](figs/new_overview3a.png){.fragment height=18cm}\n\n![](figs/new_overview3b.png){.fragment height=18cm}\n\n:::\n\n\n\n## building models for decision support without regards for the historic treatment policy is a bad idea\n\n:::{.r-stack}\n\n![](figs/policy_changea1.png){.fragment width=\"100%\"}\n\n![](figs/policy_changea3.png){.fragment width=\"100%\"}\n\n![](figs/policy_changeax.png){.fragment width=\"100%\"}\n\n![](figs/policy_changeb2.png){.fragment width=\"100%\"}\n\n![](figs/policy_changebx.png){.fragment width=\"100%\"}\n\n:::\n\n--- \n\n:::{.callout-note}\nThe question is not \"is my model accurate before / after deployment\", but did deploying the model improve patient outcomes?\n:::\n\n\n## Treatment-naive risk models\n\n:::{.r-stack}\n\n![](figs/txnaive1.png)\n\n![](figs/txnaive2b.png){.fragment}\n\n:::\n\n\\begin{align}\n    E[Y|X] \\class{fragment}{= E[E_{t~\\sim \\pi_0(X)}[Y|X,t]]}\n\\end{align}\n\n\n## Is this obvious?\n\n::: {.callout-tip}\n\nIt may seem obvious that you should not ignore historical treatments in your prediction models, if you want to improve treatment decisions, but many of these models are published daily, and some guidelines even allow for implementing these models based on predictve performance only\n\n:::\n\n## Prediction modeling is very popular in medical research\n\n![](figs/predmodelsoverview.png){fig-align='center'}\n\n## Recommended validation and reporting practices do not protect against harm\n\nbecause they do not evaluate the policy change\n\n:::{.columns}\n\n::::{.column width=\"50%\"}\n\n![](figs/ajcc_title.png){fig-align=\"center\"}\n\n::::\n\n::::{.column width=\"50%\"}\n\n![](figs/tripod_ai.png){fig-align=\"center\"}\n\n::::\n\n:::\n\n\n\n## Bigger data does not protect against harmful risk models\n\n![](figs/biggerdata.png){fig-align=\"center\"}\n\n## More flexible models do not protect against harmful risk models\n\n![](figs/morelayers.png){fig-align=\"center\"}\n\n## Gap between prediction accuracy and value for decision making\n\n![](figs/mindthegap.png){fig-align=\"center\"}\n\n## {auto-animate=true}\n\n::: {style=\"margin-top: 200px; font-size: 3em; color: red;\"}\nWhat to do?\n:::\n\n## {auto-animate=true}\n\n::: {style=\"margin-top: 100px\"}\nWhat to do?\n:::\n\n1.  Evaluate policy change (cluster randomized controlled trial)\n2.  Build models that are likely to have value for decision making\n\n# Building and validating models for decision support\n\n## Deploying a model is an intervention that changes the way treatment decisions are made\n\n![](figs/policy_changebx.png){fig-align=\"center\"}\n\n## How do we learn about the effect of an intervention?\n\nWith causal inference!\n\n- for using a decision support model, the unit of intervention is usually *the doctor*\n- randomly assign *doctors* to have access to the model or not\n- measure differences in **treatment decisions** and **patient outcomes**\n- this called a cluster RCT\n- if using model improves outcomes, use that one\n\n. . . \n\n:::{.callout-tip icon=\"false\"}\n\n## Using cluster RCTs to evaluated models for decision making is not a new idea [@cooperEvaluationMachinelearningMethods1997]\n\n“As one possibility, suppose that a trial is performed in which clinicians are randomized either to have or not to have access to such a decision aid in making decisions about where to treat patients who present with pneumonia.” \n\n:::\n\n. . .\n\n:::{.callout-warning}\n## What we don't learn\nwas the model predicting anything sensible?\n:::\n\n## So build prediction models and trial them?\n\nNot a good idea\n\n- baking a cake without a recipe\n- hoping it turns into something nice\n- not pleasant to people that need to taste the experiment\n  - (i.e. patients may have side-effects / die)\n\n## Models that are likely to be valuable for decision making\n\n- prediction under hypothetical interventions (prediction-under-intervention) models predict expected outcomes under the *hypothetical intervention* of giving a certain treatment\n\n. . . \n\n::: {.callout-tip icon=false}\n## Hilden and Habbema on prognosis [@hildenPrognosisMedicineAnalysis1987]\n\"Prognosis cannot be divorced from contemplated medical action, nor from action to be taken by the patient in response to prognostication.” \n:::\n\n- whereas *treatment-naive* prediction models average out over the historic treatment policy, prediction-under-intervention allows the user to select a treatment option\n- prediction-under-intervention is not a new idea, but language and methods on causality have come a long way since [@hildenPrognosisMedicineAnalysis1987].\n\n## Estimand for prediction-under-intervention models\n\nWhat is the estimand?\n\n- prediction: $E[Y|X]$\n- treatment effect: $E[Y|\\text{do}(T=1)] - E[Y|\\text{do}(T=0)]$\n- prediction-under-intervention: $E[Y|\\text{do}(T=t),X]$\n\n--- \n\n:::{.columns}\n\n::::{.column width=\"50%\"}\n\nusing *treatment naive* prediction models for decision support\n\n![](figs/8q8oy7_meme_crossover.jpg){fig-align=\"center\"}\n\n::::\n\n::::{.column width=\"50%\"}\n\nprediction-under-intervention\n\n![](figs/peanutbutter_chocolatesprinkles.jpg){.fragment fig-align=\"center\"}\n\n::::\n:::\n\n## More on prediction-under-intervention models\n\n- ideally estimated from RCTs\n- alternatively can use observational data and causal inference methods\n- this approach relies on **strong assumptions** especially regarding confounding\n- but likely a better recipe than *treatment-naive* models\n\n## Take-aways\n\n- (mis)using AI prediction models for treatment decisions without causal thinking and evaluation is a bad idea\n- specifically, always think about\n  - what is the effect of using this model on treatment decisions?\n  - what is the effect of this policy change on patient outcomes?\n- Prediction and causal inference come together neatly by declaring $E[Y|\\text{do}(T=t),X]$ as the estimand\n- deploying models for decision support is an intervention and should be evaluated as such\n\n. . .\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n![](figs/qr_comment.png){height=8cm fig-align='center'}\n\nFrom algorithms to action: improving patient care requires causality [@amsterdamAlgorithmsActionImproving2024]\n:::\n\n::: {.column width=\"50%\"}\n![](figs/qr_selffulfilling.png){height=8cm fig-align='center'}\n\nWhen accurate prediction models yield harmful sel-fulfilling prophecies [@vanamsterdamWhenAccuratePrediction2024a]\n:::\n::::\n\n\n## References\n\n## Estimating prediction-under-intervention models\n\n- the estimand $E[Y|\\text{do}(T=t),X]$ is an interventional distribution\n- RCTs randomly sample from interventional distributions\n- prediction-under-intervention models may be estimated and evaluated in RCT data\n- however, RCTs are typically designed to estimate a single parameter\n- prediction models need more data\n- in comes causal inference from observational data?\n\n## Challenges with observational data\n\n- assumption of no unobserved confounding may be hard to justify\n- but there's more between heaven (RCT) and earth (confounder adjustment)\n  - proxy-variable methods\n  - constant relative treatment effect assumption\n  - diff-in-diff\n  - instrumental variable analysis (high variance estimates)\n  - front-door analysis\n\n## Proxy variables?\n\n:::::{.columns}\n\n::::{.column width=\"50%\"}\n\n:::{.r-stack}\n\n![](figs/proxy_dag_01.png){.fragment fig-align=\"center\"}\n\n![](figs/proxy_dag_02.png){.fragment fig-align=\"center\"}\n\n![](figs/proxy_dag_03.png){.fragment fig-align=\"center\"}\n\n![](figs/proxy_dag_04.png){.fragment fig-align=\"center\"}\n\n:::\n\n::::\n\n::::{.column width=\"50%\"}\n\n- problem: didn't observe confounder *fitness* so cannot do confounder adjustment\n- instead, leverage *assumptions* on confounder - proxy relationship (e.g. *monotonicity*)\n- effect may still be identifyable [@vanamsterdamIndividualTreatmentEffect2022]\n\n::::\n\n:::::\n\n## Constant relative treatment effect?{.smaller}\n\n\n::: {.cell}\n\n:::\n\n\n::::::{.columns}\n\n:::::{.column width=\"60%\"}\n\n:::{.r-stack}\n\n::::{.fragment}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n::::\n\n::::{.fragment}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n::::\n\n::::{.fragment}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n::::\n\n::::{.fragment}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n::::\n\n:::\n\n:::::\n\n:::::{.column width=\"40%\"}\n\n- Widely used paradigm (cardiovascular risk, chemotherapy in breast cancer, ...)\n- Untreated risk is a quantity of the interventional distribution (i.e. *causal*)\n- Current risk-models: mix of treated / untreated patients [@amsterdamAlgorithmsActionImproving2024],\n- or ungrounded methods [@candidodosreisUpdatedPREDICTBreast2017; @xuPredictionCardiovascularDisease2021a].\n- Need better `causal' methods [@vanamsterdamConditionalAverageTreatment2023]\n:::::\n\n::::::\n\n## Prediction-under-intervention approaches sound great\n\n- but come with their own assumptions and trade-offs\n- do sensitivity analysis\n- may not have treatment information\n- may be many decision time-points, hard to formulate estimand over long time-horizon\n\n## How to proceed?\n\n- build prediction-under-intervention model with best data + assumptions\n- test policy value in historical RCT data of competing policies (e.g. current practice vs policy by new model)\n  - for each patient in RCT, determine recommended treatment according to policy\n  - if actual (randomly allocated) treatment is concordant, keep the patient\n  - if not, drop observation\n  - calculate average outcomes in the subpopulation\n  - policy with highest average outcomes is best\n- then do a cluster RCT\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}