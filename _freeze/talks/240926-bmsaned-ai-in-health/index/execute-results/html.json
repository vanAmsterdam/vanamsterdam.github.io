{
  "hash": "213c4e63426679b805f25f5da059db9f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: AI in Health Care\nsubtitle: BMS-Aned seminar\ndate: 2024-09-26\nbibliography: ../../library.bib\nformat:\n    revealjs:\n        incremental: true\n        theme: ../custom.scss\n        auto-stretch: true\n        center: true\n        fig-align: center\n        width: 1600\n        height: 900\n        pdf-separate-fragments: true\n        embed-resources: false\n---\n\n\n# What is AI?\n\n## Definition\n\n:::{.callout-tip}\n\n## What is AI?\n\nArtificial Intelligence is the branch of computer science that focuses on creating systems capable of performing tasks that typically require human intelligence. [@russellArtificialIntelligenceModern2020]\n\n:::\n\n- These tasks include: learning, reasoning, problem-solving, perception, natural language understanding, and decision-making.\n- AI systems can be designed to operate autonomously, adapt to new inputs, and improve their performance over time.\n\n## Early Milestones\n- **1940s**: Concept of AI emerged with Alan Turing's work on computation and intelligence.\n- **1956**: The term \"Artificial Intelligence\" was coined at the Dartmouth Conference by John McCarthy.\n- **1960s-1970s**: Early AI programs focused on solving algebra, proving theorems, and playing games (e.g., Chess).\n\n## Key Developments\n- **1980s**: Introduction of expert systems (rule-based systems for decision making).\n- **1990s**: Machine learning began gaining traction, allowing AI systems to improve with experience.\n- **2010s**: Deep learning and neural networks revolutionized AI, enabling breakthroughs in areas like image recognition, natural language processing, and more.\n\n---\n\n:::{.r-stack}\n\n![](figs/ai-landscape-1-bare.png){.fragment .fade-in-then-out}\n\n![](figs/ai-landscape-2-rules.png){.fragment .fade-in-then-out}\n:::\n\n## Rule-based systems are AI\n\n-   rule: all cows are animals\n-   observation: this is a cow $\\to$ it is an animal\n-   applications:\n    - medication interaction checkers\n    - bedside patient monitors\n\n---\n\n:::{.r-stack}\n\n![](figs/ai-landscape-2-rules.png){.fragment .fade-in-then-out}\n\n![](figs/ai-landscape-3-ml.png){.fragment .fade-in-then-out}\n\n:::\n\n## What is Machine Learning?\n\n::: {.center .large}\nMachine Learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. Instead, they rely on patterns and inference from data.\n:::\n\n## ML tasks\n\n::: {.columns}\n:::: {.column width=\"50%\"}\n![](figs/gen_scatter.png){width=\"100%\" fig-align=\"center\"}\n::::\n:::: {.column width=\"50%\"}\ndata:\n\n|i |length|weight|sex|\n|-:|-----:|-----:|--:|\n|1|137|30|boy|\n|2|122|24|girl|\n|3|101|18|girl|\n|...|...|...|...|\n\n::: {.fragment}\n$$l_i,w_i,s_i \\sim p(l,w,s)$$\n:::\n\n::::\n:::\n\n## ML tasks: generation\n::: {.columns}\n:::: {.column width=\"50%\"}\n![](figs/gen_full.png){width=\"100%\" fig-align=\"center\"}\n::::\n:::: {.column width=\"50%\"}\nformulate a model for *joint* distribution $p_{\\theta}$\n\nuse samples to optimize $\\theta$\n$$\n  l_j,w_j,s_j \\sim p_{\\theta}(l,w,s)\n$$\n\n|task| |\n|---:|:---|\n|generation|$l_j,w_j,s_j \\sim p_{\\theta}(l,w,s)$|\n\n::::\n:::\n\n## ML tasks: conditional generation\n::: {.columns}\n:::: {.column width=\"50%\"}\n![](figs/gen_boy.png){width=\"100%\" fig-align=\"center\"}\n::::\n:::: {.column width=\"50%\"}\nuse samples to learn model for *conditional* distribution $p$\n$$\n  l_j,w_j \\sim p_{\\theta}(l,w|s=\\text{boy})\n$$\n\n|task| |\n|---:|:---|\n|generation|$l_j,w_j,s_j \\sim p_{\\theta}(l,w,s)$|\n|conditional generation|$l_j,w_j \\sim p_{\\theta}(l,w|s=\\text{boy})$|\n::::\n:::\n\n## ML tasks: conditional generation 2\n\n::: {.columns}\n:::: {.column width=\"50%\"}\n![](figs/gen_scatter.png){width=\"100%\" fig-align=\"center\"}\n::::\n:::: {.column width=\"50%\"}\nuse samples to learn model for *conditional* distribution $p$\nof one variable\n$$\ns_j \\sim p_{\\theta}(s|l=l',w=w')\n$$\n\n|task| |\n|---:|:---|\n|generation|$l_j,w_j,s_j \\sim p_{\\theta}(l,w,s)$|\n|conditional generation|$l_j,w_j \\sim p_{\\theta}(l,w|s=\\text{boy})$|\n::::\n:::\n\n## ML tasks: discrimination\n::: {.columns}\n:::: {.column width=\"50%\"}\n![](figs/class_logistic.png){width=\"100%\" fig-align=\"center\"}\n::::\n:::: {.column width=\"50%\"}\ncall this one variable *outcome* and *classify* when expected value[^1] passes threshold (e.g. 0.5)\n$$\ns_j = p_{\\theta}(s|l=l',w=w') > 0.5\n$$\n\n|task| |\n|---:|:---|\n|generation|$l_j,w_j,s_j \\sim p_{\\theta}(l,w,s)$|\n|conditional generation|$l_j,w_j \\sim p_{\\theta}(l,w|s=\\text{boy})$|\n|discrimination|$p_{\\theta}(s|l=l_i,w=w_i) > 0.5$|\n::::\n:::\n[^1]: either calculated directly or estimated by simulation\n\n## ML tasks: reinforcement learning\n\n- e.g. computers playing games\n- maybe not so useful for clinical research as requires many experiments\n\n![](figs/reinforcement_learning.png){width=\"80%\", fig-align=\"center\"}\n\n---\n\n:::{.r-stack}\n\n![](figs/ai-landscape-2-rules.png){.fragment .fade-in-then-out}\n\n![](figs/ai-landscape-3-ml.png){.fragment .fade-in-then-out}\n\n![](figs/ai-landscape-4-rf-svm.png){.fragment .fade-in-then-out}\n\n![](figs/ai-landscape-5-dl.png){.fragment .fade-in-then-out}\n\n![](figs/ai-landscape-6-llm.png){.fragment .fade-in}\n\n:::\n\n## Neural Networks and Deep Learning\n\n### From Linear Regression to Deep Learning\n\n:::{.format-ncol=2}\n\n![](tikzs/linear-regression.png){width=\"100%\"}\n\n$$\\hat{y} = \\beta_0 + \\beta_1 x$$\n\n\n:::\n\n### Slide 1: Linear Regression\n\nLinear regression is a simple model that predicts a continuous outcome variable \\( y \\) based on one or more predictor variables \\( x \\).\n\n$$\ny = \\beta_0 + \\beta_1 x + \\epsilon\n$$\n\n- **Weights**: \\( \\beta_0 \\) (intercept), \\( \\beta_1 \\) (slope)\n- **Objective**: Minimize the sum of squared errors (SSE)\n\n\n```{tikz}\n\\begin{axis}[\n    axis lines = left,\n    xlabel = \\( x \\),\n    ylabel = \\( y \\),\n]\n\\addplot [\n    domain=0:10, \n    samples=100, \n    color=blue,\n]\n{2*x + 1};\n\\addplot[\n    only marks,\n    color=red,\n    mark=*,\n    samples=10,\n]\ntable {\n    x y\n    1 3\n    2 5\n    3 7\n    4 9\n    5 11\n    6 13\n    7 15\n    8 17\n    9 19\n    10 21\n};\n\\end{axis}\n```\n\n\n---\n\n### Slide 2: Introducing Neural Networks\n\nNeural networks extend linear regression by adding hidden layers and non-linear activation functions.\n\n$$\ny = \\sigma(Wx + b)\n$$\n\n- **Weights**: \\( W \\) (matrix of weights), \\( b \\) (bias)\n- **Activation Function**: \\( \\sigma \\) (e.g., ReLU, sigmoid)\n\n\n```{tikz}\n\\begin{tikzpicture}\n    \\node[circle, draw, minimum size=1cm] (x1) at (0,0) {$x_1$};\n    \\node[circle, draw, minimum size=1cm] (x2) at (0,-1.5) {$x_2$};\n    \\node[circle, draw, minimum size=1cm] (h1) at (2,0) {$h_1$};\n    \\node[circle, draw, minimum size=1cm] (h2) at (2,-1.5) {$h_2$};\n    \\node[circle, draw, minimum size=1cm] (y) at (4,-0.75) {$y$};\n\n    \\draw[->] (x1) -- (h1);\n    \\draw[->] (x1) -- (h2);\n    \\draw[->] (x2) -- (h1);\n    \\draw[->] (x2) -- (h2);\n    \\draw[->] (h1) -- (y);\n    \\draw[->] (h2) -- (y);\n\\end{tikzpicture}\n```\n\n\n---\n\n### Slide 3: Training Neural Networks\n\nTraining involves adjusting weights to minimize the loss function using backpropagation and gradient descent.\n\n$$\n\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n$$\n\n- **Gradient Descent**: Update weights \\( W \\) using the gradient of the loss function\n- **Backpropagation**: Compute gradients using the chain rule\n\n\n```{tikz}\n\\begin{tikzpicture}\n    \\node[circle, draw, minimum size=1cm] (x1) at (0,0) {$x_1$};\n    \\node[circle, draw, minimum size=1cm] (x2) at (0,-1.5) {$x_2$};\n    \\node[circle, draw, minimum size=1cm] (h1) at (2,0) {$h_1$};\n    \\node[circle, draw, minimum size=1cm] (h2) at (2,-1.5) {$h_2$};\n    \\node[circle, draw, minimum size=1cm] (y) at (4,-0.75) {$y$};\n\n    \\draw[->] (x1) -- (h1);\n    \\draw[->] (x1) -- (h2);\n    \\draw[->] (x2) -- (h1);\n    \\draw[->] (x2) -- (h2);\n    \\draw[->] (h1) -- (y);\n    \\draw[->] (h2) -- (y);\n\n    \\node[below=0.5cm of h2] (loss) {Loss Function};\n    \\draw[->] (y) -- (loss);\n    \\draw[->] (loss) -- (h1);\n    \\draw[->] (loss) -- (h2);\n    \\draw[->] (loss) -- (x1);\n    \\draw[->] (loss) -- (x2);\n\\end{tikzpicture}\n```\n\n\n---\n\n---\n\n:::{.r-stack}\n\n![](figs/ai-landscape-5-dl.png){.fragment .fade-in-then-out}\n\n![](figs/ai-landscape-6-llm.png){.fragment .fade-in}\n\n:::\n\n\n## AI versus stats\n\n- scaling\n- emerging\n- continual learning\n\n# What is Deep Learning?\n\n# What is an LLM?\n\n# Pitfalls\n\n## Biases\n\n## Can we lose control?\n\n## Accuracy does not imply value for decision making\n\n## References\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}