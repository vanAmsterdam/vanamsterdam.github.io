
@misc{keoghPredictionInterventionsEvaluation2024,
	title = {Prediction under interventions: evaluation of counterfactual performance using longitudinal observational data},
	shorttitle = {Prediction under interventions},
	url = {http://arxiv.org/abs/2304.10005},
	doi = {10.48550/arXiv.2304.10005},
	abstract = {Predictions under interventions are estimates of what a person's risk of an outcome would be if they were to follow a particular treatment strategy, given their individual characteristics. Such predictions can give important input to medical decision making. However, evaluating predictive performance of interventional predictions is challenging. Standard ways of evaluating predictive performance do not apply when using observational data, because prediction under interventions involves obtaining predictions of the outcome under conditions that are different to those that are observed for a subset of individuals in the validation dataset. This work describes methods for evaluating counterfactual performance of predictions under interventions for time-to-event outcomes. This means we aim to assess how well predictions would match the validation data if all individuals had followed the treatment strategy under which predictions are made. We focus on counterfactual performance evaluation using longitudinal observational data, and under treatment strategies that involve sustaining a particular treatment regime over time. We introduce an estimation approach using artificial censoring and inverse probability weighting which involves creating a validation dataset that mimics the treatment strategy under which predictions are made. We extend measures of calibration, discrimination (c-index and cumulative/dynamic AUCt) and overall prediction error (Brier score) to allow assessment of counterfactual performance. The methods are evaluated using a simulation study, including scenarios in which the methods should detect poor performance. Applying our methods in the context of liver transplantation shows that our procedure allows quantification of the performance of predictions supporting crucial decisions on organ allocation.},
	urldate = {2024-03-30},
	publisher = {arXiv},
	author = {Keogh, Ruth H. and van Geloven, Nan},
	month = jan,
	year = {2024},
	note = {arXiv:2304.10005 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/wamster3/Zotero/storage/PAGIH8KE/2304.html:text/html;Keogh_van Geloven_2024_Prediction under interventions.pdf:/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Keogh_van Geloven_2024_Prediction under interventions.pdf:application/pdf},
}

@article{amsterdamAlgorithmsActionImproving2024,
  title = {From Algorithms to Action: Improving Patient Care Requires Causality},
  shorttitle = {From Algorithms to Action},
  author = {family=Amsterdam, given=Wouter A. C., prefix=van, useprefix=false and family=Jong, given=Pim A., prefix=de, useprefix=false and Verhoeff, Joost J. C. and Leiner, Tim and Ranganath, Rajesh},
  date = {2024},
  journaltitle = {BMC Medical Informatics and Decision Making},
  volume = {24},
  number = {1},
  doi = {10.1186/s12911-024-02513-3},
  abstract = {In cancer research there is much interest in building and validating outcome prediction models to support treatment decisions. However, because most outcome prediction models are developed and validated without regard to the causal aspects of treatment decision making, many published outcome prediction models may cause harm when used for decision making, despite being found accurate in validation studies. Guidelines on prediction model validation and the checklist for risk model endorsement by the American Joint Committee on Cancer do not protect against prediction models that are accurate during development and validation but harmful when used for decision making. We explain why this is the case and how to build and validate models that are useful for decision making.},
  langid = {english},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Amsterdam et al_2024_From algorithms to action.pdf;/Users/wamster3/Zotero/storage/3AK84HYV/10.html}
}

@article{candidodosreisUpdatedPREDICTBreast2017,
  title = {An Updated {{PREDICT}} Breast Cancer Prognostication and Treatment Benefit Prediction Model with Independent Validation},
  author = {Candido dos Reis, Francisco J. and Wishart, Gordon C. and Dicks, Ed M. and Greenberg, David and Rashbass, Jem and Schmidt, Marjanka K. and family=Broek, given=Alexandra J., prefix=van den, useprefix=true and Ellis, Ian O. and Green, Andrew and Rakha, Emad and Maishman, Tom and Eccles, Diana M. and Pharoah, Paul D. P.},
  date = {2017-12},
  journaltitle = {Breast Cancer Research},
  shortjournal = {Breast Cancer Res},
  volume = {19},
  number = {1},
  pages = {58},
  issn = {1465-542X},
  doi = {10/gbhgpq},
  abstract = {Background: PREDICT is a breast cancer prognostic and treatment benefit model implemented online. The overall fit of the model has been good in multiple independent case series, but PREDICT has been shown to underestimate breast cancer specific mortality in women diagnosed under the age of 40. Another limitation is the use of discrete categories for tumour size and node status resulting in ‘step’ changes in risk estimates on moving between categories. We have refitted the PREDICT prognostic model using the original cohort of cases from East Anglia with updated survival time in order to take into account age at diagnosis and to smooth out the survival function for tumour size and node status. Methods: Multivariable Cox regression models were used to fit separate models for ER negative and ER positive disease. Continuous variables were fitted using fractional polynomials and a smoothed baseline hazard was obtained by regressing the baseline cumulative hazard for each patients against time using fractional polynomials. The fit of the prognostic models were then tested in three independent data sets that had also been used to validate the original version of PREDICT. Results: In the model fitting data, after adjusting for other prognostic variables, there is an increase in risk of breast cancer specific mortality in younger and older patients with ER positive disease, with a substantial increase in risk for women diagnosed before the age of 35. In ER negative disease the risk increases slightly with age. The association between breast cancer specific mortality and both tumour size and number of positive nodes was non-linear with a more marked increase in risk with increasing size and increasing number of nodes in ER positive disease. The overall calibration and discrimination of the new version of PREDICT (v2) was good and comparable to that of the previous version in both model development and validation data sets. However, the calibration of v2 improved over v1 in patients diagnosed under the age of 40. Conclusions: The PREDICT v2 is an improved prognostication and treatment benefit model compared with v1. The online version should continue to aid clinical decision making in women with early breast cancer.},
  langid = {english},
  annotation = {80 citations (Crossref) [2021-08-06]},
  file = {/Users/wamster3/Zotero/storage/GFURY2B3/Candido dos Reis et al. - 2017 - An updated PREDICT breast cancer prognostication a.pdf}
}

@article{collinsTRIPODAIStatement2024,
  title = {{{TRIPOD}}+{{AI}} Statement: Updated Guidance for Reporting Clinical Prediction Models That Use Regression or Machine Learning Methods},
  shorttitle = {{{TRIPOD}}+{{AI}} Statement},
  author = {Collins, Gary S. and Moons, Karel G. M. and Dhiman, Paula and Riley, Richard D. and Beam, Andrew L. and Calster, Ben Van and Ghassemi, Marzyeh and Liu, Xiaoxuan and Reitsma, Johannes B. and family=Smeden, given=Maarten, prefix=van, useprefix=false and Boulesteix, Anne-Laure and Camaradou, Jennifer Catherine and Celi, Leo Anthony and Denaxas, Spiros and Denniston, Alastair K. and Glocker, Ben and Golub, Robert M. and Harvey, Hugh and Heinze, Georg and Hoffman, Michael M. and Kengne, André Pascal and Lam, Emily and Lee, Naomi and Loder, Elizabeth W. and Maier-Hein, Lena and Mateen, Bilal A. and McCradden, Melissa D. and Oakden-Rayner, Lauren and Ordish, Johan and Parnell, Richard and Rose, Sherri and Singh, Karandeep and Wynants, Laure and Logullo, Patricia},
  date = {2024-04-16},
  journaltitle = {BMJ},
  shortjournal = {BMJ},
  volume = {385},
  pages = {e078378},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj-2023-078378},
  abstract = {{$<$}p{$>$}The TRIPOD (Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis) statement was published in 2015 to provide the minimum reporting recommendations for studies developing or evaluating the performance of a prediction model. Methodological advances in the field of prediction have since included the widespread use of artificial intelligence (AI) powered by machine learning methods to develop prediction models. An update to the TRIPOD statement is thus needed. TRIPOD+AI provides harmonised guidance for reporting prediction model studies, irrespective of whether regression modelling or machine learning methods have been used. The new checklist supersedes the TRIPOD 2015 checklist, which should no longer be used. This article describes the development of TRIPOD+AI and presents the expanded 27 item checklist with more detailed explanation of each reporting recommendation, and the TRIPOD+AI for Abstracts checklist. TRIPOD+AI aims to promote the complete, accurate, and transparent reporting of studies that develop a prediction model or evaluate its performance. Complete reporting will facilitate study appraisal, model evaluation, and model implementation.{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Collins et al_2024_TRIPOD+AI statement.pdf;/Users/wamster3/Zotero/storage/2CNVEAXD/TRIPOD-AI_round_1_summary.pdf;/Users/wamster3/Zotero/storage/LMQ9WZ65/tripod_ai.png;/Users/wamster3/Zotero/storage/MQV2KUT3/TRIPOD-AI_PPI_summary_290322_redacated.pdf;/Users/wamster3/Zotero/storage/U39YZNHD/TRIPOD-AI consensus meeting information pack_redacted.pdf;/Users/wamster3/Zotero/storage/UARTIRIP/TRIPOD-AI Delphi Round 2 approvals 2.pdf}
}

@article{cooperEvaluationMachinelearningMethods1997,
  title = {An Evaluation of Machine-Learning Methods for Predicting Pneumonia Mortality},
  author = {Cooper, Gregory F. and Aliferis, Constantin F. and Ambrosino, Richard and Aronis, John and Buchanan, Bruce G. and Caruana, Richard and Fine, Michael J. and Glymour, Clark and Gordon, Geoffrey and Hanusa, Barbara H. and Janosky, Janine E. and Meek, Christopher and Mitchell, Tom and Richardson, Thomas and Spirtes, Peter},
  date = {1997-02-01},
  journaltitle = {Artificial Intelligence in Medicine},
  shortjournal = {Artificial Intelligence in Medicine},
  volume = {9},
  number = {2},
  pages = {107--138},
  issn = {0933-3657},
  doi = {10.1016/S0933-3657(96)00367-3},
  abstract = {This paper describes the application of eight statistical and machine-learning methods to derive computer models for predicting mortality of hospital patients with pneumonia from their findings at initial presentation. The eight models were each constructed based on 9847 patient cases and they were each evaluated on 4352 additional cases. The primary evaluation metric was the error in predicted survival as a function of the fraction of patients predicted to survive. This metric is useful in assessing a model's potential to assist a clinician in deciding whether to treat a given patient in the hospital or at home. We examined the error rates of the models when predicting that a given fraction of patients will survive. We examined survival fractions between 0.1 and 0.6. Over this range, each model's predictive error rate was within 1\% of the error rate of every other model. When predicting that approximately 30\% of the patients will survive, all the models have an error rate of less than 1.5\%. The models are distinguished more by the number of variables and parameters that they contain than by their error rates; these differences suggest which models may be the most amenable to future implementation as paper-based guidelines.},
  keywords = {Clinical databases,Computer-based prediction,Machine learning,Pneumonia},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Cooper et al_1997_An evaluation of machine-learning methods for predicting pneumonia mortality.pdf;/Users/wamster3/Zotero/storage/P9885335/ml_1997.png;/Users/wamster3/Zotero/storage/QJRMCGFX/S0933365796003673.html}
}

@article{hildenPrognosisMedicineAnalysis1987,
  title = {Prognosis in Medicine: {{An}} Analysis of Its Meaning and Rôles},
  shorttitle = {Prognosis in Medicine},
  author = {Hilden, Jørgen and Habbema, J. Dik F.},
  date = {1987-10-01},
  journaltitle = {Theoretical Medicine},
  shortjournal = {Theor Med Bioeth},
  volume = {8},
  number = {3},
  pages = {349--365},
  issn = {1573-1200},
  doi = {10.1007/BF00489469},
  abstract = {The medical concept of prognosis is analysed into its basic constituents: patient data, medical intervention, outcome, utilities and probabilities; and sources of utility and probability values are discussed. Prognosis cannot be divorced from contemplated medical action, nor from action to be taken by the patient in response to prognostication. Regrettably, the usual decision-theoretic approach ignores this latter aspect. Elicitation of utilities, decision contemplation and prognostic counselling interweave, diagnostics playing a subsidiary role in decision-oriented clinical practice. At times the doctor has grounds for withholding information. As this is known to the patient, prognostic counselling becomes a conflict-prone and rationality-thwarting activity. The meaning of standard phrases such as “prognosis of a disease”, “the prognosis of this patient”, “the prognosis is unknown”, is examined.},
  langid = {english},
  keywords = {Clinical trial,Medical decision-making,Physician-patient relations,Professional jargon (medicine),Prognosis,Utility theory},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Hilden_Habbema_1987_Prognosis in medicine.pdf}
}

@online{vanamsterdamConditionalAverageTreatment2022,
  title = {Conditional Average Treatment Effect Estimation with Treatment Offset Models},
  author = {family=Amsterdam, given=Wouter A. C., prefix=van, useprefix=true and Ranganath, Rajesh},
  date = {2022-04-29},
  eprint = {2204.13975},
  eprinttype = {arxiv},
  eprintclass = {stat},
  url = {http://arxiv.org/abs/2204.13975},
  urldate = {2022-06-29},
  abstract = {Treatment effect estimates are often available from randomized controlled trials as a single average treatment effect for a certain patient population. Estimates of the conditional average treatment effect (CATE) are more useful for individualized treatment decision making, but randomized trials are often too small to estimate the CATE. There are several examples in medical literature where the assumption of a known constant relative treatment effect (e.g. an odds-ratio) is used to estimate CATE models from large observational datasets. One approach to estimating these CATE models is by using the relative treatment effect as an offset, while estimating the covariate-specific baseline risk. Whether this is a valid approach in the presence of unobserved confounding is unknown. We demonstrate for a simple example that offset models do not recover the true CATE in the presence of unobserved confounding. We then explore the magnitude of this bias in numerical experiments. For virtually all plausible confounding magnitudes, estimating the CATE using offset models is more accurate than assuming a single absolute treatment effect whenever there is sufficient variation in the baseline risk. Next, we observe that the odds-ratios reported in randomized controlled trials are not the odds-ratios that are needed in offset models because trials often report the marginal odds-ratio. We introduce a constraint to better use marginal odds-ratios from randomized controlled trials and find that the newly introduced constrained offset models have lower bias than standard offset models. Finally, we highlight directions for future research for exploiting the assumption of a constant relative treatment effect with offset models.},
  pubstate = {preprint},
  keywords = {Statistics - Methodology},
  file = {/Users/wamster3/Zotero/storage/6IK7JE3C/van Amsterdam and Ranganath - 2022 - Conditional average treatment effect estimation wi.pdf;/Users/wamster3/Zotero/storage/8B4USZQQ/2204.html}
}

@article{vanamsterdamConditionalAverageTreatment2023,
  title = {Conditional Average Treatment Effect Estimation with Marginally Constrained Models},
  author = {family=Amsterdam, given=Wouter A. C., prefix=van, useprefix=true and Ranganath, Rajesh},
  date = {2023-08-29},
  journaltitle = {Journal of Causal Inference},
  volume = {11},
  number = {1},
  pages = {20220027},
  issn = {2193-3685},
  doi = {10.1515/jci-2022-0027},
  abstract = {Abstract                            Treatment effect estimates are often available from randomized controlled trials as a single               average treatment effect               for a certain patient population. Estimates of the               conditional average treatment effect               (CATE) are more useful for individualized treatment decision-making, but randomized trials are often too small to estimate the CATE. Examples in medical literature make use of the               relative               treatment effect (e.g. an odds ratio) reported by randomized trials to estimate the CATE using large observational datasets. One approach to estimating these CATE models is by using the relative treatment effect as an               offset               , while estimating the covariate-specific untreated risk. We observe that the odds ratios reported in randomized controlled trials are not the odds ratios that are needed in offset models because trials often report the               marginal               odds ratio. We introduce a constraint or a regularizer to better use marginal odds ratios from randomized controlled trials and find that under the standard observational causal inference assumptions, this approach provides a consistent estimate of the CATE. Next, we show that the offset approach is not valid for CATE estimation in the presence of unobserved confounding. We study if the offset assumption and the marginal constraint lead to better approximations of the CATE relative to the alternative of using the average treatment effect estimate from the randomized trial. We empirically show that when the underlying CATE has sufficient variation, the constraint and offset approaches lead to closer approximations to the CATE.},
  langid = {english},
  file = {/Users/wamster3/Zotero/storage/2F3NJEFG/Van Amsterdam and Ranganath - 2023 - Conditional average treatment effect estimation wi.pdf}
}

@article{vanamsterdamIndividualTreatmentEffect2022,
  title = {Individual Treatment Effect Estimation in the Presence of Unobserved Confounding Using Proxies: A Cohort Study in Stage {{III}} Non-Small Cell Lung Cancer},
  shorttitle = {Individual Treatment Effect Estimation in the Presence of Unobserved Confounding Using Proxies},
  author = {family=Amsterdam, given=Wouter A. C., prefix=van, useprefix=true and Verhoeff, Joost J. C. and Harlianto, Netanja I. and Bartholomeus, Gijs A. and Puli, Aahlad Manas and family=Jong, given=Pim A., prefix=de, useprefix=true and Leiner, Tim and family=Lindert, given=Anne S. R., prefix=van, useprefix=true and Eijkemans, Marinus J. C. and Ranganath, Rajesh},
  date = {2022-04-07},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {12},
  number = {1},
  pages = {5848},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-09775-9},
  abstract = {Randomized Controlled Trials (RCT) are the gold standard for estimating treatment effects but some important situations in cancer care require treatment effect estimates from observational data. We developed “Proxy based individual treatment effect modeling in cancer” (PROTECT) to estimate treatment effects from observational data when there are unobserved confounders, but proxy measurements of these confounders exist. We identified an unobserved confounder in observational cancer research: overall fitness. Proxy measurements of overall fitness exist like performance score, but the fitness as observed by the treating physician is unavailable for research. PROTECT reconstructs the distribution of the unobserved confounder based on these proxy measurements to estimate the treatment effect. PROTECT was applied to an observational cohort of 504 stage III non-small cell lung cancer (NSCLC) patients, treated with concurrent chemoradiation or sequential chemoradiation. Whereas conventional confounding adjustment methods seemed to overestimate the treatment effect, PROTECT provided credible treatment effect estimates.},
  issue = {1},
  langid = {english},
  keywords = {Lung cancer,Outcomes research,Predictive markers,Prognostic markers,Statistics},
  file = {/Users/wamster3/Zotero/storage/G839AXMF/41598_2022_9775_MOESM1_ESM.pdf;/Users/wamster3/Zotero/storage/KRJZL4EQ/van Amsterdam et al. - 2022 - Individual treatment effect estimation in the pres.pdf;/Users/wamster3/Zotero/storage/D995HPCL/s41598-022-09775-9.html}
}

@online{vanamsterdamWhenAccuratePrediction2024a,
  title = {When Accurate Prediction Models Yield Harmful Self-Fulfilling Prophecies},
  author = {family=Amsterdam, given=Wouter A. C., prefix=van, useprefix=true and family=Geloven, given=Nan, prefix=van, useprefix=true and Krijthe, Jesse H. and Ranganath, Rajesh and Ciná, Giovanni},
  date = {2024-02-08},
  eprint = {2312.01210},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2312.01210},
  abstract = {Objective: Prediction models are popular in medical research and practice. By predicting an outcome of interest for specific patients, these models may help inform difficult treatment decisions, and are often hailed as the poster children for personalized, data-driven healthcare. Many prediction models are deployed for decision support based on their prediction accuracy in validation studies. We investigate whether this is a safe and valid approach. Materials and Methods: We show that using prediction models for decision making can lead to harmful decisions, even when the predictions exhibit good discrimination after deployment. These models are harmful self-fulfilling prophecies: their deployment harms a group of patients but the worse outcome of these patients does not invalidate the predictive power of the model. Results: Our main result is a formal characterization of a set of such prediction models. Next we show that models that are well calibrated before and after deployment are useless for decision making as they made no change in the data distribution. Discussion: Our results point to the need to revise standard practices for validation, deployment and evaluation of prediction models that are used in medical decisions. Conclusion: Outcome prediction models can yield harmful self-fulfilling prophecies when used for decision making, a new perspective on prediction model development, deployment and monitoring is needed.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/wamster3/Zotero/storage/TK4LYI8J/van Amsterdam et al. - 2024 - When accurate prediction models yield harmful self.pdf;/Users/wamster3/Zotero/storage/XJDVFTAH/2312.html}
}

@article{xuPredictionCardiovascularDisease2021a,
  title = {Prediction of {{Cardiovascular Disease Risk Accounting}} for {{Future Initiation}} of {{Statin Treatment}}},
  author = {Xu, Zhe and Arnold, Matthew and Stevens, David and Kaptoge, Stephen and Pennells, Lisa and Sweeting, Michael J and Barrett, Jessica and Di Angelantonio, Emanuele and Wood, Angela M},
  date = {2021-10-01},
  journaltitle = {American Journal of Epidemiology},
  volume = {190},
  number = {10},
  pages = {2000--2014},
  issn = {0002-9262, 1476-6256},
  doi = {10.1093/aje/kwab031},
  abstract = {Abstract             Cardiovascular disease (CVD) risk-prediction models are used to identify high-risk individuals and guide statin initiation. However, these models are usually derived from individuals who might initiate statins during follow-up. We present a simple approach to address statin initiation to predict “statin-naive” CVD risk. We analyzed primary care data (2004–2017) from the UK Clinical Practice Research Datalink for 1,678,727 individuals (aged 40–85 years) without CVD or statin treatment history at study entry. We derived age- and sex-specific prediction models including conventional risk factors and a time-dependent effect of statin initiation constrained to 25\% risk reduction (from trial results). We compared predictive performance and measures of public-health impact (e.g., number needed to screen to prevent 1 event) against models ignoring statin initiation. During a median follow-up of 8.9 years, 103,163 individuals developed CVD. In models accounting for (versus ignoring) statin initiation, 10-year CVD risk predictions were slightly higher; predictive performance was moderately improved. However, few individuals were reclassified to a high-risk threshold, resulting in negligible improvements in number needed to screen to prevent 1 event. In conclusion, incorporating statin effects from trial results into risk-prediction models enables statin-naive CVD risk estimation and provides moderate gains in predictive ability but had a limited impact on treatment decision-making under current guidelines in this population.},
  langid = {english},
  file = {/Users/wamster3/Zotero/storage/9D4DB9PI/Xu et al. - 2021 - Prediction of Cardiovascular Disease Risk Accounti.pdf}
}
