@article{amsterdamAlgorithmsActionImproving2024,
  title = {From Algorithms to Action: Improving Patient Care Requires Causality},
  shorttitle = {From Algorithms to Action},
  author = {family=Amsterdam, given=Wouter A. C., prefix=van, useprefix=false and family=Jong, given=Pim A., prefix=de, useprefix=false and Verhoeff, Joost J. C. and Leiner, Tim and Ranganath, Rajesh},
  date = {2024},
  journaltitle = {BMC Medical Informatics and Decision Making},
  volume = {24},
  number = {1},
  doi = {10.1186/s12911-024-02513-3},
  abstract = {In cancer research there is much interest in building and validating outcome prediction models to support treatment decisions. However, because most outcome prediction models are developed and validated without regard to the causal aspects of treatment decision making, many published outcome prediction models may cause harm when used for decision making, despite being found accurate in validation studies. Guidelines on prediction model validation and the checklist for risk model endorsement by the American Joint Committee on Cancer do not protect against prediction models that are accurate during development and validation but harmful when used for decision making. We explain why this is the case and how to build and validate models that are useful for decision making.},
  langid = {english},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Amsterdam et al_2024_From algorithms to action.pdf;/Users/wamster3/Zotero/storage/3AK84HYV/10.html}
}

@article{collinsTRIPODAIStatement2024,
  title = {{{TRIPOD}}+{{AI}} Statement: Updated Guidance for Reporting Clinical Prediction Models That Use Regression or Machine Learning Methods},
  shorttitle = {{{TRIPOD}}+{{AI}} Statement},
  author = {Collins, Gary S. and Moons, Karel G. M. and Dhiman, Paula and Riley, Richard D. and Beam, Andrew L. and Calster, Ben Van and Ghassemi, Marzyeh and Liu, Xiaoxuan and Reitsma, Johannes B. and family=Smeden, given=Maarten, prefix=van, useprefix=false and Boulesteix, Anne-Laure and Camaradou, Jennifer Catherine and Celi, Leo Anthony and Denaxas, Spiros and Denniston, Alastair K. and Glocker, Ben and Golub, Robert M. and Harvey, Hugh and Heinze, Georg and Hoffman, Michael M. and Kengne, André Pascal and Lam, Emily and Lee, Naomi and Loder, Elizabeth W. and Maier-Hein, Lena and Mateen, Bilal A. and McCradden, Melissa D. and Oakden-Rayner, Lauren and Ordish, Johan and Parnell, Richard and Rose, Sherri and Singh, Karandeep and Wynants, Laure and Logullo, Patricia},
  date = {2024-04-16},
  journaltitle = {BMJ},
  shortjournal = {BMJ},
  volume = {385},
  pages = {e078378},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj-2023-078378},
  abstract = {{$<$}p{$>$}The TRIPOD (Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis) statement was published in 2015 to provide the minimum reporting recommendations for studies developing or evaluating the performance of a prediction model. Methodological advances in the field of prediction have since included the widespread use of artificial intelligence (AI) powered by machine learning methods to develop prediction models. An update to the TRIPOD statement is thus needed. TRIPOD+AI provides harmonised guidance for reporting prediction model studies, irrespective of whether regression modelling or machine learning methods have been used. The new checklist supersedes the TRIPOD 2015 checklist, which should no longer be used. This article describes the development of TRIPOD+AI and presents the expanded 27 item checklist with more detailed explanation of each reporting recommendation, and the TRIPOD+AI for Abstracts checklist. TRIPOD+AI aims to promote the complete, accurate, and transparent reporting of studies that develop a prediction model or evaluate its performance. Complete reporting will facilitate study appraisal, model evaluation, and model implementation.{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Collins et al_2024_TRIPOD+AI statement.pdf;/Users/wamster3/Zotero/storage/2CNVEAXD/TRIPOD-AI_round_1_summary.pdf;/Users/wamster3/Zotero/storage/LMQ9WZ65/tripod_ai.png;/Users/wamster3/Zotero/storage/MQV2KUT3/TRIPOD-AI_PPI_summary_290322_redacated.pdf;/Users/wamster3/Zotero/storage/U39YZNHD/TRIPOD-AI consensus meeting information pack_redacted.pdf;/Users/wamster3/Zotero/storage/UARTIRIP/TRIPOD-AI Delphi Round 2 approvals 2.pdf}
}

@article{cooperEvaluationMachinelearningMethods1997,
  title = {An Evaluation of Machine-Learning Methods for Predicting Pneumonia Mortality},
  author = {Cooper, Gregory F. and Aliferis, Constantin F. and Ambrosino, Richard and Aronis, John and Buchanan, Bruce G. and Caruana, Richard and Fine, Michael J. and Glymour, Clark and Gordon, Geoffrey and Hanusa, Barbara H. and Janosky, Janine E. and Meek, Christopher and Mitchell, Tom and Richardson, Thomas and Spirtes, Peter},
  date = {1997-02-01},
  journaltitle = {Artificial Intelligence in Medicine},
  shortjournal = {Artificial Intelligence in Medicine},
  volume = {9},
  number = {2},
  pages = {107--138},
  issn = {0933-3657},
  doi = {10.1016/S0933-3657(96)00367-3},
  abstract = {This paper describes the application of eight statistical and machine-learning methods to derive computer models for predicting mortality of hospital patients with pneumonia from their findings at initial presentation. The eight models were each constructed based on 9847 patient cases and they were each evaluated on 4352 additional cases. The primary evaluation metric was the error in predicted survival as a function of the fraction of patients predicted to survive. This metric is useful in assessing a model's potential to assist a clinician in deciding whether to treat a given patient in the hospital or at home. We examined the error rates of the models when predicting that a given fraction of patients will survive. We examined survival fractions between 0.1 and 0.6. Over this range, each model's predictive error rate was within 1\% of the error rate of every other model. When predicting that approximately 30\% of the patients will survive, all the models have an error rate of less than 1.5\%. The models are distinguished more by the number of variables and parameters that they contain than by their error rates; these differences suggest which models may be the most amenable to future implementation as paper-based guidelines.},
  keywords = {Clinical databases,Computer-based prediction,Machine learning,Pneumonia},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Cooper et al_1997_An evaluation of machine-learning methods for predicting pneumonia mortality.pdf;/Users/wamster3/Zotero/storage/P9885335/ml_1997.png;/Users/wamster3/Zotero/storage/QJRMCGFX/S0933365796003673.html}
}

@article{hildenPrognosisMedicineAnalysis1987,
  title = {Prognosis in Medicine: {{An}} Analysis of Its Meaning and Rôles},
  shorttitle = {Prognosis in Medicine},
  author = {Hilden, Jørgen and Habbema, J. Dik F.},
  date = {1987-10-01},
  journaltitle = {Theoretical Medicine},
  shortjournal = {Theor Med Bioeth},
  volume = {8},
  number = {3},
  pages = {349--365},
  issn = {1573-1200},
  doi = {10.1007/BF00489469},
  abstract = {The medical concept of prognosis is analysed into its basic constituents: patient data, medical intervention, outcome, utilities and probabilities; and sources of utility and probability values are discussed. Prognosis cannot be divorced from contemplated medical action, nor from action to be taken by the patient in response to prognostication. Regrettably, the usual decision-theoretic approach ignores this latter aspect. Elicitation of utilities, decision contemplation and prognostic counselling interweave, diagnostics playing a subsidiary role in decision-oriented clinical practice. At times the doctor has grounds for withholding information. As this is known to the patient, prognostic counselling becomes a conflict-prone and rationality-thwarting activity. The meaning of standard phrases such as “prognosis of a disease”, “the prognosis of this patient”, “the prognosis is unknown”, is examined.},
  langid = {english},
  keywords = {Clinical trial,Medical decision-making,Physician-patient relations,Professional jargon (medicine),Prognosis,Utility theory},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Hilden_Habbema_1987_Prognosis in medicine.pdf}
}

@online{vanamsterdamWhenAccuratePrediction2024a,
  title = {When Accurate Prediction Models Yield Harmful Self-Fulfilling Prophecies},
  author = {family=Amsterdam, given=Wouter A. C., prefix=van, useprefix=true and family=Geloven, given=Nan, prefix=van, useprefix=true and Krijthe, Jesse H. and Ranganath, Rajesh and Ciná, Giovanni},
  date = {2024-02-08},
  eprint = {2312.01210},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2312.01210},
  abstract = {Objective: Prediction models are popular in medical research and practice. By predicting an outcome of interest for specific patients, these models may help inform difficult treatment decisions, and are often hailed as the poster children for personalized, data-driven healthcare. Many prediction models are deployed for decision support based on their prediction accuracy in validation studies. We investigate whether this is a safe and valid approach. Materials and Methods: We show that using prediction models for decision making can lead to harmful decisions, even when the predictions exhibit good discrimination after deployment. These models are harmful self-fulfilling prophecies: their deployment harms a group of patients but the worse outcome of these patients does not invalidate the predictive power of the model. Results: Our main result is a formal characterization of a set of such prediction models. Next we show that models that are well calibrated before and after deployment are useless for decision making as they made no change in the data distribution. Discussion: Our results point to the need to revise standard practices for validation, deployment and evaluation of prediction models that are used in medical decisions. Conclusion: Outcome prediction models can yield harmful self-fulfilling prophecies when used for decision making, a new perspective on prediction model development, deployment and monitoring is needed.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/wamster3/Zotero/storage/TK4LYI8J/van Amsterdam et al. - 2024 - When accurate prediction models yield harmful self.pdf;/Users/wamster3/Zotero/storage/XJDVFTAH/2312.html}
}
