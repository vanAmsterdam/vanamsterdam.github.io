@article{alaaMachineLearningGuide2021,
  title = {Machine Learning to Guide the Use of Adjuvant Therapies for Breast Cancer},
  author = {Alaa, Ahmed M. and Gurdasani, Deepti and Harris, Adrian L. and Rashbass, Jem and {van der Schaar}, Mihaela},
  year = {2021},
  month = jun,
  journal = {Nature Machine Intelligence},
  pages = {1--11},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10/gk6bh7},
  urldate = {2021-07-13},
  abstract = {Accurate prediction of the individualized survival benefit of adjuvant therapy is key to making informed therapeutic decisions for patients with early invasive breast cancer. Machine learning technologies can enable accurate prognostication of patient outcomes under different treatment options by modelling complex interactions between risk factors in a data-driven fashion. Here, we use an automated and interpretable machine learning algorithm to develop a breast cancer prognostication and treatment benefit prediction model---Adjutorium---using data from large-scale cohorts of nearly one million women captured in the national cancer registries of the United Kingdom and the United States. We trained and internally validated the Adjutorium model on 395,862 patients from the UK National Cancer Registration and Analysis Service (NCRAS), and then externally validated the model among 571,635 patients from the US Surveillance, Epidemiology, and End Results (SEER) programme. Adjutorium exhibited significantly improved accuracy compared to the major prognostic tool in current clinical use (PREDICT v2.1) in both internal and external validation. Importantly, our model substantially improved accuracy in specific subgroups known to be under-served by existing models. Adjutorium is currently implemented as a web-based decision support tool (https://vanderschaar-lab.com/adjutorium/) to aid decisions on adjuvant therapy in women with early breast cancer, and can be publicly accessed by patients and clinicians worldwide.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Research\\
Subject\_term: Breast cancer;Prognosis\\
Subject\_term\_id: breast-cancer;prognosis},
  file = {/Users/wamster3/Zotero/storage/99UWTJ2A/s42256-021-00353-8.html}
}

@incollection{bareinboimPearlsHierarchyFoundations2022,
  title = {On {{Pearl}}'s {{Hierarchy}} and the {{Foundations}} of {{Causal Inference}} (1st Edition)},
  booktitle = {Probabilistic and {{Causal Inference}}: The {{Works}} of {{Judea Pearl}}},
  author = {Bareinboim, Elias and Correa, Juan and Ibeling, Duligur and Icard, Thomas},
  editor = {Geffner, Hector and Dechter, Rita and Halpern, Joseph},
  year = {2022},
  pages = {507--556},
  publisher = {ACM Books},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Bareinboim et al. - 2022 - On Pearl's Hierarchy and the Foundations of Causal Inference (1st edition).pdf;/Users/wamster3/Zotero/storage/PKFIMXHP/BAROPH-2.html}
}

@article{candidodosreisUpdatedPREDICTBreast2017,
  title = {An Updated {{PREDICT}} Breast Cancer Prognostication and Treatment Benefit Prediction Model with Independent Validation},
  author = {{Candido dos Reis}, Francisco J. and Wishart, Gordon C. and Dicks, Ed M. and Greenberg, David and Rashbass, Jem and Schmidt, Marjanka K. and {van den Broek}, Alexandra J. and Ellis, Ian O. and Green, Andrew and Rakha, Emad and Maishman, Tom and Eccles, Diana M. and Pharoah, Paul D. P.},
  year = {2017},
  month = dec,
  journal = {Breast Cancer Research},
  volume = {19},
  number = {1},
  pages = {58},
  issn = {1465-542X},
  doi = {10/gbhgpq},
  urldate = {2021-08-06},
  abstract = {Background: PREDICT is a breast cancer prognostic and treatment benefit model implemented online. The overall fit of the model has been good in multiple independent case series, but PREDICT has been shown to underestimate breast cancer specific mortality in women diagnosed under the age of 40. Another limitation is the use of discrete categories for tumour size and node status resulting in `step' changes in risk estimates on moving between categories. We have refitted the PREDICT prognostic model using the original cohort of cases from East Anglia with updated survival time in order to take into account age at diagnosis and to smooth out the survival function for tumour size and node status. Methods: Multivariable Cox regression models were used to fit separate models for ER negative and ER positive disease. Continuous variables were fitted using fractional polynomials and a smoothed baseline hazard was obtained by regressing the baseline cumulative hazard for each patients against time using fractional polynomials. The fit of the prognostic models were then tested in three independent data sets that had also been used to validate the original version of PREDICT. Results: In the model fitting data, after adjusting for other prognostic variables, there is an increase in risk of breast cancer specific mortality in younger and older patients with ER positive disease, with a substantial increase in risk for women diagnosed before the age of 35. In ER negative disease the risk increases slightly with age. The association between breast cancer specific mortality and both tumour size and number of positive nodes was non-linear with a more marked increase in risk with increasing size and increasing number of nodes in ER positive disease. The overall calibration and discrimination of the new version of PREDICT (v2) was good and comparable to that of the previous version in both model development and validation data sets. However, the calibration of v2 improved over v1 in patients diagnosed under the age of 40. Conclusions: The PREDICT v2 is an improved prognostication and treatment benefit model compared with v1. The online version should continue to aid clinical decision making in women with early breast cancer.},
  langid = {english},
  annotation = {80 citations (Crossref) [2021-08-06]},
  file = {/Users/wamster3/Zotero/storage/GFURY2B3/Candido dos Reis et al. - 2017 - An updated PREDICT breast cancer prognostication a.pdf}
}

@article{collinsTRIPOD+AIStatementUpdated2024,
  title = {{{TRIPOD}}+{{AI}} Statement: Updated Guidance for Reporting Clinical Prediction Models That Use Regression or Machine Learning Methods},
  shorttitle = {{{TRIPOD}}+{{AI}} Statement},
  author = {Collins, Gary S. and Moons, Karel G. M. and Dhiman, Paula and Riley, Richard D. and Beam, Andrew L. and Calster, Ben Van and Ghassemi, Marzyeh and Liu, Xiaoxuan and Reitsma, Johannes B. and van Smeden, Maarten and Boulesteix, Anne-Laure and Camaradou, Jennifer Catherine and Celi, Leo Anthony and Denaxas, Spiros and Denniston, Alastair K. and Glocker, Ben and Golub, Robert M. and Harvey, Hugh and Heinze, Georg and Hoffman, Michael M. and Kengne, Andr{\'e} Pascal and Lam, Emily and Lee, Naomi and Loder, Elizabeth W. and {Maier-Hein}, Lena and Mateen, Bilal A. and McCradden, Melissa D. and {Oakden-Rayner}, Lauren and Ordish, Johan and Parnell, Richard and Rose, Sherri and Singh, Karandeep and Wynants, Laure and Logullo, Patricia},
  year = {2024},
  month = apr,
  journal = {BMJ},
  volume = {385},
  pages = {e078378},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj-2023-078378},
  urldate = {2024-04-16},
  abstract = {{$<$}p{$>$}The TRIPOD (Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis) statement was published in 2015 to provide the minimum reporting recommendations for studies developing or evaluating the performance of a prediction model. Methodological advances in the field of prediction have since included the widespread use of artificial intelligence (AI) powered by machine learning methods to develop prediction models. An update to the TRIPOD statement is thus needed. TRIPOD+AI provides harmonised guidance for reporting prediction model studies, irrespective of whether regression modelling or machine learning methods have been used. The new checklist supersedes the TRIPOD 2015 checklist, which should no longer be used. This article describes the development of TRIPOD+AI and presents the expanded 27 item checklist with more detailed explanation of each reporting recommendation, and the TRIPOD+AI for Abstracts checklist. TRIPOD+AI aims to promote the complete, accurate, and transparent reporting of studies that develop a prediction model or evaluate its performance. Complete reporting will facilitate study appraisal, model evaluation, and model implementation.{$<$}/p{$>$}},
  chapter = {Research Methods \&amp; Reporting},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
  langid = {english},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Collins et al_2024_TRIPOD+AI statement.pdf;/Users/wamster3/Zotero/storage/2CNVEAXD/TRIPOD-AI_round_1_summary.pdf;/Users/wamster3/Zotero/storage/LMQ9WZ65/tripod_ai.png;/Users/wamster3/Zotero/storage/MQV2KUT3/TRIPOD-AI_PPI_summary_290322_redacated.pdf;/Users/wamster3/Zotero/storage/U39YZNHD/TRIPOD-AI consensus meeting information pack_redacted.pdf;/Users/wamster3/Zotero/storage/UARTIRIP/TRIPOD-AI Delphi Round 2 approvals 2.pdf}
}

@article{cooperEvaluationMachinelearningMethods1997,
  title = {An Evaluation of Machine-Learning Methods for Predicting Pneumonia Mortality},
  author = {Cooper, Gregory F. and Aliferis, Constantin F. and Ambrosino, Richard and Aronis, John and Buchanan, Bruce G. and Caruana, Richard and Fine, Michael J. and Glymour, Clark and Gordon, Geoffrey and Hanusa, Barbara H. and Janosky, Janine E. and Meek, Christopher and Mitchell, Tom and Richardson, Thomas and Spirtes, Peter},
  year = {1997},
  month = feb,
  journal = {Artificial Intelligence in Medicine},
  volume = {9},
  number = {2},
  pages = {107--138},
  issn = {0933-3657},
  doi = {10.1016/S0933-3657(96)00367-3},
  urldate = {2024-04-05},
  abstract = {This paper describes the application of eight statistical and machine-learning methods to derive computer models for predicting mortality of hospital patients with pneumonia from their findings at initial presentation. The eight models were each constructed based on 9847 patient cases and they were each evaluated on 4352 additional cases. The primary evaluation metric was the error in predicted survival as a function of the fraction of patients predicted to survive. This metric is useful in assessing a model's potential to assist a clinician in deciding whether to treat a given patient in the hospital or at home. We examined the error rates of the models when predicting that a given fraction of patients will survive. We examined survival fractions between 0.1 and 0.6. Over this range, each model's predictive error rate was within 1\% of the error rate of every other model. When predicting that approximately 30\% of the patients will survive, all the models have an error rate of less than 1.5\%. The models are distinguished more by the number of variables and parameters that they contain than by their error rates; these differences suggest which models may be the most amenable to future implementation as paper-based guidelines.},
  keywords = {Clinical databases,Computer-based prediction,Machine learning,Pneumonia},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Cooper et al_1997_An evaluation of machine-learning methods for predicting pneumonia mortality.pdf;/Users/wamster3/Zotero/storage/P9885335/ml_1997.png;/Users/wamster3/Zotero/storage/QJRMCGFX/S0933365796003673.html}
}

@article{dahabrehCausalInferenceEffects2024,
  title = {Causal {{Inference About}} the {{Effects}} of {{Interventions From Observational Studies}} in {{Medical Journals}}},
  author = {Dahabreh, Issa J. and {Bibbins-Domingo}, Kirsten},
  year = {2024},
  month = jun,
  journal = {JAMA},
  volume = {331},
  number = {21},
  pages = {1845--1853},
  issn = {0098-7484},
  doi = {10.1001/jama.2024.7741},
  urldate = {2024-06-05},
  abstract = {Many medical journals, including JAMA, restrict the use of causal language to the reporting of randomized clinical trials. Although well-conducted randomized clinical trials remain the preferred approach for answering causal questions, methods for observational studies have advanced such that causal interpretations of the results of well-conducted observational studies may be possible when strong assumptions hold. Furthermore, observational studies may be the only practical source of information for answering some questions about the causal effects of medical or policy interventions, can support the study of interventions in populations and settings that reflect practice, and can help identify interventions for further experimental investigation. Identifying opportunities for the appropriate use of causal language when describing observational studies is important for communication in medical journals.A structured approach to whether and how causal language may be used when describing observational studies would enhance the communication of research goals, support the assessment of assumptions and design and analytic choices, and allow for more clear and accurate interpretation of results. Building on the extensive literature on causal inference across diverse disciplines, we suggest a framework for observational studies that aim to provide evidence about the causal effects of interventions based on 6 core questions: what is the causal question; what quantity would, if known, answer the causal question; what is the study design; what causal assumptions are being made; how can the observed data be used to answer the causal question in principle and in practice; and is a causal interpretation of the analyses tenable?Adoption of the proposed framework to identify when causal interpretation is appropriate in observational studies promises to facilitate better communication between authors, reviewers, editors, and readers. Practical implementation will require cooperation between editors, authors, and reviewers to operationalize the framework and evaluate its effect on the reporting of empirical research.},
  file = {/Users/wamster3/Zotero/storage/XN9Q3CNL/2818746.html}
}

@article{flanaginWhatDoesProposed2024,
  title = {What {{Does}} the {{Proposed Causal Inference Framework}} for {{Observational Studies Mean}} for {{JAMA}} and the {{JAMA Network Journals}}?},
  author = {Flanagin, Annette and Lewis, Roger J. and Muth, Christopher C. and Curfman, Gregory},
  year = {2024},
  month = jun,
  journal = {JAMA},
  volume = {331},
  number = {21},
  pages = {1812--1813},
  issn = {0098-7484},
  doi = {10.1001/jama.2024.8107},
  urldate = {2024-06-07},
  abstract = {The Special Communication ``Causal Inferences About the Effects of Interventions From Observational Studies in Medical Journals,'' published in this issue of JAMA, provides a rationale and framework for considering causal inference from observational studies published by medical journals. Our intent is to invite discussion of this framework, explore its application in the context of specific study designs, and actively examine how this framework could be implemented and used by authors, peer reviewers, and editors of medical journals, including JAMA and the journals of the JAMA Network. Our overarching goal is to ensure that findings from observational designs may be appropriately interpreted in thoughtful and circumspect manners and applied by readers, other researchers, and clinicians, with the ultimate goal of improving patient care and public and global health.},
  file = {/Users/wamster3/Zotero/storage/M45EGPEY/2818747.html}
}

@inproceedings{hartfordDeepIVFlexible2017,
  title = {Deep {{IV}}: {{A Flexible Approach}} for {{Counterfactual Prediction}}},
  shorttitle = {Deep {{IV}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Hartford, Jason and Lewis, Greg and {Leyton-Brown}, Kevin and Taddy, Matt},
  year = {2017},
  month = jul,
  pages = {1414--1423},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2021-09-18},
  langid = {english},
  file = {/Users/wamster3/Zotero/storage/I5D4N2WT/Hartford et al. - 2017 - Deep IV A Flexible Approach for Counterfactual Pr.pdf;/Users/wamster3/Zotero/storage/XJQY474P/Hartford et al. - 2017 - Deep IV A Flexible Approach for Counterfactual Pr.pdf}
}

@book{hernanCausalInferenceWhat2020a,
  title = {Causal {{Inference}}: {{What If}}},
  author = {Hernan, Miguel A and Robins, James M},
  year = {2020},
  langid = {english},
  file = {/Users/wamster3/Zotero/storage/9T4MFXV8/Hernan and Robins - Causal Inference What If.pdf}
}

@article{hernanCWordScientificEuphemisms2018,
  title = {The {{C-Word}}: {{Scientific Euphemisms Do Not Improve Causal Inference From Observational Data}}},
  shorttitle = {The {{C-Word}}},
  author = {Hern{\'a}n, Miguel A.},
  year = {2018},
  month = may,
  journal = {American Journal of Public Health},
  volume = {108},
  number = {5},
  pages = {616--619},
  issn = {0090-0036},
  doi = {10.2105/AJPH.2018.304337},
  urldate = {2024-06-07},
  abstract = {Causal inference is a core task of science. However, authors and editors often refrain from explicitly acknowledging the causal goal of research projects; they refer to causal effect estimates as associational estimates., This commentary argues that using the term ``causal'' is necessary to improve the quality of observational research., Specifically, being explicit about the causal objective of a study reduces ambiguity in the scientific question, errors in the data analysis, and excesses in the interpretation of the results.},
  pmcid = {PMC5888052},
  pmid = {29565659},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Hern√°n_2018_The C-Word.pdf}
}

@article{hildenPrognosisMedicineAnalysis1987,
  title = {Prognosis in Medicine: {{An}} Analysis of Its Meaning and R{\^o}les},
  shorttitle = {Prognosis in Medicine},
  author = {Hilden, J{\o}rgen and Habbema, J. Dik F.},
  year = {1987},
  month = oct,
  journal = {Theoretical Medicine},
  volume = {8},
  number = {3},
  pages = {349--365},
  issn = {1573-1200},
  doi = {10.1007/BF00489469},
  urldate = {2024-04-25},
  abstract = {The medical concept of prognosis is analysed into its basic constituents: patient data, medical intervention, outcome, utilities and probabilities; and sources of utility and probability values are discussed. Prognosis cannot be divorced from contemplated medical action, nor from action to be taken by the patient in response to prognostication. Regrettably, the usual decision-theoretic approach ignores this latter aspect. Elicitation of utilities, decision contemplation and prognostic counselling interweave, diagnostics playing a subsidiary role in decision-oriented clinical practice. At times the doctor has grounds for withholding information. As this is known to the patient, prognostic counselling becomes a conflict-prone and rationality-thwarting activity. The meaning of standard phrases such as ``prognosis of a disease'', ``the prognosis of this patient'', ``the prognosis is unknown'', is examined.},
  langid = {english},
  keywords = {Clinical trial,Medical decision-making,Physician-patient relations,Professional jargon (medicine),Prognosis,Utility theory},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Hilden_Habbema_1987_Prognosis in medicine.pdf}
}

@misc{InstructionsAuthorsJAMA,
  title = {Instructions for {{Authors}} {\textbar} {{JAMA}} {\textbar} {{JAMA Network}}},
  urldate = {2021-08-03},
  howpublished = {https://jamanetwork.com/journals/jama/pages/instructions-for-authors},
  file = {/Users/wamster3/Zotero/storage/9AAPYFE3/for-authors.html}
}

@misc{kaplanScalingLawsNeural2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  number = {arXiv:2001.08361},
  eprint = {2001.08361},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.08361},
  urldate = {2024-03-01},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wamster3/Zotero/storage/3YLKWM3J/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf;/Users/wamster3/Zotero/storage/KPJE5YN3/2001.html}
}

@article{karmaliBloodPressureloweringTreatment2018,
  title = {Blood Pressure-Lowering Treatment Strategies Based on Cardiovascular Risk versus Blood Pressure: {{A}} Meta-Analysis of Individual Participant Data},
  shorttitle = {Blood Pressure-Lowering Treatment Strategies Based on Cardiovascular Risk versus Blood Pressure},
  author = {Karmali, Kunal N. and {Lloyd-Jones}, Donald M. and van der Leeuw, Joep and Jr, David C. Goff and Yusuf, Salim and Zanchetti, Alberto and Glasziou, Paul and Jackson, Rodney and Woodward, Mark and Rodgers, Anthony and Neal, Bruce C. and Berge, Eivind and Teo, Koon and Davis, Barry R. and Chalmers, John and Pepine, Carl and Rahimi, Kazem and Sundstr{\"o}m, Johan and Collaboration, on behalf of the Blood Pressure Lowering Treatment Trialists'},
  year = {2018},
  month = mar,
  journal = {PLOS Medicine},
  volume = {15},
  number = {3},
  pages = {e1002538},
  publisher = {Public Library of Science},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1002538},
  urldate = {2022-07-18},
  abstract = {Background Clinical practice guidelines have traditionally recommended blood pressure treatment based primarily on blood pressure thresholds. In contrast, using predicted cardiovascular risk has been advocated as a more effective strategy to guide treatment decisions for cardiovascular disease (CVD) prevention. We aimed to compare outcomes from a blood pressure-lowering treatment strategy based on predicted cardiovascular risk with one based on systolic blood pressure (SBP) level. Methods and findings We used individual participant data from the Blood Pressure Lowering Treatment Trialists' Collaboration (BPLTTC) from 1995 to 2013. Trials randomly assigned participants to either blood pressure-lowering drugs versus placebo or more intensive versus less intensive blood pressure-lowering regimens. We estimated 5-y risk of CVD events using a multivariable Weibull model previously developed in this dataset. We compared the two strategies at specific SBP thresholds and across the spectrum of risk and blood pressure levels studied in BPLTTC trials. The primary outcome was number of CVD events avoided per persons treated. We included data from 11 trials (47,872 participants). During a median of 4.0 y of follow-up, 3,566 participants (7.5\%) experienced a major cardiovascular event. Areas under the curve comparing the two treatment strategies throughout the range of possible thresholds for CVD risk and SBP demonstrated that, on average, a greater number of CVD events would be avoided for a given number of persons treated with the CVD risk strategy compared with the SBP strategy (area under the curve 0.71 [95\% confidence interval (CI) 0.70--0.72] for the CVD risk strategy versus 0.54 [95\% CI 0.53--0.55] for the SBP strategy). Compared with treating everyone with SBP {$\geq$} 150 mmHg, a CVD risk strategy would require treatment of 29\% (95\% CI 26\%--31\%) fewer persons to prevent the same number of events or would prevent 16\% (95\% CI 14\%--18\%) more events for the same number of persons treated. Compared with treating everyone with SBP {$\geq$} 140 mmHg, a CVD risk strategy would require treatment of 3.8\% (95\% CI 12.5\% fewer to 7.2\% more) fewer persons to prevent the same number of events or would prevent 3.1\% (95\% CI 1.5\%--5.0\%) more events for the same number of persons treated, although the former estimate was not statistically significant. In subgroup analyses, the CVD risk strategy did not appear to be more beneficial than the SBP strategy in patients with diabetes mellitus or established CVD. Conclusions A blood pressure-lowering treatment strategy based on predicted cardiovascular risk is more effective than one based on blood pressure levels alone across a range of thresholds. These results support using cardiovascular risk assessment to guide blood pressure treatment decision-making in moderate- to high-risk individuals, particularly for primary prevention.},
  langid = {english},
  keywords = {ACE inhibitor therapy,Aged,Antihypertensive Agents,Blood pressure,Blood Pressure,Blood Pressure Determination,Cardiovascular disease risk,Cardiovascular diseases,Cardiovascular Diseases,Cardiovascular therapy,Diabetes mellitus,Female,Humans,Hypertension,Kaplan-Meier Estimate,Male,Medical risk factors,Middle Aged,Practice Guidelines as Topic,Primary Prevention,Randomized Controlled Trials as Topic,Risk Assessment,Risk Factors,Stroke,Treatment guidelines,Treatment Outcome},
  file = {/Users/wamster3/Zotero/storage/3BW5G5YX/Karmali et al. - 2018 - Blood pressure-lowering treatment strategies based.pdf;/Users/wamster3/Zotero/storage/4ZQQ93CA/Karmali et al. - 2018 - Blood pressure-lowering treatment strategies based.pdf;/Users/wamster3/Zotero/storage/5RZ24D58/article.html}
}

@misc{keoghPredictionInterventionsEvaluation2024,
  title = {Prediction under Interventions: Evaluation of Counterfactual Performance Using Longitudinal Observational Data},
  shorttitle = {Prediction under Interventions},
  author = {Keogh, Ruth H. and {van Geloven}, Nan},
  year = {2024},
  month = jan,
  number = {arXiv:2304.10005},
  eprint = {2304.10005},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.10005},
  urldate = {2024-03-30},
  abstract = {Predictions under interventions are estimates of what a person's risk of an outcome would be if they were to follow a particular treatment strategy, given their individual characteristics. Such predictions can give important input to medical decision making. However, evaluating predictive performance of interventional predictions is challenging. Standard ways of evaluating predictive performance do not apply when using observational data, because prediction under interventions involves obtaining predictions of the outcome under conditions that are different to those that are observed for a subset of individuals in the validation dataset. This work describes methods for evaluating counterfactual performance of predictions under interventions for time-to-event outcomes. This means we aim to assess how well predictions would match the validation data if all individuals had followed the treatment strategy under which predictions are made. We focus on counterfactual performance evaluation using longitudinal observational data, and under treatment strategies that involve sustaining a particular treatment regime over time. We introduce an estimation approach using artificial censoring and inverse probability weighting which involves creating a validation dataset that mimics the treatment strategy under which predictions are made. We extend measures of calibration, discrimination (c-index and cumulative/dynamic AUCt) and overall prediction error (Brier score) to allow assessment of counterfactual performance. The methods are evaluated using a simulation study, including scenarios in which the methods should detect poor performance. Applying our methods in the context of liver transplantation shows that our procedure allows quantification of the performance of predictions supporting crucial decisions on organ allocation.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Keogh_van Geloven_2024_Prediction under interventions.pdf;/Users/wamster3/Zotero/storage/PAGIH8KE/2304.html}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {Nov./1998},
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {00189219},
  doi = {10.1109/5.726791},
  urldate = {2024-09-20},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@article{miaoIdentifyingCausalEffects2018,
  title = {Identifying Causal Effects with Proxy Variables of an Unmeasured Confounder},
  author = {Miao, Wang and Geng, Zhi and Tchetgen Tchetgen, Eric J},
  year = {2018},
  month = dec,
  journal = {Biometrika},
  volume = {105},
  number = {4},
  pages = {987--993},
  issn = {0006-3444},
  doi = {10.1093/biomet/asy038},
  urldate = {2023-07-05},
  abstract = {We consider a causal effect that is confounded by an unobserved variable, but for which observed proxy variables of the confounder are available. We show that with at least two independent proxy variables satisfying a certain rank condition, the causal effect can be nonparametrically identified, even if the measurement error mechanism, i.e., the conditional distribution of the proxies given the confounder, may not be identified. Our result generalizes the identification strategy of Kuroki \&amp; Pearl (2014), which rests on identification of the measurement error mechanism. When only one proxy for the confounder is available, or when the required rank condition is not met, we develop a strategy for testing the null hypothesis of no causal effect.},
  file = {/Users/wamster3/Zotero/storage/X2S2THRS/Miao et al. - 2018 - Identifying causal effects with proxy variables of.pdf;/Users/wamster3/Zotero/storage/5W35LV8E/5073056.html}
}

@article{moenDeepLearningCellular2019,
  title = {Deep Learning for Cellular Image Analysis},
  author = {Moen, Erick and Bannon, Dylan and Kudo, Takamasa and Graf, William and Covert, Markus and Van Valen, David},
  year = {2019},
  month = dec,
  journal = {Nature Methods},
  volume = {16},
  number = {12},
  pages = {1233--1246},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-019-0403-1},
  urldate = {2024-09-25},
  langid = {english},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Moen et al. - 2019 - Deep learning for cellular image analysis.pdf}
}

@book{pearlBookWhyNew2018,
  title = {The {{Book}} of {{Why}}: {{The New Science}} of {{Cause}} and {{Effect}}},
  shorttitle = {The {{Book}} of {{Why}}},
  author = {Pearl, Judea and Mackenzie, Dana},
  year = {2018},
  month = may,
  edition = {1st edition},
  publisher = {Basic Books},
  address = {New York},
  abstract = {A Turing Award-winning computer scientist and statistician shows how understanding causality has revolutionized science and will revolutionize artificial intelligence "Correlation is not causation." This mantra, chanted by scientists for more than a century, has led to a virtual prohibition on causal talk. Today, that taboo is dead. The causal revolution, instigated by Judea Pearl and his colleagues, has cut through a century of confusion and established causality -- the study of cause and effect -- on a firm scientific basis. His work explains how we can know easy things, like whether it was rain or a sprinkler that made a sidewalk wet; and how to answer hard questions, like whether a drug cured an illness. Pearl's work enables us to know not just whether one thing causes another: it lets us explore the world that is and the worlds that could have been. It shows us the essence of human thought and key to artificial intelligence. Anyone who wants to understand either needs The Book of Why.},
  isbn = {978-0-465-09760-9},
  langid = {english}
}

@book{pearlCausality2009,
  title = {Causality},
  author = {Pearl, Judea},
  year = {2009},
  month = sep,
  publisher = {Cambridge University Press},
  abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 5,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.},
  langid = {english}
}

@misc{pearlThreeLayerCausal,
  title = {The {{Three Layer Causal Hierarchy}}},
  author = {Pearl, Judea},
  urldate = {2024-11-04},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/3-layer-causal-hierarchy.pdf}
}

@article{puliGeneralControlFunctions2021,
  title = {General {{Control Functions}} for {{Causal Effect Estimation}} from {{Instrumental Variables}}},
  author = {Puli, Aahlad Manas and Ranganath, Rajesh},
  year = {2021},
  month = feb,
  journal = {arXiv:1907.03451 [cs, stat]},
  eprint = {1907.03451},
  primaryclass = {cs, stat},
  urldate = {2021-09-18},
  abstract = {Causal effect estimation relies on separating the variation in the outcome into parts due to the treatment and due to the confounders. To achieve this separation, practitioners often use external sources of randomness that only influence the treatment called instrumental variables (IVs). We study variables constructed from treatment and IV that help estimate effects, called control functions. We characterize general control functions for effect estimation in a meta-identification result. Then, we show that structural assumptions on the treatment process allow the construction of general control functions, thereby guaranteeing identification. To construct general control functions and estimate effects, we develop the general control function method (GCFN). GCFN's first stage called variational decoupling (VDE) constructs general control functions by recovering the residual variation in the treatment given the IV. Using VDE's control function, GCFN's second stage estimates effects via regression. Further, we develop semi-supervised GCFN to construct general control functions using subsets of data that have both IV and confounders observed as supervision; this needs no structural treatment process assumptions. We evaluate GCFN on low and high dimensional simulated data and on recovering the causal effect of slave export on modern community trust.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wamster3/Zotero/storage/GQMQW3N9/Puli and Ranganath - 2021 - General Control Functions for Causal Effect Estima.pdf;/Users/wamster3/Zotero/storage/L7MP6EUJ/1907.html}
}

@book{russellArtificialIntelligenceModern2020,
  title = {Artificial {{Intelligence}}: {{A Modern Approach}}},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Russell, Stuart and Norvig, Peter},
  year = {2020},
  month = apr,
  publisher = {Pearson},
  abstract = {The most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence  The long-anticipated revision of  Artificial Intelligence: A Modern Approach  explores the full breadth and depth of the field of artificial intelligence (AI). The 4th Edition brings readers up to date on the latest technologies, presents concepts in a more unified manner, and offers new or expanded coverage of machine learning, deep learning, transfer learning, multiagent systems, robotics, natural language processing, causality, probabilistic programming, privacy, fairness, and safe AI.},
  googlebooks = {koFptAEACAAJ},
  isbn = {978-0-13-461099-3},
  langid = {english}
}

@article{samuelStudiesMachineLearning1959,
  title = {Some {{Studies}} in {{Machine Learning Using}} the {{Game}} of {{Checkers}}},
  author = {Samuel, A. L.},
  year = {1959},
  month = jul,
  journal = {IBM Journal of Research and Development},
  volume = {3},
  number = {3},
  pages = {210--229},
  issn = {0018-8646, 0018-8646},
  doi = {10.1147/rd.33.0210},
  urldate = {2024-09-20},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Samuel - 1959 - Some Studies in Machine Learning Using the Game of Checkers.pdf}
}

@article{selBuildingDigitalTwins2024,
  title = {Building {{Digital Twins}} for {{Cardiovascular Health}}: {{From Principles}} to {{Clinical Impact}}},
  shorttitle = {Building {{Digital Twins}} for {{Cardiovascular Health}}},
  author = {Sel, Kaan and Osman, Deen and Zare, Fatemeh and Masoumi Shahrbabak, Sina and Brattain, Laura and Hahn, Jin-Oh and Inan, Omer T. and Mukkamala, Ramakrishna and Palmer, Jeffrey and Paydarfar, David and Pettigrew, Roderic I. and Quyyumi, Arshed A. and Telfer, Brian and Jafari, Roozbeh},
  year = {2024},
  month = oct,
  journal = {Journal of the American Heart Association},
  volume = {13},
  number = {19},
  pages = {e031981},
  issn = {2047-9980},
  doi = {10.1161/JAHA.123.031981},
  urldate = {2024-11-06},
  abstract = {The past several decades have seen rapid advances in diagnosis and treatment of cardiovascular diseases and stroke, enabled by technological breakthroughs in imaging, genomics, and physiological monitoring, coupled with therapeutic interventions. We now face the challenge of how to (1) rapidly process large, complex multimodal and multiscale medical measurements; (2) map all available data streams to the trajectories of disease states over the patient's lifetime; and (3) apply this information for optimal clinical interventions and outcomes. Here we review new advances that may address these challenges using digital twin technology to fulfill the promise of personalized cardiovascular medical practice. Rooted in engineering mechanics and manufacturing, the digital twin is a virtual representation engineered to model and simulate its physical counterpart. Recent breakthroughs in scientific computation, artificial intelligence, and sensor technology have enabled rapid bidirectional interactions between the virtual-physical counterparts with measurements of the physical twin that inform and improve its virtual twin, which in turn provide updated virtual projections of disease trajectories and anticipated clinical outcomes. Verification, validation, and uncertainty quantification builds confidence and trust by clinicians and patients in the digital twin and establishes boundaries for the use of simulations in cardiovascular medicine. Mechanistic physiological models form the fundamental building blocks of the personalized digital twin that continuously forecast optimal management of cardiovascular health using individualized data streams. We present exemplars from the existing body of literature pertaining to mechanistic model development for cardiovascular dynamics and summarize existing technical challenges and opportunities pertaining to the foundation of a digital twin.},
  langid = {english}
}

@article{shalitEstimatingIndividualTreatment2017,
  title = {Estimating Individual Treatment Effect: Generalization Bounds and Algorithms},
  shorttitle = {Estimating Individual Treatment Effect},
  author = {Shalit, Uri and Johansson, Fredrik D. and Sontag, David},
  year = {2017},
  month = may,
  journal = {arXiv:1606.03976 [cs, stat]},
  eprint = {1606.03976},
  primaryclass = {cs, stat},
  urldate = {2021-09-23},
  abstract = {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a "balanced" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/wamster3/Zotero/storage/Z6J6KCXQ/Shalit et al. - 2017 - Estimating individual treatment effect generaliza.pdf;/Users/wamster3/Zotero/storage/QHQW4T9B/1606.html}
}

@article{vanamsterdamAlgorithmsActionImproving2024,
  title = {From Algorithms to Action: Improving Patient Care Requires Causality},
  shorttitle = {From Algorithms to Action},
  author = {{van Amsterdam}, W.A.C. and de Jong, Pim A. and Verhoeff, Joost J. C. and Leiner, Tim and Ranganath, Rajesh},
  year = {2024},
  journal = {BMC Medical Informatics and Decision Making},
  volume = {24},
  number = {1},
  doi = {10.1186/s12911-024-02513-3},
  urldate = {2024-04-26},
  abstract = {In cancer research there is much interest in building and validating outcome prediction models to support treatment decisions. However, because most outcome prediction models are developed and validated without regard to the causal aspects of treatment decision making, many published outcome prediction models may cause harm when used for decision making, despite being found accurate in validation studies. Guidelines on prediction model validation and the checklist for risk model endorsement by the American Joint Committee on Cancer do not protect against prediction models that are accurate during development and validation but harmful when used for decision making. We explain why this is the case and how to build and validate models that are useful for decision making.},
  copyright = {All rights reserved},
  langid = {english},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Amsterdam et al_2024_From algorithms to action.pdf;/Users/wamster3/Zotero/storage/3AK84HYV/10.html}
}

@article{vanamsterdamConditionalAverageTreatment2023,
  title = {Conditional Average Treatment Effect Estimation with Marginally Constrained Models},
  author = {{van Amsterdam}, Wouter A. C. and Ranganath, Rajesh},
  year = {2023},
  month = aug,
  journal = {Journal of Causal Inference},
  volume = {11},
  number = {1},
  pages = {20220027},
  issn = {2193-3685},
  doi = {10.1515/jci-2022-0027},
  urldate = {2023-09-11},
  abstract = {Abstract                            Treatment effect estimates are often available from randomized controlled trials as a single               average treatment effect               for a certain patient population. Estimates of the               conditional average treatment effect               (CATE) are more useful for individualized treatment decision-making, but randomized trials are often too small to estimate the CATE. Examples in medical literature make use of the               relative               treatment effect (e.g. an odds ratio) reported by randomized trials to estimate the CATE using large observational datasets. One approach to estimating these CATE models is by using the relative treatment effect as an               offset               , while estimating the covariate-specific untreated risk. We observe that the odds ratios reported in randomized controlled trials are not the odds ratios that are needed in offset models because trials often report the               marginal               odds ratio. We introduce a constraint or a regularizer to better use marginal odds ratios from randomized controlled trials and find that under the standard observational causal inference assumptions, this approach provides a consistent estimate of the CATE. Next, we show that the offset approach is not valid for CATE estimation in the presence of unobserved confounding. We study if the offset assumption and the marginal constraint lead to better approximations of the CATE relative to the alternative of using the average treatment effect estimate from the randomized trial. We empirically show that when the underlying CATE has sufficient variation, the constraint and offset approaches lead to closer approximations to the CATE.},
  copyright = {All rights reserved},
  langid = {english},
  file = {/Users/wamster3/Zotero/storage/2F3NJEFG/Van Amsterdam and Ranganath - 2023 - Conditional average treatment effect estimation wi.pdf}
}

@article{vanamsterdamIndividualTreatmentEffect2022,
  title = {Individual Treatment Effect Estimation in the Presence of Unobserved Confounding Using Proxies: A Cohort Study in Stage {{III}} Non-Small Cell Lung Cancer},
  shorttitle = {Individual Treatment Effect Estimation in the Presence of Unobserved Confounding Using Proxies},
  author = {{van Amsterdam}, Wouter A. C. and Verhoeff, Joost J. C. and Harlianto, Netanja I. and Bartholomeus, Gijs A. and Puli, Aahlad Manas and {de Jong}, Pim A. and Leiner, Tim and {van Lindert}, Anne S. R. and Eijkemans, Marinus J. C. and Ranganath, Rajesh},
  year = {2022},
  month = apr,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {5848},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-09775-9},
  urldate = {2022-04-12},
  abstract = {Randomized Controlled Trials (RCT) are the gold standard for estimating treatment effects but some important situations in cancer care require treatment effect estimates from observational data. We developed ``Proxy based individual treatment effect modeling in cancer'' (PROTECT) to estimate treatment effects from observational data when there are unobserved confounders, but proxy measurements of these confounders exist. We identified an unobserved confounder in observational cancer research: overall fitness. Proxy measurements of overall fitness exist like performance score, but the fitness as observed by the treating physician is unavailable for research. PROTECT reconstructs the distribution of the unobserved confounder based on these proxy measurements to estimate the treatment effect. PROTECT was applied to an observational cohort of 504 stage III non-small cell lung cancer (NSCLC) patients, treated with concurrent chemoradiation or sequential chemoradiation. Whereas conventional confounding adjustment methods seemed to overestimate the treatment effect, PROTECT provided credible treatment effect estimates.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Lung cancer,Outcomes research,Predictive markers,Prognostic markers,Statistics},
  file = {/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/van Amsterdam et al. - 2022 - Individual treatment effect estimation in the presence of unobserved confounding using proxies a co.pdf;/Users/wamster3/Zotero/storage/D995HPCL/s41598-022-09775-9.html}
}

@misc{vanamsterdamWhenAccuratePrediction2024,
  title = {When Accurate Prediction Models Yield Harmful Self-Fulfilling Prophecies},
  author = {{van Amsterdam}, W.A.C. and {van Geloven}, Nan and Krijthe, Jesse H. and Ranganath, Rajesh and Cin{\'a}, Giovanni},
  year = {2024},
  month = feb,
  number = {arXiv:2312.01210},
  eprint = {2312.01210},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.01210},
  urldate = {2024-03-02},
  abstract = {Objective: Prediction models are popular in medical research and practice. By predicting an outcome of interest for specific patients, these models may help inform difficult treatment decisions, and are often hailed as the poster children for personalized, data-driven healthcare. Many prediction models are deployed for decision support based on their prediction accuracy in validation studies. We investigate whether this is a safe and valid approach. Materials and Methods: We show that using prediction models for decision making can lead to harmful decisions, even when the predictions exhibit good discrimination after deployment. These models are harmful self-fulfilling prophecies: their deployment harms a group of patients but the worse outcome of these patients does not invalidate the predictive power of the model. Results: Our main result is a formal characterization of a set of such prediction models. Next we show that models that are well calibrated before and after deployment are useless for decision making as they made no change in the data distribution. Discussion: Our results point to the need to revise standard practices for validation, deployment and evaluation of prediction models that are used in medical decisions. Conclusion: Outcome prediction models can yield harmful self-fulfilling prophecies when used for decision making, a new perspective on prediction model development, deployment and monitoring is needed.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/wamster3/Zotero/storage/J67PUPJZ/van Amsterdam et al. - 2024 - When accurate prediction models yield harmful self.pdf;/Users/wamster3/Zotero/storage/35JQRZ6M/2312.html}
}

@misc{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-25},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{waldFittingStraightLines1940,
  title = {The {{Fitting}} of {{Straight Lines}} If {{Both Variables}} Are {{Subject}} to {{Error}}},
  author = {Wald, Abraham},
  year = {1940},
  month = sep,
  journal = {The Annals of Mathematical Statistics},
  volume = {11},
  number = {3},
  pages = {284--300},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177731868},
  urldate = {2021-09-18},
  abstract = {The Annals of Mathematical Statistics},
  file = {/Users/wamster3/Zotero/storage/P7R6X5JS/Wald - 1940 - The Fitting of Straight Lines if Both Variables ar.pdf;/Users/wamster3/Zotero/storage/RUANYBXX/1177731868.html}
}

@article{wangMatchingConditionalMarginal2003,
  title = {Matching Conditional and Marginal Shapes in Binary Random Intercept Models Using a Bridge Distribution Function},
  author = {Wang, Zengri and Louis, Thomas A.},
  year = {2003},
  month = dec,
  journal = {Biometrika},
  volume = {90},
  number = {4},
  pages = {765--775},
  issn = {0006-3444},
  doi = {10.1093/biomet/90.4.765},
  urldate = {2024-08-23},
  abstract = {Random effects logistic regression models are often used to model clustered binary response data. Regression parameters in these models have a conditional, subject-specific interpretation in that they quantify regression effects for each cluster. Very often, the logistic functional shape conditional on the random effects does not carry over to the marginal scale. Thus, parameters in these models usually do not have an explicit marginal, population-averaged interpretation. We study a bridge distribution function for the random effect in the random intercept logistic regression model. Under this distributional assumption, the marginal functional shape is still of logistic form, and thus regression parameters have an explicit marginal interpretation. The main advantage of this approach is that likelihood inference can be obtained for either marginal or conditional regression inference within a single model framework. The generality of the results and some properties of the bridge distribution functions are discussed. An example is used for illustration.},
  file = {/Users/wamster3/Zotero/storage/NJ4H6LYQ/Wang and Louis - 2003 - Matching conditional and marginal shapes in binary random intercept models using a bridge distributi.pdf;/Users/wamster3/Zotero/storage/9TUNYDTC/256628.html}
}

@article{xuPredictionCardiovascularDisease2021,
  title = {Prediction of {{Cardiovascular Disease Risk Accounting}} for {{Future Initiation}} of {{Statin Treatment}}},
  author = {Xu, Zhe and Arnold, Matthew and Stevens, David and Kaptoge, Stephen and Pennells, Lisa and Sweeting, Michael J and Barrett, Jessica and Di Angelantonio, Emanuele and Wood, Angela M},
  year = {2021},
  month = oct,
  journal = {American Journal of Epidemiology},
  volume = {190},
  number = {10},
  pages = {2000--2014},
  issn = {0002-9262, 1476-6256},
  doi = {10.1093/aje/kwab031},
  urldate = {2023-08-12},
  abstract = {Abstract             Cardiovascular disease (CVD) risk-prediction models are used to identify high-risk individuals and guide statin initiation. However, these models are usually derived from individuals who might initiate statins during follow-up. We present a simple approach to address statin initiation to predict ``statin-naive'' CVD risk. We analyzed primary care data (2004--2017) from the UK Clinical Practice Research Datalink for 1,678,727 individuals (aged 40--85 years) without CVD or statin treatment history at study entry. We derived age- and sex-specific prediction models including conventional risk factors and a time-dependent effect of statin initiation constrained to 25\% risk reduction (from trial results). We compared predictive performance and measures of public-health impact (e.g., number needed to screen to prevent 1 event) against models ignoring statin initiation. During a median follow-up of 8.9 years, 103,163 individuals developed CVD. In models accounting for (versus ignoring) statin initiation, 10-year CVD risk predictions were slightly higher; predictive performance was moderately improved. However, few individuals were reclassified to a high-risk threshold, resulting in negligible improvements in number needed to screen to prevent 1 event. In conclusion, incorporating statin effects from trial results into risk-prediction models enables statin-naive CVD risk estimation and provides moderate gains in predictive ability but had a limited impact on treatment decision-making under current guidelines in this population.},
  langid = {english},
  file = {/Users/wamster3/Zotero/storage/9D4DB9PI/Xu et al. - 2021 - Prediction of Cardiovascular Disease Risk Accounti.pdf}
}
