---
title: The need for speed, performing simulation studies in R, JAX and Julia
bibliography: references.bib
eval: false
draft: false
categories:
- r
- julia
- jax
- python
---


When using simulations to support scientific claims, the more experiments the better.
Having the ability to perform many experiments fast allows researchers to:

- test bigger (or finer) experimental grids
- attain lower variance by having more repeated experiments
- test new ideas faster

The R language has been a popular language among many biostatisticians for a long time, but it is not generally considered the top performing language in terms of speed, especially when it comes to leveraging GPUs for calculations that are amenable to vectorization (such as performing simulation studies across a large grid).
In recent years, [JAX](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html) (developed by Google) and [Julia](https://julialang.org) have arisen as general scientific computation frameworks.
JAX and Julia have grown in popularity both in the neural network community as in other scientific communities [e.g. @DifferentiableUniverseInitiativeJax_cosmo2024; @SciMLOpenSource]
In this blog post I'll compare R with JAX and Julia for a simple simulation study setup with logistic regression.

We'll look at the following comparisons:

1. R single thread
2. R multi-threaded
3. JAX on CPU
4. Julia single thread
5. Julia multi-threading

## R vs JAX vs Julia a high level overview
Without going in too much details, JAX works by translating python code into an intermediate language that can be run very efficiently on different hardware backends (CPU, GPU, TPU) through just-in-time compilation (JIT).
JAX prides itself on providing composable transformations for vectorization (`vmap`), paralellization (`pmap`) and automatic differentiation (`grad`), all compatible with `jit`
In R, most of the heavy lifting in terms of computation (such as fitting a logistic regression model) is implemented in high-speed languages such as C++ or Fortran.
The usual R-code merely provides an interface to these languages and allows the user to feed in data and analyze results.
Whereas using JAX and R means working with two languages (one language to write accessible code, another to do fast computation), Julia is a just-in-time compiled language where such translation is not needed.

## The basic setup

We'll be running a simple logistic regression simulation setup, where for each observation:

$$
\begin{align}
\mathbf{x}_{\text{full}} &\sim \mathcal{N}(0,I) \in \mathbb{R}^{10} \\
y &= ||\mathbf{x}||_0 > 0 \\
\mathbf{x}_{\text{obs}} &= [x_0\ x_i \ldots x_9]
\end{align}
$$

So $y$ is the sum of elements of $\mathbf{x}_{\text{full}}$ and the observed $\mathbf{x}_{\text{obs}}$ contains only the first 9 out of 10 elements of $\mathbf{x}_{\text{full}}$.

We will model this data with logistic regression:

$$
\text{logit}(y) = \mathbf{x}_{\text{obs}} \boldsymbol{\beta}' 
$$

where $\boldsymbol{\beta} = [\beta_1,\ldots,\beta_9]$ is a 9-dimensional parameter vector that is to be estimated (we're excluding the usual intercept term).
We'll generate `nrep` independent datasets and estimate $\boldsymbol{\beta}$ in each one, and finally calculate the average parameter estimates $\frac{1}{\text{nrep}}\sum_{i=1}^{\text{nrep}}\boldsymbol{\beta}^i$, averaged over each repetition i.

## The code

Making the data is pretty similar in all cases, except that Jax requires an explicit random key.

::: {.panel-tabset}
## R

``` {.r}
make_data <- function(n=1e3L) {
    x_vec = rnorm(n*10)
    X_full = matrix(x_vec, ncol=10)
    eta = rowSums(X_full)
    y = eta > 0
    # return only first 9 column to have some noise
    X = X_full[,1:9]
    return(list(X=X,y=y))
}
```

## Python

``` {.python}
def make_data(k, n=int(1e3)):
    X_full = random.normal(k, (n,10)) # JAX needs explicit keys for psuedo random number generation
    eta = jnp.sum(X_full, axis=-1)
    y = eta > 0
    # return only first 9 column to have some noise
    X = X_full[:,:9]
    return (X, y)
```

## Julia

``` {.julia}
function make_data(n::Integer=1000)
    X_full = randn(n,10)
    eta = vec(sum(X_full, dims=2))
    y = eta .> 0 # vectorized greater than 0 comparison
    X = X_full[:,1:9]
    return X, y
end
```

:::

Now we'll write the code for a single analysis step, generating data and fitting the logistic regression.
For R and Julia we will use the `glm` function to estimate the logistic regression model.
The Julia code looks much like the R code
In JAX there is no `glm` function.
Instead, we need to specify an objective function and will use a general purpose optimizer.
[JaxOpt](https://jaxopt.github.io/) provides both `binary_logreg` as an objective function and `LBFGS`, a popular general purpose optimizer, which we'll use here.

::: {.panel-tabset}
## R

``` {.r}
solve <- function(...) {
  data = make_data()
  fit = glm(data$y~data$X-1, family='binomial')
  coefs = coef(fit)
  return(coefs)
}
```

## Python

``` {.python}
# initialize a generic solver with the correct objective function
solver = LBFGS(binary_logreg)
w_init = jnp.zeros((9,))

@jit # jit toggles just-in-time compilation, one of the main features of JAX
def solve(k):
    # need to specify parameter initialization values
    data = make_data(k)
    param, state = solver.run(w_init, data)
    return param
```

## Julia

``` {.julia}
function solve(i::Int64=1)
    X, y = make_data()
    fit = glm(X, y, Bernoulli())
    coefs = coef(fit)
    return coefs
end
```

:::

Finally we run the experiments `nrep` times and calculate the average coefficient vector

::: {.panel-tabset}
## R

``` {.r}
set.seed(240316)
params <- lapply(1:nreps, solve)
outmat <- do.call(rbind, params)
means <- colMeans(outmat)
print(means)
```

## Python

``` {.python}
k0 = random.PRNGKey(240316)
ks = random.split(k, nreps)
params = vmap(solve)(ks) # vmap vectorizes computations
means = jnp.mean(params, axis=0)
print(means)
```

## Julia

``` {.julia}
Random.seed!(240316)
outmat = zeros(nreps, 9)

@threads for i in 1:nreps # use @threads for multi-threading
    solution = solve()
    outmat[i,:] = solve()
end

means = mean(outmat, dims=1)
print(means)

```

:::

I benchmarked each run with an external `time` command in Bash or ZSH and wrote the results to a file.
See [runexps.sh](runexps.sh)

## The speed

First, let's see how running time increases with 

```{r}
#| label: fig-times
#| eval: true
#| echo: false

suppressMessages(library(data.table))
suppressMessages(library(purrr))
library(stringr)
library(ggplot2); theme_set(theme_bw())
library(knitr)

lns <- readr::read_lines('timings.txt')

timings <- data.table(raw_string=lns)
timings[, string:=str_trim(raw_string)] # remove white space
timings <- timings[str_starts(string, "Rscript|julia|python")] # remove warings / errors
timings[, command:=word(string)]
timings[command=='Rscript', language:='r']
timings[command=='python', language:='jax']
timings[command=='julia', language:='julia']
timings[, time_str:=str_extract(string, "(?<=cpu\\ )(.*)(?= total)")]
timings[, milliseconds:=as.integer(str_extract(time_str, "(\\d+)$"))]
timings[, seconds     :=as.integer(str_extract(time_str, "(\\d+)(?=.)"))]
timings[, minutes     :=as.integer(str_extract(time_str, "(\\d+)(?=:)"))]
timings[is.na(minutes), minutes:=0]
timings[, jax_primitive:='map']
timings[, machine:='macm1']
timings[, nthreads:=as.integer(str_extract(string, "(\\d+)(?= nthreads)"))]
timings[, max_threads:=max(nthreads, na.rm=T)]
timings[is.na(nthreads) & language == 'jax', nthreads:=max_threads]
timings[is.na(nthreads) & language %in% c('r', 'julia'), nthreads:=1L]

# check if there are timings from bash
if (file.exists('bashtimings.txt')) {
# if (F) {
  blns <- readr::read_lines('bashtimings.txt')
  btimings <- data.table(raw_string=blns)
  btimings[, string:=str_trim(raw_string)] # remove white space
  btimings[, string:=str_replace(string, "real\t", "")]
  btimings[, minutes:=as.integer(str_extract(string, "^(\\d+)"))]
  btimings[, seconds:=as.integer(str_extract(string, "(?<=m)(\\d+)"))]
  btimings[, milliseconds:=as.integer(str_extract(string, "(?<=m)(\\d+)"))]
  btimings[, nthreads:=as.integer(str_extract(string, "(\\d+)(?= nthreads)"))]
  btimings[, language:=str_extract(string, "(?<=s )[a-z]+")]
  btimings[, jax_primitive:='vmap']
  btimings[, machine:='linux']
  timings <- rbindlist(list(timings, btimings
  ), fill=T)
}

timings[, nreps:=as.integer(str_extract(string, "(\\d+)(?= nreps)"))]
# max_threads <- timings[, max(nthreads, na.rm=T), by='machine']
timings[, max_threads:=max(nthreads), by='machine']

timings[, sec_total:=60*minutes + seconds + milliseconds / 1000]
timings[, min_total:=sec_total / 60]

# add some vars
timings[, n_per_min:=nreps / min_total]
timings[, log_n_per_min:=log(n_per_min)]

timings <- timings[n_per_min < 1e6]

ggplot(timings[nthreads==max_threads], aes(x=nreps, y=min_total, col=language)) +
  geom_point() + geom_line() + 
  scale_x_log10() + scale_y_log10() + 
  ggtitle(paste0("time to run experiments with max threads")) +
  facet_grid(machine+nthreads~., labeller='label_both')
```

Something quirky is happening here, Jax takes less time to complete 10e5 experiments than 10e6 experiments.
This indicates an error: out of memory.

```{r}
#| label: fig-speeds
#| eval: true
#| echo: false

ggplot(timings[nthreads==max_threads], aes(x=nreps, y=n_per_min, col=language)) +
  geom_point() + geom_line() + 
  scale_x_log10() + scale_y_log10() + 
  ggtitle(paste0("experiments per minute with max threads")) +
  facet_grid(machine+nthreads~., labeller='label_both')
```

Now let's check how the number of threads matter for R and julia

```{r}
#| label: fig-threads
#| eval: true
#| echo: false

ggplot(timings[language!='jax'], aes(x=nreps, y=n_per_min, col=language)) +
  geom_point() + geom_line(aes(linetype=factor(nthreads))) + 
  scale_x_log10() + scale_y_log10() + 
  facet_grid(machine~language) +
  # facet_grid(~language) +
  ggtitle("time to run experiments")
```

Let's see the top speeds per language

```{r}
#| label: tbl-best-per-language
#| eval: true
#| echo: false

timings[order(-n_per_min), lang_order:=1:.N, by=c('language', 'machine')]
tabcols <- c('nreps', 'nthreads', 'min_total', 'n_per_min', 'language', 'machine')
kable(timings[lang_order==1, .SD, .SDcols=tabcols])
```

Let's see the top performing settings, also compared to the best R setting

```{r}
#| label: tbl-topspeeds
#| eval: true
#| echo: false

# best_r_speed <- as.numeric(timings[lang_order==1&language=='r', n_per_min])
best_r_speeds <- timings[lang_order==1&language=='r', list(n_per_min, machine)]
timings[best_r_speeds, vs_r_speedup:=n_per_min/i.n_per_min, on='machine']
tabcols <- c(tabcols, 'vs_r_speedup')
kable(timings[order(-n_per_min), .SD, .SDcols=tabcols][1:10])
```

All results:

```{r}
#| label: tbl-speeds
#| eval: true
#| echo: false

kable(timings[, .SD, .SDcols=tabcols])
```

Speed plots

## Conclusions

assuming familiarity R > Julia > JAX

Dimensions:

- have GPU -- have many threads
- rely on standard stats / need summary stats -- have custom objectives / need autograd



## Discussion

JAX was able to get a 86 times speed-up compared to single-threaded R, and 8 times compared to R on 10 threads.
JAX does come with some 'sharp bits' (see link) that if not followed correctly lead to suboptimal performance and/or unexpected behavior, e.g. it requires functional programming.

Note that JAX-opt gives pretty-bare bones results with only parameters, likelihood of the observed data at the final parameter estimate and some optimization statistics (number of iterations, convergence).
But not e.g. variance estimates and all sorts of other things returned by glm in R.

In conculsion, should you learn JAX for simulations? 
- If you can program in python / numpy, yes
- If you'll write custom objectives and want to apply gradient-based optimization, yes.
- If you don't have a GPU but have many CPU-threads available, probably no

Finally, when it comes to analyzing results from millions of simulation experiments, R's data.table really shines when it comes to speed in reading, writing, subsetting and analyzing results.
It's orders of magnitudes faster than pythons pandas for many tasks and pretty much the fastest framework for in-memory data.frame-style operations across all languages.

My workflow:
1. run simulations in JAX
2. analyze results in R using data.table

### Limitations

Cannot easily set number of threads in JAX (see e.g. [this](https://github.com/google/jax/issues/1539) issue on github).

## Extensions

Julia is another JIT-compiled language that would be interesting to compare against.


## Computing environment

All tests conducted on linux cluster with 12 threads and a NVIDIA Quadro P-6000 GPU


## Full scripts

::: {.panel-tabset}
## R

``` {.r}
{{< include scripts/rspeed.R >}}
```

## Python

``` {.python}
{{< include scripts/jaxspeed.py >}}
```

## Julia

``` {.julia}
{{< include scripts/jlspeed.jl >}}
```
:::



## References

::: {#refs}
:::


