---
title: The need for speed, performing simulation studies in R, JAX and Julia
bibliography: references.bib
eval: false
draft: false
categories:
- r
- julia
- jax
- python
- simulation studies
---

Simulation experiments are important when evaluating methods but also for applied work in for example power analyses [e.g. @vanamsterdamAssociationMuscleQuantity2022] or sensitivity analyses [e.g. @vanamsterdamIndividualTreatmentEffect2022].
When using simulations to support scientific claims, the more experiments the better.
Being able to perform simulation experiments faster allows researchers to:

- test bigger (or finer) experimental grids
- attain lower variance by having more repeated experiments
- test new ideas faster

The R language has been a popular language among many biostatisticians for a long time, but it is not generally considered the top performing language in terms of speed.
<!--, especially when it comes to leveraging GPUs for calculations that are amenable to vectorization (such as performing simulation studies across a large grid).-->
In recent years, [JAX](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html) (developed by Google) and [Julia](https://julialang.org) have arisen as general scientific computation frameworks.
JAX and Julia have grown in popularity both in the neural network community as in other scientific communities [e.g. @DifferentiableUniverseInitiativeJax_cosmo2024; @SciMLOpenSource].
In this blog post I'll compare R with JAX and Julia for a simple simulation study setup with logistic regression.

We'll look at the following comparisons:

1. R single thread
2. R multi-threaded
3. JAX
4. Julia single thread
5. Julia multi-threading

## JAX and Julia vs R: a high level overview
JAX is a rising star in computer science and natural sciences.
Without going in too much details, JAX works by translating python code into an intermediate language that can be run very efficiently on different hardware backends (CPU, GPU, TPU), possibly with just-in-time compilation (JIT).
JAX prides itself on providing composable transformations for vectorization (`vmap`), paralellization (`pmap`) and automatic differentiation (`grad`), all compatible with `jit`.
In R, most of the heavy lifting in terms of computation (such as fitting a logistic regression model) is implemented in high-speed languages such as C++ or Fortran.
The usual R-code merely provides an interface to these languages and allows the user to feed in data and analyze results.
Whereas using JAX and R means working with two languages (one language to write accessible code, another to do fast computation), Julia is a just-in-time compiled language where such translation is not needed.

## The basic setup

We'll use a simple logistic regression simulation setup, where for each observation:

$$
\begin{align}
\mathbf{x}_{\text{full}} &\sim \mathcal{N}(0,I) \in \mathbb{R}^{10} \\
y &= ||\mathbf{x}||_0 > 0 \\
\mathbf{x}_{\text{obs}} &= [x_0\ x_i \ldots x_9]
\end{align}
$$

So $y$ is the sum of elements of $\mathbf{x}_{\text{full}}$ and the observed $\mathbf{x}_{\text{obs}}$ contains only the first 9 out of 10 elements of $\mathbf{x}_{\text{full}}$.

We will model this data with logistic regression:

$$
\begin{align}
    \text{logit}(y) &= \mathbf{x}_{\text{obs}} \boldsymbol{\beta}'\\
    y &\sim \text{Bernoulli} (\sigma (\mathbf{x}_{\text{obs}} \boldsymbol{\beta}'))
\end{align}
$$

where $\boldsymbol{\beta} = [\beta_1,\ldots,\beta_9]$ is a 9-dimensional parameter vector that is to be estimated (we're excluding the usual intercept term).
We'll generate `nrep` independent datasets and estimate $\boldsymbol{\beta}$ in each one, and finally calculate the average parameter estimates $\frac{1}{\text{nrep}}\sum_{i=1}^{\text{nrep}}\boldsymbol{\beta}^i$.

### Hardware

The hardware I had available for this comparison is:

- macm1: 2020 macbook air M1, 8Gb RAM, 8 threads
- linux: linux machine, 64Gb RAM, 12 threads (Intel(R) Xeon(R) W-2135 CPU @ 3.70GH )


## The code

### Making data

Making the data is pretty similar in all cases, except that JAX requires an explicit random key.

::: {.panel-tabset}
## R

``` {.r}
make_data <- function(n=1e3L) {
    x_vec = rnorm(n*10)
    X_full = matrix(x_vec, ncol=10)
    eta = rowSums(X_full)
    y = eta > 0
    # return only first 9 column to have some noise
    X = X_full[,1:9]
    return(list(X=X,y=y))
}
```

## Python

``` {.python}
def make_data(k, n=int(1e3)):
    X_full = random.normal(k, (n,10)) # JAX needs explicit keys for psuedo random number generation
    eta = jnp.sum(X_full, axis=-1)
    y = eta > 0
    # return only first 9 column to have some noise
    X = X_full[:,:9]
    return (X, y)
```

## Julia

``` {.julia}
function make_data(n::Integer=1000)
    X_full = randn(n,10)
    eta = vec(sum(X_full, dims=2))
    y = eta .> 0 # vectorized greater than 0 comparison
    X = X_full[:,1:9]
    return X, y
end
```

:::

### Run single experiment

Now we'll write the code for a single analysis step, generating data and fitting the logistic regression.
For R and Julia we will use the `glm` function to estimate the logistic regression model.
The Julia code looks much like the R code.
As far as I know there is no equivalent `glm` function implemented in JAX.
Instead, we need to specify an objective function and will use a general purpose optimizer.
[JaxOpt](https://jaxopt.github.io/) provides both `binary_logreg` as an objective function and `LBFGS`, a popular general purpose optimizer, which we'll use here.

::: {.panel-tabset}
## R

``` {.r}
solve <- function(...) {
  data = make_data()
  fit = glm(data$y~data$X-1, family='binomial')
  coefs = coef(fit)
  return(coefs)
}
```

## Python

``` {.python}
# initialize a generic solver with the correct objective function
solver = LBFGS(binary_logreg)
w_init = jnp.zeros((9,))

@jit # jit toggles just-in-time compilation, one of the main features of JAX
def solve(k):
    data = make_data(k)
    param, state = solver.run(w_init, data)
    return param
```

## Julia

``` {.julia}
function solve(i::Int64=1)
    X, y = make_data()
    fit = glm(X, y, Bernoulli())
    coefs = coef(fit)
    return coefs
end
```

:::

### Iterate over runs / settings

Finally we run the experiments `nrep` times and calculate the average coefficient vector.

#### JAX primitive: map versus vmap
Note that in JAX there are multiple ways to do this, most notably `map` and `vmap`.
Whereas `map` may offer speedups compared to R due to jit-compiliation, for most purposes `vmap` [is recommended](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.map.html) as it allows JAX to find ways of making the computation more efficient.
For example, a vector-vector multiplication *vectorized* over an input of vectors is equivalent to a single matrix-vector multiplication.
JAX's intermediate language finds these possible optimizations and swaps in the more efficient approach.
Vectorized code runs in parallel and can be much faster.
Note that in our case, vectorization may not be too beneficial as running `LBFGS` on different datasets may not lend itself to vectorizations (compared e.g. to neural network computations on batches of data).
A downside of vectorization is that it requires more memory: all the datasets and optimization steps happen in parallel, whereas with loop-based execution, only the coefficients of each time step need to be stored.

::: {.panel-tabset}
## R

``` {.r}
if (nthreads == 1) {
    set.seed(240316)
    params <- lapply(1:nreps, solve)
} else {
    params <- future_map(1:nreps, solve, .options=furrr_options(seed=240316))
}
outmat <- do.call(rbind, params)
means <- colMeans(outmat)
print(means[1])
```

## Python

``` {.python}
k0 = random.PRNGKey(240316)
ks = random.split(k0, args.nreps)
if args.primitive == 'map':
    params = lax.map(solve, ks)
elif args.primitive == 'vmap':
    params = vmap(solve)(ks)
else:
    raise ValueError(f"unrecognized primitive: {args.primitive}, choose map or vmap")

means = jnp.mean(params, axis=0)
print(means[0])
```

## Julia

``` {.julia}
Random.seed!(240316)
outmat = zeros(nreps, 9)

@threads for i in 1:nreps # use @threads for multi-threading
    solution = solve()
    outmat[i,:] = solution
end

means = mean(outmat, dims=1)
print(means[1])
```

:::

### Bash scripts for speed comparisons

I benchmarked each run with an external `time` command in Bash or ZSH and wrote the results to a file.

::: {.panel-tabset}
## Bash (linux)

```{bash}
#| code-fold: true
{{< include brunexps.sh >}}
```

## ZSH (macos)

```{bash}
#| code-fold: true
{{< include runexps.sh >}}
```
:::

## The speed

### Running time

First, let's see how running time increases with the number of experiments, using all available threads.
You cannot easily set number of threads in JAX (see e.g. [this](https://github.com/google/jax/issues/1539) issue on github), so all JAX computations use all threads.

```{r}
#| label: fig-times
#| fig-cap: "Time to run experiments on the maximum number of threads"
#| eval: true
#| code-fold: true

suppressMessages({
    library(dplyr)
    library(data.table)
    library(purrr)
    library(stringr)
    library(ggplot2); theme_set(theme_bw())
    library(knitr)
    library(kableExtra)
})

# get timings from mac
lns <- readr::read_lines('timings.txt')
# get timings from linux machine
blns <- readr::read_lines('bashtimings.txt')

# remove lines with warnings / errors printed to txt file
lns <- str_subset(lns, "^Rscript|julia|python")
mtimings <- data.table(raw_string=lns)
btimings <- data.table(raw_string=blns)

# remove white space and first word (for linux)
timings <- rbindlist(list(macm1=mtimings, linux=btimings), idcol='machine')
timings[, string:=str_trim(raw_string)] # remove white space
timings[, string:=str_replace(string, "^real\t", "")] # remove first word linux

# find the language from the string
timings[machine=='macm1', command:=word(string)]
timings[command=='Rscript', language:='r']
timings[command=='python', language:='jax']
timings[command=='julia', language:='julia']
timings[machine=='linux', language:=str_extract(string, "(?<=s )[a-z]+")]

# grab number of threads and reps
timings[, nthreads:=as.integer(str_extract(string, "(\\d+)(?= nthreads)"))]
timings[, max_threads:=max(nthreads, na.rm=T), by='machine']
timings[is.na(nthreads) & language == 'jax', nthreads:=max_threads]
timings[is.na(nthreads) & language %in% c('r', 'julia'), nthreads:=1L]
timings[, nreps:=as.integer(str_extract(string, "(\\d+)(?= nreps)"))]
timings[, max_threads:=max(nthreads), by='machine']

# find jax primitive
timings[, primitive:=str_extract(string, "(\\w+)(?= primitive)")]
timings[machine=='macm1'&language=='jax'&is.na(primitive), primitive:='map']
#timings <- timings[!(language=='jax' & primitive!='map')]
# timings[language=='jax', language:=paste0(language, '-', primitive)]
timings[language!='jax', primitive:='map']
timings <- timings[!str_ends(primitive, 'nojit')]

# grab the time 
timings[machine=='macm1', time_str:=str_extract(string, "(?<=cpu\\ )(.*)(?= total)")]
timings[, milliseconds:=as.integer(str_extract(time_str, "(\\d+)$"))]
timings[, seconds     :=as.integer(str_extract(time_str, "(\\d+)(?=.)"))]
timings[, minutes     :=as.integer(str_extract(time_str, "(\\d+)(?=:)"))]
timings[, hours       :=as.integer(str_extract(time_str, "(\\d+)(?=:(\\d+:))"))]
timings[machine=='linux', minutes:=as.integer(str_extract(string, "^(\\d+)"))]
timings[machine=='linux', seconds:=as.integer(str_extract(string, "(?<=m)(\\d+)"))]
timings[machine=='linux', milliseconds:=as.integer(str_extract(string, "(\\d+)(?=s)"))]
timings[is.na(minutes), minutes:=0]
timings[is.na(hours), hours:=0]


timings[, sec_total:=60*60*hours + 60*minutes + seconds + milliseconds / 1000]
timings[, min_total:=sec_total / 60]

# add some vars
timings[, n_per_min:=nreps / min_total]
timings[, n_per_min3:=n_per_min/1000]

# remove a couple of failed runs where time went down for more experiments (= out of memory)
#timings <- timings[n_per_min < 1.5e6]


fwrite(timings, 'allresults.csv', row.names=F)

ggplot(timings[nthreads==max_threads], aes(x=nreps, y=min_total, col=language)) +
  geom_point() + geom_line(aes(linetype=primitive)) + 
  scale_x_log10() + scale_y_log10() + 
  # facet_grid(machine+nthreads~primitive, labeller='label_both')
  facet_grid(machine+nthreads~., labeller='label_both')
```

Note that for JAX `vmap` the clock time actually goes *down* when the number of experiment increases.
This is not some magic speedup but the machine running out of memory and thus not completing the experiment, a downside of vectorization.
We'll exclude these runs of the further comparisons.
For `map` this is not the case.

```{r}
#| label: exclude-oom
#| eval: true
#| echo: false

timings <- timings[!((language == 'jax') & (!primitive %in% c('map', 'scan')) & (nreps > 1e5))]

```

### Speed

Let's look at the speeds.

```{r}
#| label: fig-speeds
#| fig-cap: "Speed: number of repetitions per minute versus of number of experiments"
#| eval: true
#| echo: false

ggplot(timings[nthreads==max_threads], aes(x=nreps, y=n_per_min, col=language)) +
  geom_point() + geom_line(aes(linetype=primitive)) + 
  scale_x_log10() + scale_y_log10() + 
  facet_grid(machine+nthreads~., labeller='label_both')
```

Why is the speed going down for Julia on the macm1 machine after 1e6 experiments? 
Turns out there is not enough RAM to fit the experiments and the system switches to swap memory which is much slower than using RAM (even on a mac arm64).
The speed of R stopped increasing after 1e6 experiments so I didn't run more experiments.

### Threads vs Speed

Now let's check how much extra speed we get from using more threads in R and Julia.

```{r}
#| label: fig-threads
#| fig-cap: "Scaling of speed with number of threads"
#| eval: true
#| echo: false

ggplot(timings[!str_starts(language, 'jax')], aes(x=nreps, y=n_per_min, col=language)) +
  geom_point() + geom_line(aes(linetype=factor(nthreads))) + 
  scale_x_log10() + scale_y_log10() + 
  facet_grid(machine~language)
```

```{r}
#| label: tab-threads
#| tbl-cap: "Scaling of speed with number of threads, number of experiments per minute (1000s)"
#| eval: true
#| echo: false

tw <- dcast(timings[!str_starts(language, 'jax')], language+nreps~machine+nthreads, value.var='n_per_min3')
tw[, `:=`(speedup6=linux_6/linux_1, speedup12=linux_12/linux_1, speedup8=macm1_8/macm1_1)]
outcols <- c('language', 'nreps', 'linux_1', 'linux_6', 'speedup6', 'linux_12', 'speedup12', 'macm1_1', 'macm1_8', 'speedup8')

tw[, .SD, .SDcols=outcols] %>%
  mutate_if(is.double, ~format(round(., 1), nsmall = 1)) %>%
  kable(align='r') %>%
  row_spec(4, hline_after=T) %>%
  column_spec(3, border_right=T) %>%
  column_spec(5, border_right=T) %>%
  column_spec(7, border_right=T) %>%
  column_spec(8, border_left=T, border_right=T) %>%
  kable_styling()
```

Speed increases with increasing number of threads, though not with a simple linear scaling in the number of threads.
The speed increase is similar for R and Julia.


### Top speeds per language

Let's see the top speeds per language, also compered to the top R speed on that machine.

```{r}
#| label: tbl-best-per-language
#| tbl-cap: "Best speeds per language and machine"
#| eval: true
#| echo: false

timings[order(-n_per_min), lang_order:=1:.N, by=c('language', 'machine')]
best_r_speeds <- timings[lang_order==1&language=='r', list(n_per_min, machine)]
timings[best_r_speeds, vs_r_speedup:=n_per_min/i.n_per_min, on='machine']

tabcolnames <- c('nreps'='n experiments',
                 'nthreads'='n threads',
                 'min_total'='running time (minutes)',
                 'n_per_min3'='experiments per minute (x1000)',
                 'vs_r_speedup'='speed-up vs best R',
                 'language'='language',
                 'primitive'='primitive',
                 'machine'='machine')
tabcols <- names(tabcolnames)

timings[lang_order==1, .SD, .SDcols=tabcols] %>%
    .[order(machine,language)] %>%
    mutate_if(is.double, ~format(round(., 1), nsmall = 1)) %>%
    kable(align='r', col.names=tabcolnames)
```

### Top speeds overall

Top 10 speeds overall

```{r}
#| label: tbl-topspeeds
#| tbl-cap: "top 10 speeds overall"
#| eval: true
#| echo: false

timings[order(-n_per_min), .SD, .SDcols=tabcols][1:10] %>%
    mutate_if(is.double, ~format(round(., 1), nsmall = 1)) %>%
    kable(align='r', col.names = tabcolnames)
```

All results are available in a csv file [here](allresults.csv)

## Takeaways

### Speed

In this setup, both on a Mac M1 and a Linux machine,

1. Julia was 10-11 times faster than R
2. JAX was 1.7 times faster than R

### Code

1. Julia code seems quite close to R code.
2. In this simple example, the JAX code doesn't seem too dounting. However, JAX comes with some [sharp bits](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) and may be harder to program efficiently.[^1]
   
[^1]: One concrete example with my own previous simulation studies [@vanamsterdamConditionalAverageTreatment2023] is that I used first used `jax.numpy` arrays for different parameters and then a `scipy` function to create all combinations of these parameters. Creating this grid of parameters this way forced copying of `jax.numpy` arrays from the GPU back to CPU and then copying the grid back to GPU. This made the entire process orders of magnitude slower (it was a large grid O(1e12)). Gotchas like these can bite you. Also, JAX relies on pure functions that cannot depend on global variables.

### Caveats

1. JAX needs to recompile when the size of the data changes. When running experiments with e.g. different sizes of data, JAX will become slower because it needs to recompile, or you'll need to find other solutions like padding smaller data with dummy data and giving these dummy data 0 weights in the objective functions.
2. I didn't have a CUDA-enabled GPU machine for this comparison, `vmap` may be come (much) more performant on a GPU
3. JAX gives bare bones results. If you want to do e.g. significance testing of coefficients, or model comparisons, you will need to find implementations for this or implement this yourself. R and Julia (specifically the [GLM package](https://juliastats.org/GLM.jl/stable/) provide a much wider suite of methods
4. In JAX I used general purpose optimizer. There may be more efficient ways of estimating a logistic regression model, whose optimizations are implemented in R and Julia but not JAX. In this sense it may not be a *fair* comparison, though these optimizations would need to be sought or implemented in JAX.
5. JAX has *autograd*. When writing custom objective functions JAX can automatically calculate gradients and hessians, making it possible to use general purpose first or second order optimizers [e.g. @vanamsterdamConditionalAverageTreatment2023].

### Extensions

#### Optimizing memory usage

Since in this case we only need the avarage of the coefficients we need not store all intermediate results. All languages may beccome much more efficient if we can program this in.

- The [julia domumentation states](https://docs.julialang.org/en/v1/manual/multi-threading/#Atomic-Operations) that certain operations to *atomic* data structures can be done in a safe-way while multithreading. Instead of returning all coefficients of all datasets, we could calculate the average value of the coefficients (and e.g. the avareage of the squares of the values) with less memory overhead by:
    1. instantiate an atomic vector of 9 coefficients
    2. let every experiment (which may be in different threads) add its value to this shared atomic vector with `atomic_add!`
    3. at the end, calculate the mean by dividing by `nreps`.
- R functions can also overwrite global variables, but a question is whether this can be done in a multi-threading safe way
- In JAX we may use `scan` to keep track of a running sum of coefficients and then `vmap` a bunch of `scan` computations

In future posts I plan to dive in to dive in to these optimizations to squeeze more out of these languages.

## Conclusion

::: {.callout-tip}
## What should you use for glm-like simulation studies?

Probably, Julia
:::

## Session Info

::: {.panel-tabset}
## R

``` {.r}
#| echo: false
#| eval: false
library(sessioninfo)
session_info(pkgs = "attached", to_file="_rsession.txt")

{{< include _rsession.txt >}}
```



## Python

``` {.python}
{{< include _pythonenv.txt >}}
```

## Julia

``` {.julia}
{{< include jlspeed/Project.toml >}}
```
:::

## Full scripts

::: {.panel-tabset}
## R

``` {.r}
{{< include _scripts/rspeed.R >}}
```

## Python

{{< include _scripts/jaxspeed.py >}}

## Julia

``` {.julia}
{{< include _scripts/jlspeed.jl >}}
```
:::



## References

::: {#refs}
:::


