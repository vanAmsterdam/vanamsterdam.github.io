[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Partial residual plots with multiply imputed data\n\n\n\n\n\n\nr\n\n\nlinear regression\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\nWouter van Amsterdam\n\n\n\n\n\n\n\n\n\n\n\n\nThe need for speed, performing simulation studies in R, JAX and Julia\n\n\n\n\n\n\nr\n\n\njulia\n\n\njax\n\n\npython\n\n\nsimulation studies\n\n\n\n\n\n\n\n\n\nWouter van Amsterdam\n\n\n\n\n\n\n\n\n\n\n\n\nThe difference between intervention and counterfactuals\n\n\n\n\n\n\ncausal inference\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJul 25, 2021\n\n\nWouter van Amsterdam\n\n\n\n\n\n\n\n\n\n\n\n\nWhen good predictions lead to bad decisions\n\n\n\n\n\n\ncausal inference\n\n\npredictions\n\n\n\n\n\n\n\n\n\nJul 20, 2021\n\n\nWouter van Amsterdam\n\n\n\n\n\n\n\n\n\n\n\n\nFinding the functional form for multiple linear regression\n\n\n\n\n\n\nstatistics\n\n\nsimulations\n\n\nlinear regression\n\n\n\n\n\n\n\n\n\nAug 16, 2019\n\n\nWouter van Amsterdam\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#why-i-use-x-1",
    "href": "talks/240312-dsbiostats-twitter/index.html#why-i-use-x-1",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "Why (I) use X?",
    "text": "Why (I) use X?\n\n\n\nlearn\n\npapers\nevents (AAAI 2019)\ndiscussions of (/ with?) famous researchers\n\nconnect\nadvertise work"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#why-not-use-x",
    "href": "talks/240312-dsbiostats-twitter/index.html#why-not-use-x",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "Why not use X?",
    "text": "Why not use X?\n\ntwitter is dead, and X is dying\ndistractions\npolitics"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#x-compared-to",
    "href": "talks/240312-dsbiostats-twitter/index.html#x-compared-to",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "X compared to",
    "text": "X compared to\n\nlinkedin: fake interactions\ntwitter: too real interactions\nmastodon: too hacky / nerdy?\nthreads: twitter clone by meta (facebook)\nbluesky: way to go?"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#twitter-history-and-status",
    "href": "talks/240312-dsbiostats-twitter/index.html#twitter-history-and-status",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "Twitter history and status",
    "text": "Twitter history and status\n\njoined: Jan 2014   \n500-ish followers (Maarten has approx. 41400)\n500-ish following (this determines your timeline)\n15-minutes a day but waning"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#first-tweet-that-got-a-like",
    "href": "talks/240312-dsbiostats-twitter/index.html#first-tweet-that-got-a-like",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "first tweet that got a like",
    "text": "first tweet that got a like"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#first-ad-for-own-paper-on-causal-ml",
    "href": "talks/240312-dsbiostats-twitter/index.html#first-ad-for-own-paper-on-causal-ml",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "first ad for own paper on causal ML",
    "text": "first ad for own paper on causal ML"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#first-joke-kind-of-success",
    "href": "talks/240312-dsbiostats-twitter/index.html#first-joke-kind-of-success",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "first joke (kind-of success)",
    "text": "first joke (kind-of success)"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#first-tweet-that-went-viral-recommending-a-paper",
    "href": "talks/240312-dsbiostats-twitter/index.html#first-tweet-that-went-viral-recommending-a-paper",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "first tweet that went ‘viral’: recommending a paper",
    "text": "first tweet that went ‘viral’: recommending a paper\n\n\n\n\n\n\n259 retweets\n697 likes\n126k views\n8 new followers\nrespected researcher in field asked if I’m interested in doing a post-doc"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#first-paper-run-through-tweet",
    "href": "talks/240312-dsbiostats-twitter/index.html#first-paper-run-through-tweet",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "first paper run-through tweet",
    "text": "first paper run-through tweet"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#first-paper-run-through-tweet-1",
    "href": "talks/240312-dsbiostats-twitter/index.html#first-paper-run-through-tweet-1",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "first paper run-through tweet",
    "text": "first paper run-through tweet"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#first-paper-run-through-tweet-2",
    "href": "talks/240312-dsbiostats-twitter/index.html#first-paper-run-through-tweet-2",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "first paper run-through tweet",
    "text": "first paper run-through tweet"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#first-paper-run-through-tweet-3",
    "href": "talks/240312-dsbiostats-twitter/index.html#first-paper-run-through-tweet-3",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "first paper run-through tweet",
    "text": "first paper run-through tweet"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#first-paper-run-through-tweet-4",
    "href": "talks/240312-dsbiostats-twitter/index.html#first-paper-run-through-tweet-4",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "first paper run-through tweet",
    "text": "first paper run-through tweet"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#first-paper-run-through-tweet-5",
    "href": "talks/240312-dsbiostats-twitter/index.html#first-paper-run-through-tweet-5",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "first paper run-through tweet",
    "text": "first paper run-through tweet\n\n\n\n\n\n35 retweets\n86 likes\n18k views\n0 new followers\nnow 20 citations (5 if not for twitter?)"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#reactions-to-paper-run-through-tweet",
    "href": "talks/240312-dsbiostats-twitter/index.html#reactions-to-paper-run-through-tweet",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "reactions to paper run-through tweet",
    "text": "reactions to paper run-through tweet"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#reactions-to-paper-run-through-tweet-1",
    "href": "talks/240312-dsbiostats-twitter/index.html#reactions-to-paper-run-through-tweet-1",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "reactions to paper run-through tweet",
    "text": "reactions to paper run-through tweet"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#favorite-tweet-meme",
    "href": "talks/240312-dsbiostats-twitter/index.html#favorite-tweet-meme",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "favorite tweet (meme)",
    "text": "favorite tweet (meme)"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#reactions-to-meme-tweet",
    "href": "talks/240312-dsbiostats-twitter/index.html#reactions-to-meme-tweet",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "reactions to meme-tweet",
    "text": "reactions to meme-tweet\n\nhttps://x.com/JAASANTINHA/status/1417936729600823300?s=20"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#biggest-own-tweet-thesis-cover",
    "href": "talks/240312-dsbiostats-twitter/index.html#biggest-own-tweet-thesis-cover",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "biggest own tweet (thesis cover):",
    "text": "biggest own tweet (thesis cover):\n\n\n\n\n\n\n93k impressions\n673 likes\n53 retweets\n6 new followers"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#ask-for-help-tweet",
    "href": "talks/240312-dsbiostats-twitter/index.html#ask-for-help-tweet",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "ask for help-tweet",
    "text": "ask for help-tweet"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#use-x-for",
    "href": "talks/240312-dsbiostats-twitter/index.html#use-x-for",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "Use X for",
    "text": "Use X for\n\nlearning\nconnecting\nadvertising:\n\nnew work\nvacancies"
  },
  {
    "objectID": "talks/240312-dsbiostats-twitter/index.html#i-may-leave-x",
    "href": "talks/240312-dsbiostats-twitter/index.html#i-may-leave-x",
    "title": "Wouter’s twitter X-peri(ments/ences)",
    "section": "I may leave X",
    "text": "I may leave X\nnot sure where to go\n\n\n\n©Wouter van Amsterdam (: WvanAmsterdam)"
  },
  {
    "objectID": "talks/230301-lungsurv-manchester/index.html#section-105",
    "href": "talks/230301-lungsurv-manchester/index.html#section-105",
    "title": "Individual treatment effect estimation in the presence of unobserved confounding using proxies",
    "section": "",
    "text": "©Wouter van Amsterdam (: WvanAmsterdam)"
  },
  {
    "objectID": "talks/230810-mlhc-causality-decision-making/index.html#section-63",
    "href": "talks/230810-mlhc-causality-decision-making/index.html#section-63",
    "title": "The value of observational causal inference for medical decision making",
    "section": "",
    "text": "©Wouter van Amsterdam (: WvanAmsterdam)"
  },
  {
    "objectID": "posts/210725-counterfactualvsinterventional.html",
    "href": "posts/210725-counterfactualvsinterventional.html",
    "title": "The difference between intervention and counterfactuals",
    "section": "",
    "text": "Sometimes there is confusion about the difference between counterfactual predictions and interventional predictions. According to the ladder of causation introduced by Pearl and presented in the ‘Book of Why’, interventional is rung 2 and counterfactuals are rung 3 (associations are rung 1). Models that predict the treatment effect for a new patient based on some covariates \\(X\\) require interventional models, not counterfactual. The difference between interventional and counterfactual models is relevant as counterfactual models require more assumptions, they require knowledge of the structural mechanisms. In words, the interventional question is “What is the expected outcome under treatment \\(t\\) given that we know \\(X\\)”, or “What is the expected difference in outcomes between treatment \\(t\\) and \\(t'\\) given that we know \\(X\\)”. The counterfactual question is “What would have been the outcome if we had given treatment \\(t'\\) given that we gave \\(t\\) and observed outcome \\(y\\)”."
  },
  {
    "objectID": "posts/210725-counterfactualvsinterventional.html#intro",
    "href": "posts/210725-counterfactualvsinterventional.html#intro",
    "title": "The difference between intervention and counterfactuals",
    "section": "",
    "text": "Sometimes there is confusion about the difference between counterfactual predictions and interventional predictions. According to the ladder of causation introduced by Pearl and presented in the ‘Book of Why’, interventional is rung 2 and counterfactuals are rung 3 (associations are rung 1). Models that predict the treatment effect for a new patient based on some covariates \\(X\\) require interventional models, not counterfactual. The difference between interventional and counterfactual models is relevant as counterfactual models require more assumptions, they require knowledge of the structural mechanisms. In words, the interventional question is “What is the expected outcome under treatment \\(t\\) given that we know \\(X\\)”, or “What is the expected difference in outcomes between treatment \\(t\\) and \\(t'\\) given that we know \\(X\\)”. The counterfactual question is “What would have been the outcome if we had given treatment \\(t'\\) given that we gave \\(t\\) and observed outcome \\(y\\)”."
  },
  {
    "objectID": "posts/210725-counterfactualvsinterventional.html#example",
    "href": "posts/210725-counterfactualvsinterventional.html#example",
    "title": "The difference between intervention and counterfactuals",
    "section": "Example",
    "text": "Example\nTo illustrate the difference between a counterfactual prediction and an interventional prediction (or conditional average treatment effect estimates), consider this very simple setup.\nYou have data from a randomized trial with two treatment arms \\(t \\in \\{0,1\\}\\) and an outcome \\(Y\\) on a continuous scale. Denote \\(Y_0\\) the potential outcome under intervening on treatment \\(t=0\\) and \\(Y_1\\) the potential outcome under intervening on treatment \\(t=1\\). As we are dealing with data from a randomized trial, we can easily estimate the average treatment effect as \\(E[Y_1 - Y_0] = E[Y|t=0] - E[Y|t=1]\\), assuming consistency (ignorability and overlap are satisfied due to the study design).\nA counterfactual question is: what would have been the outcome \\(Y_0\\) under treatment \\(t=1\\), given that we observed the outcome \\(y_1\\) under treamtent \\(t=1\\), so it is \\(E[Y_0|Y=y_1, t=1]\\).\nNow assume that the data come from a mixture of Gaussians such that\n\\[y|t \\sim (1 - t) \\mathcal{N}(1,0.1^2) + t \\mathcal{N}(10,2.5^2)\\]\nAnd \\(p(t=1)=0.5\\) so both arms are equally large. Treatment \\(t=1\\) leads to higher outcome but also more spread. The relevant interventional expectations are easily calculated by just calculating group means \\(E[Y_0] = E[Y|t=0] = 1\\), \\(E[Y_1] = E[Y|t=1] = 10\\).\n\nCalculating the counterfactuals\nTo see that calculating counterfactuals requires more knowledge, namely of the structural equations, we now calculate the counterfactual prediction for a patient with \\(Y=Y_1=15\\). This is a patient with a relatively large ‘residual’, the outcome is 2 standard deviations above the mean for treatment group \\(t=1\\).\nFirst we calculate the counterfactual outcome under a wrong outcome model. Researchers tried to model the outcomes using linear regression, and failed to appreciate the difference in variances between the two treatment arms (heteroscedasticity). Assuming a large sample, they will arrive at a model:\n\\[\\hat{y}_{\\text{wrong}} = 1 + t * 9 + \\mathcal{N}(0,\\sigma^2)\\]\nWhere \\(\\sigma = \\sqrt{\\frac{0.1^2 + 2.5^2}{2}} \\approx 1.77\\) (standard devation of mixture of Gaussians with (conditional) mean of 0 and standard deviations 0.1 and 2.5, with 50 / 50 mixing). Note that the estimate of the treatment effect is correct, and so are \\(E[Y_0]\\) and \\(E[Y_1]\\). If there was a binary pre-treatment covariate, the conditional average treatment effect could be estimated by repeating this exercise for both levels of the covariate. To calculate the counterfactual outcome of our patient, we first need to determine the value of their noise variable for the outcome. According to \\(\\hat{y}_{\\text{wrong}}\\), the residual for a patient with \\(Y_1=15\\) is \\(5\\), which is \\(5/\\sigma \\approx 2.82\\) standard deviations away from the expected value for \\(t=1\\). Given this residual we can now calculate the counterfactual:\n\\[\\widehat{E_{\\text{wrong}}}[Y_0|Y=15,t=1] \\approx E[Y_0] + 2.82 \\sigma =6\\]\nGiven that we know the data generating mechanism, we know that this counterfactual prediction is 50 standard deviations from the conditional mean of \\(t=0\\) in the data generating mechanism, clearly this counterfactual prediction is wrong.\nIf we did model the data correctly with a mixture of Gaussians indexed by the treatment group, we would instead say that \\(Y_1=15\\) is 2 standard deviations above the conditional mean, and we would calculate:\n\\[\\widehat{E^*}[Y_0|Y=15,t=1] = E[Y_0] + 2 * 0.1 =1.2\\]\nWhich is correct."
  },
  {
    "objectID": "posts/210725-counterfactualvsinterventional.html#conclusion",
    "href": "posts/210725-counterfactualvsinterventional.html#conclusion",
    "title": "The difference between intervention and counterfactuals",
    "section": "Conclusion",
    "text": "Conclusion\nTo calculate counterfactual predictions, you need to correctly specify the structural equations. For treatment recommendations for future patients, these are not needed, interventional estimates (conditional average treatment effect) are sufficient, and obviously the factual outcome is not observed yet so it is impossible to calculate counterfactuals (the factual is not yet known).\nPost-script: for ‘real’ patient counsellling the expected values under the treatments would generally not suffice, some measure of spread / uncertainty would be required. Ideally, one would learn the distribution of the potential outcomes."
  },
  {
    "objectID": "posts/240308-jaxopt-vs-r-vs-julia/index.html",
    "href": "posts/240308-jaxopt-vs-r-vs-julia/index.html",
    "title": "The need for speed, performing simulation studies in R, JAX and Julia",
    "section": "",
    "text": "Simulation experiments are important when evaluating methods but also for applied work in for example power analyses (e.g. Amsterdam, Harlianto, et al. 2022) or sensitivity analyses (e.g. Amsterdam, Verhoeff, et al. 2022). When using simulations to support scientific claims, the more experiments the better. Being able to perform simulation experiments faster allows researchers to:\nThe R language has been a popular language among many biostatisticians for a long time, but it is not generally considered the top performing language in terms of speed.  In recent years, JAX (developed by Google) and Julia have arisen as general scientific computation frameworks. JAX and Julia have grown in popularity both in the neural network community as in other scientific communities (e.g. “DifferentiableUniverseInitiative/Jax_cosmo” 2024; “SciML: Open Source Software for Scientific Machine Learning” n.d.). In this blog post I’ll compare R with JAX and Julia for a simple simulation study setup with logistic regression.\nWe’ll look at the following comparisons:"
  },
  {
    "objectID": "posts/240308-jaxopt-vs-r-vs-julia/index.html#jax-and-julia-vs-r-a-high-level-overview",
    "href": "posts/240308-jaxopt-vs-r-vs-julia/index.html#jax-and-julia-vs-r-a-high-level-overview",
    "title": "The need for speed, performing simulation studies in R, JAX and Julia",
    "section": "JAX and Julia vs R: a high level overview",
    "text": "JAX and Julia vs R: a high level overview\nJAX is a rising star in computer science and natural sciences. Without going in too much details, JAX works by translating python code into an intermediate language that can be run very efficiently on different hardware backends (CPU, GPU, TPU), possibly with just-in-time compilation (JIT). JAX prides itself on providing composable transformations for vectorization (vmap), paralellization (pmap) and automatic differentiation (grad), all compatible with jit. In R, most of the heavy lifting in terms of computation (such as fitting a logistic regression model) is implemented in high-speed languages such as C++ or Fortran. The usual R-code merely provides an interface to these languages and allows the user to feed in data and analyze results. Whereas using JAX and R means working with two languages (one language to write accessible code, another to do fast computation), Julia is a just-in-time compiled language where such translation is not needed."
  },
  {
    "objectID": "posts/240308-jaxopt-vs-r-vs-julia/index.html#the-basic-setup",
    "href": "posts/240308-jaxopt-vs-r-vs-julia/index.html#the-basic-setup",
    "title": "The need for speed, performing simulation studies in R, JAX and Julia",
    "section": "The basic setup",
    "text": "The basic setup\nWe’ll use a simple logistic regression simulation setup, where for each observation:\n\\[\n\\begin{align}\n\\mathbf{x}_{\\text{full}} &\\sim \\mathcal{N}(0,I) \\in \\mathbb{R}^{10} \\\\\ny &= ||\\mathbf{x}||_0 &gt; 0 \\\\\n\\mathbf{x}_{\\text{obs}} &= [x_0\\ x_i \\ldots x_9]\n\\end{align}\n\\]\nSo \\(y\\) is the sum of elements of \\(\\mathbf{x}_{\\text{full}}\\) and the observed \\(\\mathbf{x}_{\\text{obs}}\\) contains only the first 9 out of 10 elements of \\(\\mathbf{x}_{\\text{full}}\\).\nWe will model this data with logistic regression:\n\\[\n\\begin{align}\n    \\text{logit}(y) &= \\mathbf{x}_{\\text{obs}} \\boldsymbol{\\beta}'\\\\\n    y &\\sim \\text{Bernoulli} (\\sigma (\\mathbf{x}_{\\text{obs}} \\boldsymbol{\\beta}'))\n\\end{align}\n\\]\nwhere \\(\\boldsymbol{\\beta} = [\\beta_1,\\ldots,\\beta_9]\\) is a 9-dimensional parameter vector that is to be estimated (we’re excluding the usual intercept term). We’ll generate nrep independent datasets and estimate \\(\\boldsymbol{\\beta}\\) in each one, and finally calculate the average parameter estimates \\(\\frac{1}{\\text{nrep}}\\sum_{i=1}^{\\text{nrep}}\\boldsymbol{\\beta}^i\\).\n\nHardware\nThe hardware I had available for this comparison is:\n\nmacm1: 2020 macbook air M1, 8Gb RAM, 8 threads\nlinux: linux machine, 64Gb RAM, 12 threads (Intel(R) Xeon(R) W-2135 CPU @ 3.70GH )"
  },
  {
    "objectID": "posts/240308-jaxopt-vs-r-vs-julia/index.html#the-code",
    "href": "posts/240308-jaxopt-vs-r-vs-julia/index.html#the-code",
    "title": "The need for speed, performing simulation studies in R, JAX and Julia",
    "section": "The code",
    "text": "The code\n\nMaking data\nMaking the data is pretty similar in all cases, except that JAX requires an explicit random key.\n\nRPythonJulia\n\n\nmake_data &lt;- function(n=1e3L) {\n    x_vec = rnorm(n*10)\n    X_full = matrix(x_vec, ncol=10)\n    eta = rowSums(X_full)\n    y = eta &gt; 0\n    # return only first 9 column to have some noise\n    X = X_full[,1:9]\n    return(list(X=X,y=y))\n}\n\n\ndef make_data(k, n=int(1e3)):\n    X_full = random.normal(k, (n,10)) # JAX needs explicit keys for psuedo random number generation\n    eta = jnp.sum(X_full, axis=-1)\n    y = eta &gt; 0\n    # return only first 9 column to have some noise\n    X = X_full[:,:9]\n    return (X, y)\n\n\nfunction make_data(n::Integer=1000)\n    X_full = randn(n,10)\n    eta = vec(sum(X_full, dims=2))\n    y = eta .&gt; 0 # vectorized greater than 0 comparison\n    X = X_full[:,1:9]\n    return X, y\nend\n\n\n\n\n\nRun single experiment\nNow we’ll write the code for a single analysis step, generating data and fitting the logistic regression. For R and Julia we will use the glm function to estimate the logistic regression model. The Julia code looks much like the R code. As far as I know there is no equivalent glm function implemented in JAX. Instead, we need to specify an objective function and will use a general purpose optimizer. JaxOpt provides both binary_logreg as an objective function and LBFGS, a popular general purpose optimizer, which we’ll use here.\n\nRPythonJulia\n\n\nsolve &lt;- function(...) {\n  data = make_data()\n  fit = glm(data$y~data$X-1, family='binomial')\n  coefs = coef(fit)\n  return(coefs)\n}\n\n\n# initialize a generic solver with the correct objective function\nsolver = LBFGS(binary_logreg)\nw_init = jnp.zeros((9,))\n\n@jit # jit toggles just-in-time compilation, one of the main features of JAX\ndef solve(k):\n    data = make_data(k)\n    param, state = solver.run(w_init, data)\n    return param\n\n\nfunction solve(i::Int64=1)\n    X, y = make_data()\n    fit = glm(X, y, Bernoulli())\n    coefs = coef(fit)\n    return coefs\nend\n\n\n\n\n\nIterate over runs / settings\nFinally we run the experiments nrep times and calculate the average coefficient vector.\n\nJAX primitive: map versus vmap\nNote that in JAX there are multiple ways to do this, most notably map and vmap. Whereas map may offer speedups compared to R due to jit-compiliation, for most purposes vmap is recommended as it allows JAX to find ways of making the computation more efficient. For example, a vector-vector multiplication vectorized over an input of vectors is equivalent to a single matrix-vector multiplication. JAX’s intermediate language finds these possible optimizations and swaps in the more efficient approach. Vectorized code runs in parallel and can be much faster. Note that in our case, vectorization may not be too beneficial as running LBFGS on different datasets may not lend itself to vectorizations (compared e.g. to neural network computations on batches of data). A downside of vectorization is that it requires more memory: all the datasets and optimization steps happen in parallel, whereas with loop-based execution, only the coefficients of each time step need to be stored.\n\nRPythonJulia\n\n\nif (nthreads == 1) {\n    set.seed(240316)\n    params &lt;- lapply(1:nreps, solve)\n} else {\n    params &lt;- future_map(1:nreps, solve, .options=furrr_options(seed=240316))\n}\noutmat &lt;- do.call(rbind, params)\nmeans &lt;- colMeans(outmat)\nprint(means[1])\n\n\nk0 = random.PRNGKey(240316)\nks = random.split(k0, args.nreps)\nif args.primitive == 'map':\n    params = lax.map(solve, ks)\nelif args.primitive == 'vmap':\n    params = vmap(solve)(ks)\nelse:\n    raise ValueError(f\"unrecognized primitive: {args.primitive}, choose map or vmap\")\n\nmeans = jnp.mean(params, axis=0)\nprint(means[0])\n\n\nRandom.seed!(240316)\noutmat = zeros(nreps, 9)\n\n@threads for i in 1:nreps # use @threads for multi-threading\n    solution = solve()\n    outmat[i,:] = solution\nend\n\nmeans = mean(outmat, dims=1)\nprint(means[1])\n\n\n\n\n\n\nBash scripts for speed comparisons\nI benchmarked each run with an external time command in Bash or ZSH and wrote the results to a file.\n\nBash (linux)ZSH (macos)\n\n\n\n\nCode\n#!/bin/bash\n\nfor nreps in 1000 10000 100000 1000000 10000000\ndo\n    echo $nreps\n    { time python scripts/jaxspeed.py $nreps map; } 2&gt;&1 | grep real &gt;&gt; bashtimings.txt\n    sed -i '$s/$/ jax '\"${nreps}\"' nreps 12 nthreads map primitive/' bashtimings.txt\n    { time python scripts/jaxspeed.py $nreps vmap; } 2&gt;&1 | grep real &gt;&gt; bashtimings.txt\n    sed -i '$s/$/ jax '\"${nreps}\"' nreps 12 nthreads vmap primitive/' bashtimings.txt\n\n    for nthreads in 1 6 12\n    do\n        echo $nthreads\n        { time julia -t $nthreads scripts/jlspeed.jl $nreps ; } 2&gt;&1 | grep real &gt;&gt; bashtimings.txt\n        sed -i '$s/$/ julia '\"${nreps}\"' nreps '\"${nthreads}\"' nthreads/' bashtimings.txt\n        { time Rscript scripts/rspeed.R $nreps $nthreads ; } 2&gt;&1 | grep real &gt;&gt; bashtimings.txt\n        sed -i '$s/$/ r '\"${nreps}\"' nreps '\"${nthreads}\"' nthreads/' bashtimings.txt\n    done\ndone\n\n\n\n\n\n\nCode\n#!/bin/zsh\n\nfor nreps in 1000 10000 100000 1000000 10000000\ndo\n    echo $nreps\n    { time python scripts/jaxspeed.py $nreps map ; } 2&gt;&gt; timings.txt\n    sed -i '' '$s/$/ '\"${nreps}\"' nreps 8 threads map primitive/' timings.txt\n    { time python scripts/jaxspeed.py $nreps vmap ; } 2&gt;&gt; timings.txt\n    sed -i '' '$s/$/ '\"${nreps}\"' nreps 8 threads vmap primitive/' timings.txt\n\n    for nthreads in 1 8\n    do\n        echo $nthreads\n        { time julia -t $nthreads scripts/jlspeed.jl $nreps ; } 2&gt;&gt; timings.txt\n        sed -i '' '$s/$/ '\"${nreps}\"' nreps '\"${nthreads}\"' nthreads/' timings.txt\n        { time Rscript scripts/rspeed.R $nreps $nthreads ; } 2&gt;&gt; timings.txt\n        sed -i '' '$s/$/ '\"${nreps}\"' nreps '\"${nthreads}\"' nthreads/' timings.txt\n    done\ndone"
  },
  {
    "objectID": "posts/240308-jaxopt-vs-r-vs-julia/index.html#the-speed",
    "href": "posts/240308-jaxopt-vs-r-vs-julia/index.html#the-speed",
    "title": "The need for speed, performing simulation studies in R, JAX and Julia",
    "section": "The speed",
    "text": "The speed\n\nRunning time\nFirst, let’s see how running time increases with the number of experiments, using all available threads. You cannot easily set number of threads in JAX (see e.g. this issue on github), so all JAX computations use all threads.\n\n\nCode\nsuppressMessages({\n    library(dplyr)\n    library(data.table)\n    library(purrr)\n    library(stringr)\n    library(ggplot2); theme_set(theme_bw())\n    library(knitr)\n    library(kableExtra)\n})\n\n# get timings from mac\nlns &lt;- readr::read_lines('timings.txt')\n# get timings from linux machine\nblns &lt;- readr::read_lines('bashtimings.txt')\n\n# remove lines with warnings / errors printed to txt file\nlns &lt;- str_subset(lns, \"^Rscript|julia|python\")\nmtimings &lt;- data.table(raw_string=lns)\nbtimings &lt;- data.table(raw_string=blns)\n\n# remove white space and first word (for linux)\ntimings &lt;- rbindlist(list(macm1=mtimings, linux=btimings), idcol='machine')\ntimings[, string:=str_trim(raw_string)] # remove white space\ntimings[, string:=str_replace(string, \"^real\\t\", \"\")] # remove first word linux\n\n# find the language from the string\ntimings[machine=='macm1', command:=word(string)]\ntimings[command=='Rscript', language:='r']\ntimings[command=='python', language:='jax']\ntimings[command=='julia', language:='julia']\ntimings[machine=='linux', language:=str_extract(string, \"(?&lt;=s )[a-z]+\")]\n\n# grab number of threads and reps\ntimings[, nthreads:=as.integer(str_extract(string, \"(\\\\d+)(?= nthreads)\"))]\ntimings[, max_threads:=max(nthreads, na.rm=T), by='machine']\ntimings[is.na(nthreads) & language == 'jax', nthreads:=max_threads]\ntimings[is.na(nthreads) & language %in% c('r', 'julia'), nthreads:=1L]\ntimings[, nreps:=as.integer(str_extract(string, \"(\\\\d+)(?= nreps)\"))]\ntimings[, max_threads:=max(nthreads), by='machine']\n\n# find jax primitive\ntimings[, primitive:=str_extract(string, \"(\\\\w+)(?= primitive)\")]\ntimings[machine=='macm1'&language=='jax'&is.na(primitive), primitive:='map']\n#timings &lt;- timings[!(language=='jax' & primitive!='map')]\n# timings[language=='jax', language:=paste0(language, '-', primitive)]\ntimings[language!='jax', primitive:='map']\ntimings &lt;- timings[!str_ends(primitive, 'nojit')]\n\n# grab the time \ntimings[machine=='macm1', time_str:=str_extract(string, \"(?&lt;=cpu\\\\ )(.*)(?= total)\")]\ntimings[, milliseconds:=as.integer(str_extract(time_str, \"(\\\\d+)$\"))]\ntimings[, seconds     :=as.integer(str_extract(time_str, \"(\\\\d+)(?=.)\"))]\ntimings[, minutes     :=as.integer(str_extract(time_str, \"(\\\\d+)(?=:)\"))]\ntimings[, hours       :=as.integer(str_extract(time_str, \"(\\\\d+)(?=:(\\\\d+:))\"))]\ntimings[machine=='linux', minutes:=as.integer(str_extract(string, \"^(\\\\d+)\"))]\ntimings[machine=='linux', seconds:=as.integer(str_extract(string, \"(?&lt;=m)(\\\\d+)\"))]\ntimings[machine=='linux', milliseconds:=as.integer(str_extract(string, \"(\\\\d+)(?=s)\"))]\ntimings[is.na(minutes), minutes:=0]\ntimings[is.na(hours), hours:=0]\n\n\ntimings[, sec_total:=60*60*hours + 60*minutes + seconds + milliseconds / 1000]\ntimings[, min_total:=sec_total / 60]\n\n# add some vars\ntimings[, n_per_min:=nreps / min_total]\ntimings[, n_per_min3:=n_per_min/1000]\n\n# remove a couple of failed runs where time went down for more experiments (= out of memory)\n#timings &lt;- timings[n_per_min &lt; 1.5e6]\n\n\nfwrite(timings, 'allresults.csv', row.names=F)\n\nggplot(timings[nthreads==max_threads], aes(x=nreps, y=min_total, col=language)) +\n  geom_point() + geom_line(aes(linetype=primitive)) + \n  scale_x_log10() + scale_y_log10() + \n  # facet_grid(machine+nthreads~primitive, labeller='label_both')\n  facet_grid(machine+nthreads~., labeller='label_both')\n\n\n\n\n\n\n\n\nFigure 1: Time to run experiments on the maximum number of threads\n\n\n\n\n\nNote that for JAX vmap the clock time actually goes down when the number of experiment increases. This is not some magic speedup but the machine running out of memory and thus not completing the experiment, a downside of vectorization. We’ll exclude these runs of the further comparisons. For map this is not the case.\n\n\nSpeed\nLet’s look at the speeds.\n\n\n\n\n\n\n\n\nFigure 2: Speed: number of repetitions per minute versus of number of experiments\n\n\n\n\n\nWhy is the speed going down for Julia on the macm1 machine after 1e6 experiments? Turns out there is not enough RAM to fit the experiments and the system switches to swap memory which is much slower than using RAM (even on a mac arm64). The speed of R stopped increasing after 1e6 experiments so I didn’t run more experiments.\n\n\nThreads vs Speed\nNow let’s check how much extra speed we get from using more threads in R and Julia.\n\n\n\n\n\n\n\n\nFigure 3: Scaling of speed with number of threads\n\n\n\n\n\n\n\n\n\nScaling of speed with number of threads, number of experiments per minute (1000s)\n\n\nlanguage\nnreps\nlinux_1\nlinux_6\nspeedup6\nlinux_12\nspeedup12\nmacm1_1\nmacm1_8\nspeedup8\n\n\n\n\njulia\n1e+03\n10.7\n11.8\n1.1\n11.0\n1.0\n16.2\n16.3\n1.0\n\n\njulia\n1e+04\n69.5\n108.1\n1.6\n101.0\n1.5\n96.9\n147.6\n1.5\n\n\njulia\n1e+05\n146.0\n514.8\n3.5\n567.1\n3.9\n151.3\n494.4\n3.3\n\n\njulia\n1e+06\n162.8\n857.3\n5.3\n1001.3\n6.2\n163.9\n983.6\n6.0\n\n\njulia\n1e+07\n168.6\n875.7\n5.2\n1059.3\n6.3\n163.9\n245.9\n1.5\n\n\nr\n1e+03\n14.8\n14.5\n1.0\n9.2\n0.6\n20.8\n23.7\n1.1\n\n\nr\n1e+04\n15.3\n53.0\n3.5\n52.4\n3.4\n22.5\n72.7\n3.2\n\n\nr\n1e+05\n14.6\n74.1\n5.1\n89.8\n6.2\n24.6\n98.2\n4.0\n\n\nr\n1e+06\n13.9\n70.8\n5.1\n91.0\n6.6\n20.9\n82.0\n3.9\n\n\n\n\n\n\n\n\nSpeed increases with increasing number of threads, though not with a simple linear scaling in the number of threads. The speed increase is similar for R and Julia.\n\n\nTop speeds per language\nLet’s see the top speeds per language, also compered to the top R speed on that machine.\n\n\n\n\nTable 1: Best speeds per language and machine\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn experiments\nn threads\nrunning time (minutes)\nexperiments per minute (x1000)\nspeed-up vs best R\nlanguage\nprimitive\nmachine\n\n\n\n\n1e+05\n12\n0.6\n161.9\n1.8\njax\nvmap\nlinux\n\n\n1e+07\n12\n9.4\n1059.3\n11.6\njulia\nmap\nlinux\n\n\n1e+06\n12\n11.0\n91.0\n1.0\nr\nmap\nlinux\n\n\n1e+06\n8\n6.1\n163.9\n1.7\njax\nmap\nmacm1\n\n\n1e+06\n8\n1.0\n983.6\n10.0\njulia\nmap\nmacm1\n\n\n1e+05\n8\n1.0\n98.2\n1.0\nr\nmap\nmacm1\n\n\n\n\n\n\n\n\n\n\nTop speeds overall\nTop 10 speeds overall\n\n\n\n\nTable 2: top 10 speeds overall\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn experiments\nn threads\nrunning time (minutes)\nexperiments per minute (x1000)\nspeed-up vs best R\nlanguage\nprimitive\nmachine\n\n\n\n\n1e+07\n12\n9.4\n1059.3\n11.6\njulia\nmap\nlinux\n\n\n1e+06\n12\n1.0\n1001.3\n11.0\njulia\nmap\nlinux\n\n\n1e+06\n8\n1.0\n983.6\n10.0\njulia\nmap\nmacm1\n\n\n1e+07\n6\n11.4\n875.7\n9.6\njulia\nmap\nlinux\n\n\n1e+06\n6\n1.2\n857.3\n9.4\njulia\nmap\nlinux\n\n\n1e+05\n12\n0.2\n567.1\n6.2\njulia\nmap\nlinux\n\n\n1e+05\n6\n0.2\n514.8\n5.7\njulia\nmap\nlinux\n\n\n1e+05\n8\n0.2\n494.4\n5.0\njulia\nmap\nmacm1\n\n\n1e+07\n8\n40.7\n245.9\n2.5\njulia\nmap\nmacm1\n\n\n1e+07\n1\n59.3\n168.6\n1.9\njulia\nmap\nlinux\n\n\n\n\n\n\n\n\nAll results are available in a csv file here"
  },
  {
    "objectID": "posts/240308-jaxopt-vs-r-vs-julia/index.html#takeaways",
    "href": "posts/240308-jaxopt-vs-r-vs-julia/index.html#takeaways",
    "title": "The need for speed, performing simulation studies in R, JAX and Julia",
    "section": "Takeaways",
    "text": "Takeaways\n\nSpeed\nIn this setup, both on a Mac M1 and a Linux machine,\n\nJulia was 10-11 times faster than R\nJAX was 1.7 times faster than R\n\n\n\nCode\n\nJulia code seems quite close to R code.\nIn this simple example, the JAX code doesn’t seem too dounting. However, JAX comes with some sharp bits and may be harder to program efficiently.1\n\n\n\nCaveats\n\nJAX needs to recompile when the size of the data changes. When running experiments with e.g. different sizes of data, JAX will become slower because it needs to recompile, or you’ll need to find other solutions like padding smaller data with dummy data and giving these dummy data 0 weights in the objective functions.\nI didn’t have a CUDA-enabled GPU machine for this comparison, vmap may be come (much) more performant on a GPU\nJAX gives bare bones results. If you want to do e.g. significance testing of coefficients, or model comparisons, you will need to find implementations for this or implement this yourself. R and Julia (specifically the GLM package provide a much wider suite of methods\nIn JAX I used general purpose optimizer. There may be more efficient ways of estimating a logistic regression model, whose optimizations are implemented in R and Julia but not JAX. In this sense it may not be a fair comparison, though these optimizations would need to be sought or implemented in JAX.\nJAX has autograd. When writing custom objective functions JAX can automatically calculate gradients and hessians, making it possible to use general purpose first or second order optimizers (e.g. Amsterdam and Ranganath 2023).\n\n\n\nExtensions\n\nOptimizing memory usage\nSince in this case we only need the avarage of the coefficients we need not store all intermediate results. All languages may beccome much more efficient if we can program this in.\n\nThe julia domumentation states that certain operations to atomic data structures can be done in a safe-way while multithreading. Instead of returning all coefficients of all datasets, we could calculate the average value of the coefficients (and e.g. the avareage of the squares of the values) with less memory overhead by:\n\ninstantiate an atomic vector of 9 coefficients\nlet every experiment (which may be in different threads) add its value to this shared atomic vector with atomic_add!\nat the end, calculate the mean by dividing by nreps.\n\nR functions can also overwrite global variables, but a question is whether this can be done in a multi-threading safe way\nIn JAX we may use scan to keep track of a running sum of coefficients and then vmap a bunch of scan computations\n\nIn future posts I plan to dive in to dive in to these optimizations to squeeze more out of these languages."
  },
  {
    "objectID": "posts/240308-jaxopt-vs-r-vs-julia/index.html#conclusion",
    "href": "posts/240308-jaxopt-vs-r-vs-julia/index.html#conclusion",
    "title": "The need for speed, performing simulation studies in R, JAX and Julia",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\n\n\n\nWhat should you use for glm-like simulation studies?\n\n\n\nProbably, Julia"
  },
  {
    "objectID": "posts/240308-jaxopt-vs-r-vs-julia/index.html#session-info",
    "href": "posts/240308-jaxopt-vs-r-vs-julia/index.html#session-info",
    "title": "The need for speed, performing simulation studies in R, JAX and Julia",
    "section": "Session Info",
    "text": "Session Info\n\nRPythonJulia\n\n\n#| echo: false\n#| eval: false\nlibrary(sessioninfo)\nsession_info(pkgs = \"attached\", to_file=\"_rsession.txt\")\n\n─ Session info ────────────────────────────────────────────────────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       macOS Sonoma 14.3\n system   aarch64, darwin23.0.0\n ui       RStudio\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Amsterdam\n date     2024-03-21\n rstudio  2023.12.1+402 Ocean Storm (desktop)\n pandoc   3.1.12.2 @ /opt/homebrew/bin/ (via rmarkdown)\n\n─ Packages ────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n data.table  * 1.14.8  2023-02-17 [1] CRAN (R 4.3.1)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.1)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.1)\n kableExtra  * 1.4.0   2024-01-24 [1] CRAN (R 4.3.2)\n knitr       * 1.43    2023-05-25 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.1)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.3.2)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.1)\n\n [1] /opt/homebrew/lib/R/4.3/site-library\n [2] /opt/homebrew/Cellar/r/4.3.2/lib/R/library\n\n───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\njax==0.4.25\njaxlib==0.4.25\njaxopt==0.8.3\nml-dtypes==0.3.2\nnumpy==1.26.4\nopt-einsum==3.3.0\nscipy==1.12.0\n\n\nname = \"jlspeed\"\nuuid = \"a8cd5241-1987-4548-90e5-ac0f39e812f2\"\nauthors = [\"Wouter van Amsterdam\"]\nversion = \"0.1.0\"\n\n[deps]\nArgParse = \"c7e460c6-2fb9-53a9-8c5b-16f535851c63\"\nGLM = \"38e38edf-8417-5370-95a0-9cbb8c7f171a\"\nRandom = \"9a3f8284-a2c9-5f02-9a11-845980a1fd5c\"\nStatsBase = \"2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91\""
  },
  {
    "objectID": "posts/240308-jaxopt-vs-r-vs-julia/index.html#full-scripts",
    "href": "posts/240308-jaxopt-vs-r-vs-julia/index.html#full-scripts",
    "title": "The need for speed, performing simulation studies in R, JAX and Julia",
    "section": "Full scripts",
    "text": "Full scripts\n\nRPythonJulia\n\n\n# rspeed\nargs = commandArgs(trailingOnly = T)\nif (length(args) == 0) {\n  nreps = 100\n  nthreads = 1\n} else if (length(args) == 1) {\n  nreps = as.integer(args[1])\n  nthreads = 1\n} else {\n  nreps = as.integer(args[1])\n  nthreads = as.integer(args[2])\n  suppressMessages(library(furrr))\n  plan(multisession, workers=nthreads)\n}\n\nmake_data &lt;- function(n=1e3L) {\n    x_vec = rnorm(n*10)\n    X_full = matrix(x_vec, ncol=10)\n    eta = rowSums(X_full)\n    y = eta &gt; 0\n    # return only first 9 column to have some noise\n    X = X_full[,1:9]\n    return(list(X=X,y=y))\n}\n\nsolve &lt;- function(...) {\n  data = make_data()\n  fit = glm(data$y~data$X-1, family='binomial')\n  coefs = coef(fit)\n  return(coefs)\n}\n\nif (nthreads == 1) {\n    set.seed(240316)\n    params &lt;- lapply(1:nreps, solve)\n} else {\n    params &lt;- future_map(1:nreps, solve, .options=furrr_options(seed=240316))\n}\noutmat &lt;- do.call(rbind, params)\nmeans &lt;- colMeans(outmat)\nprint(means[1])\n\n\n\nimport jax, jaxopt jax.config.update(‘jax_platform_name’, ‘cpu’) # make sure jax doesnt use a gpu if it’s available from jax import numpy as jnp, random, vmap, jit, lax from jaxopt import LBFGS from jaxopt.objective import binary_logreg from argparse import ArgumentParser\nparser = ArgumentParser() parser.add_argument(‘nreps’, nargs=“?”, type=int, default=int(10)) parser.add_argument(‘primitive’, nargs=“?”, type=str, default=“vmap”)\ndef make_data(k, n=int(1e3)): X_full = random.normal(k, (n,10)) # JAX needs explicit keys for psuedo random number generation eta = jnp.sum(X_full, axis=-1) y = eta &gt; 0 # return only first 9 column to have some noise X = X_full[:,:9] return (X, y)\n\ninitialize a generic solver with the correct objective function\nsolver = LBFGS(binary_logreg) # need to specify parameter initialization values w_init = jnp.zeros((9,))\n(jit?) # jit toggles just-in-time compilation, one of the main features of JAX def solve(k): data = make_data(k) param, state = solver.run(w_init, data) return param\nif name == ‘main’: args = parser.parse_args() k0 = random.PRNGKey(240316) ks = random.split(k0, args.nreps) if args.primitive == ‘map’: params = lax.map(solve, ks) elif args.primitive == ‘vmap’: params = vmap(solve)(ks) else: raise ValueError(f”unrecognized primitive: {args.primitive}, choose map or vmap”)\nmeans = jnp.mean(params, axis=0)\nprint(means[0])\n\n\n\nusing Random, GLM, StatsBase, ArgParse\nimport Base.Threads.@threads\n\nfunction parse_cmdline()\n    parser = ArgParseSettings()\n\n    @add_arg_table parser begin\n        \"nreps\"\n          help = \"number of repetitions\"\n          required = false\n          arg_type = Int\n          default = 10\n    end\n\n    return parse_args(parser)\nend\n\nfunction make_data(n::Integer=1000)\n    X_full = randn(n,10)\n    eta = vec(sum(X_full, dims=2))\n    y = eta .&gt; 0 # vectorized greater than 0 comparison\n    X = X_full[:,1:9]\n    return X, y\nend\n\nfunction solve(i::Int64=1)\n    X, y = make_data()\n    fit = glm(X, y, Bernoulli())\n    coefs = coef(fit)\n    return coefs\nend\n\nfunction main()\n    args = parse_cmdline()\n    nreps = get(args, \"nreps\", 10)\n\n    Random.seed!(240316)\n    outmat = zeros(nreps, 9)\n\n    @threads for i in 1:nreps # use @threads for multi-threading\n        solution = solve()\n        outmat[i,:] = solution\n    end\n\n    means = mean(outmat, dims=1)\n    print(means[1])\nend\n\nmain()"
  },
  {
    "objectID": "posts/240308-jaxopt-vs-r-vs-julia/index.html#references",
    "href": "posts/240308-jaxopt-vs-r-vs-julia/index.html#references",
    "title": "The need for speed, performing simulation studies in R, JAX and Julia",
    "section": "References",
    "text": "References\n\n\nAmsterdam, Wouter A. C. van, Netanja I. Harlianto, Joost J. C. Verhoeff, Pim Moeskops, Pim A. de Jong, and Tim Leiner. 2022. “The Association Between Muscle Quantity and Overall Survival Depends on Muscle Radiodensity: A Cohort Study in Non-Small-Cell Lung Cancer Patients.” Journal of Personalized Medicine 12 (7): 1191. https://doi.org/10.3390/jpm12071191.\n\n\nAmsterdam, Wouter A. C. van, and Rajesh Ranganath. 2023. “Conditional Average Treatment Effect Estimation with Marginally Constrained Models.” Journal of Causal Inference 11 (1): 20220027. https://doi.org/10.1515/jci-2022-0027.\n\n\nAmsterdam, Wouter A. C. van, Joost J. C. Verhoeff, Netanja I. Harlianto, Gijs A. Bartholomeus, Aahlad Manas Puli, Pim A. de Jong, Tim Leiner, Anne S. R. van Lindert, Marinus J. C. Eijkemans, and Rajesh Ranganath. 2022. “Individual Treatment Effect Estimation in the Presence of Unobserved Confounding Using Proxies: A Cohort Study in Stage III Non-Small Cell Lung Cancer.” Scientific Reports 12 (1): 5848. https://doi.org/10.1038/s41598-022-09775-9.\n\n\n“DifferentiableUniverseInitiative/Jax_cosmo.” 2024. Differentiable Universe Initiative. https://github.com/DifferentiableUniverseInitiative/jax_cosmo.\n\n\n“SciML: Open Source Software for Scientific Machine Learning.” n.d. Accessed March 14, 2024. https://sciml.ai."
  },
  {
    "objectID": "posts/240308-jaxopt-vs-r-vs-julia/index.html#footnotes",
    "href": "posts/240308-jaxopt-vs-r-vs-julia/index.html#footnotes",
    "title": "The need for speed, performing simulation studies in R, JAX and Julia",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne concrete example with my own previous simulation studies (Amsterdam and Ranganath 2023) is that I used first used jax.numpy arrays for different parameters and then a scipy function to create all combinations of these parameters. Creating this grid of parameters this way forced copying of jax.numpy arrays from the GPU back to CPU and then copying the grid back to GPU. This made the entire process orders of magnitude slower (it was a large grid O(1e12)). Gotchas like these can bite you. Also, JAX relies on pure functions that cannot depend on global variables.↩︎"
  },
  {
    "objectID": "posts/240322-mice-partial-residual-plot.html",
    "href": "posts/240322-mice-partial-residual-plot.html",
    "title": "Partial residual plots with multiply imputed data",
    "section": "",
    "text": "In an earlier blog post I show how to plot the dependence of a response variable on covariate, both conditional on other covariates with a partial residual plot In this post I investigate how to do this when there are missing values using package mice.\nThis post will rely in Hanne Oberman’s vignette on ggmice, a plotting package for mice::mids objects."
  },
  {
    "objectID": "posts/240322-mice-partial-residual-plot.html#data-and-imputation",
    "href": "posts/240322-mice-partial-residual-plot.html#data-and-imputation",
    "title": "Partial residual plots with multiply imputed data",
    "section": "Data and imputation",
    "text": "Data and imputation\nIn this post we’ll use the boys dataset which is provided in the mice package. The mice package implements multiple imputation through chained equations1.\n\nlibrary(mice)\nlibrary(ggplot2); theme_set(theme_bw())\n\ndf &lt;- boys\n\nnimps &lt;- 5\nimp &lt;- mice(df, m = nimps, method = \"pmm\")"
  },
  {
    "objectID": "posts/240322-mice-partial-residual-plot.html#partial-residual-plot-with-complete-data",
    "href": "posts/240322-mice-partial-residual-plot.html#partial-residual-plot-with-complete-data",
    "title": "Partial residual plots with multiply imputed data",
    "section": "Partial residual plot with complete data",
    "text": "Partial residual plot with complete data\nWe’ll assume we’re interested in how weight (wgt) depends on height (hgt), corrected for all other variables. Both have missing values in the dataset.\nWe can use one of the imputed datasets to show how partial residual plots are made with complete data. With complete data, partial residual plots can be created like so:\n\ndf1 &lt;- complete(imp, 1)\n\nget_partial_resid &lt;- function(data) {\n  fit &lt;- lm(wgt~., data=data)\n  yresid &lt;- resid(fit)\n  return(yresid + coef(fit)['hgt'] * data$hgt)\n}\n\nplotdata1 &lt;- data.frame(hgt=df1$hgt, y=get_partial_resid(df1))\n\nggplot(plotdata1, aes(x=hgt, y=y)) + geom_point()\n\n\n\n\n\n\n\nFigure 1: Partial residual plot on complete data\n\n\n\n\n\nNote how this differs from the marginal association between hgt and wgt. The difference between the plots is explained by other covariates that are correlated both with hgt and wgt.\n\nggplot(df1, aes(x=hgt, y=wgt)) + geom_point()\n\n\n\n\n\n\n\nFigure 2: marginal association between age and height"
  },
  {
    "objectID": "posts/240322-mice-partial-residual-plot.html#putting-them-together",
    "href": "posts/240322-mice-partial-residual-plot.html#putting-them-together",
    "title": "Partial residual plots with multiply imputed data",
    "section": "Putting them together",
    "text": "Putting them together\nThe issue with the partial residual plot Figure 1 is that the residuals depend on the imputed values for all variables. How to go about this? We can treat the residuals as we would ‘coefficients’ in imputation and use Rubin’s rules on them. Let’s see how to do this with mice.\n\nimpdfs &lt;- complete(imp, \"all\")\nimpdfs &lt;- lapply(impdfs, function(data) data.frame(data, partial_resid=get_partial_resid(data)))\nimpdfs &lt;- lapply(1:nimps, function(i) data.frame(impdfs[[i]], impidx=i))\nimpdf_long &lt;- do.call(rbind, impdfs)\n\nNote that we cannot directly pool the residuals because they may be correlated with the imputed values for hgt. Pooling imputed values for hgt and the partial residual removes this correlation. Instead we could just plot all the values.\n\nggplot(impdf_long, aes(x=hgt, y=partial_resid)) + geom_point(aes(shape=factor(impidx)), alpha=0.5)\n\n\n\n\n\n\n\nFigure 3: partial residual plot with imputed datasets\n\n\n\n\n\nIn the future I’d like to learn how to make good bivariate estimates of these points (e.g. fitting a bivariate normal distribution). This should for example also come up in calculating sensitivity and specificity on multiply imputed datasets, as these are clearly correlated. Fully bayesian imputation is another possibility obviously."
  },
  {
    "objectID": "posts/240322-mice-partial-residual-plot.html#footnotes",
    "href": "posts/240322-mice-partial-residual-plot.html#footnotes",
    "title": "Partial residual plots with multiply imputed data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nmultiple imputation with chained equations sequentially imputes values for all variables with missing values by building prediction models for each variable based on other variables. This imputation is done multiple times with multiple random seeds and thus results in a number of different imputed datasets. A typical analysis workflow is to do these imputations and on each imputed dataset fit a model of interest. The coefficients of these models can then be pooled using Rubin’s rules.↩︎"
  },
  {
    "objectID": "posts/190816-lm-functional-form.html",
    "href": "posts/190816-lm-functional-form.html",
    "title": "Finding the functional form for multiple linear regression",
    "section": "",
    "text": "A frequent question that comes up when modeling continuous outcomes with multiple linear regression is what the correct functional form for the relationship between the independent variables is. TLDR: the answer is a partial residual plot. Here I will generate some data to illustrate this"
  },
  {
    "objectID": "posts/190816-lm-functional-form.html#functional-relationship-in-linear-regression",
    "href": "posts/190816-lm-functional-form.html#functional-relationship-in-linear-regression",
    "title": "Finding the functional form for multiple linear regression",
    "section": "",
    "text": "A frequent question that comes up when modeling continuous outcomes with multiple linear regression is what the correct functional form for the relationship between the independent variables is. TLDR: the answer is a partial residual plot. Here I will generate some data to illustrate this"
  },
  {
    "objectID": "posts/190816-lm-functional-form.html#data",
    "href": "posts/190816-lm-functional-form.html#data",
    "title": "Finding the functional form for multiple linear regression",
    "section": "Data",
    "text": "Data\n\nsuppressMessages({require(ggplot2); theme_set(theme_bw())})\nset.seed(12345)\nN = 1000\nx &lt;- runif(N, min = 0, max=2*pi)\nw &lt;- .5*x + sin(x) + rnorm(N, sd=.25)\nsy &lt;- rnorm(N, sd=.1)\ny &lt;- x + w + sy\ndf = data.frame(x,w,y)\n\nThe data consists of two real-valued ‘independent’ variables \\(x,w\\), where\n\\[\n\\begin{align}\nx &\\sim U(0, 2 \\pi) \\\\\n\\epsilon_w &\\sim N(0, 0.25) \\\\\n\\epsilon_y &\\sim N(0, 0.1) \\\\\nw &:= \\frac{x}{2} + \\sin(x) \\\\\ny &:= x + w + \\epsilon_y\n\\end{align}\n\\]\nClearly \\(y\\) is linear in both \\(x\\) and \\(w\\). A plot of the data:\n\nggplot(df, aes(x=x, y=w,col=y, size=y)) + \n  geom_point() + theme_minimal()\n\n\n\n\njoint distribution of x, w, y"
  },
  {
    "objectID": "posts/190816-lm-functional-form.html#plots",
    "href": "posts/190816-lm-functional-form.html#plots",
    "title": "Finding the functional form for multiple linear regression",
    "section": "Plots",
    "text": "Plots\n\nIncorrect: marginal association\nLet’s say we’re particularly interested in the relationship between \\(y\\) and \\(x\\), both conditional on \\(w\\). Looking at the marginal association between \\(y\\) and \\(x\\) with a scatterplot will set us on the wrong foot, because of the association between \\(x\\) and \\(w\\).\n\nggplot(df, aes(x=x,y=y)) + \n  geom_point()\n\n\n\n\nmarginal association between x and y\n\n\n\n\n\n\nCorrect: partial residual plot\nTo construct the correct plot, we can generate a partial residual plot, which is created with resid(lm(y~x+w))+b_x x ~ x.\nWhere b_x is the regression coefficient found through linear regression of \\(y\\) on \\(x\\) and \\(w\\). In a plot:\n\nlmfit &lt;- lm(y~x+w)\nyresid &lt;- resid(lmfit)\nb_x &lt;- coef(lmfit)['x']\nplotdata &lt;- data.frame(x, y=yresid + b_x * x)\n\nggplot(plotdata, aes(x=x,y=y)) + \n  geom_point() + \n  ylab(\"resid(lm(y~x+w)) + b_x x\")\n\n\n\n\n\n\n\nFigure 1: partial residual plot\n\n\n\n\n\n\n\nPartial residual plot when y is not linear in x\nWhat if \\(y\\) were not linear in \\(x\\)?\n\ny2 &lt;- x^2 + w + sy\nlmfit2 &lt;- lm(y2 ~ x + w)\nb_x2 &lt;- coef(lmfit2)['x']\nplotdata2 &lt;- data.frame(x, y=resid(lmfit2) + b_x2 * x)\n\nggplot(plotdata2, aes(x=x,y=y)) + \n  geom_point() + \n  ylab(\"resid(lm(y~x+w)) + b_x x\")\n\n\n\n\npartial residual plot when y not linear in x"
  },
  {
    "objectID": "posts/190816-lm-functional-form.html#conclusion",
    "href": "posts/190816-lm-functional-form.html#conclusion",
    "title": "Finding the functional form for multiple linear regression",
    "section": "Conclusion",
    "text": "Conclusion\nThe functional relationship between an outcome and a covariate in a linear regression conditional on other covariates is visualized with a partial residual plot, or:\n\nlmfit &lt;- lm(y~x+w)\nplot(resid(lmfit) + coef(lmfit)['x'] * x ~ x)"
  },
  {
    "objectID": "posts/210720-good_predictions_bad_decisions.html",
    "href": "posts/210720-good_predictions_bad_decisions.html",
    "title": "When good predictions lead to bad decisions",
    "section": "",
    "text": "A common premise in prediction research for clinical outcomes is that better predictions lead to better (informed) decisions. This causal statement, that the intervention of introducing a new prediction rule leads to better decisions and thus better outcomes, is generally not substantiated with sufficient causal arguments. We now present an example where naively introducing a validated new prediction rule can lead to worse clinical decisions.\nFor a certain cancer type there are two treatment options: treatment A and treatment B. From randomized trials, it is known that treatment A is more effective in treating the tumor than treatment B. There is no known variation of treatment effect among subgroups defined by clinical patient characteristics. However, not all patients respond well to treatment. Treatment A is a longer and more intensive treatment regimen than treatment B and leads to more side-effects. The consensus is that it is unethical to give treatment A to patients with a lower than 10% chance of surviving one year, due to the higher risk of side-effects and the lengthy treatment regimen associated with treatment A. In current clinical practice, the probability of 1-year survival is estimated using clinical characteristics. A new research group tries to improve the 1-year survival predictions using a new biomarker. The research endeavor is a success as it turns out that predicting survival with the clinical characteristics and the new biomarker is significantly more accurate than using only the clinical characteristics. A high value for the biomarker is associated with worse overall survival. Having conducted a predictive study, it is not discovered that the treatment effect of treatment A versus B is actually more in favor of treatment A for patients with higher levels of the biomarker. If the new prediction rule would be implemented naively with the same 10% cut-off for 1-year overall survival, this would lead to worse treatment decisions than without using the new prediction model. Some patients with high biomarker values will fall under the 10% cut-off based on the new biomarker, while without the biomarker they would have had a higher than 10% survival probability. This erroneously leads to not recommending treatment A, even though these patients have a high benefit of treatment A.\nNote that this is not an unreasonable example for cancer, as aggressive / fast-growing cancers tend to respond better to treatments like chemotherapy and radiotherapy. One example is non-seminoma versus seminoma testicular cancer."
  },
  {
    "objectID": "posts/210720-good_predictions_bad_decisions.html#defining-the-policies",
    "href": "posts/210720-good_predictions_bad_decisions.html#defining-the-policies",
    "title": "When good predictions lead to bad decisions",
    "section": "Defining the policies",
    "text": "Defining the policies\nThe current clinical policy is:\n\\[\\pi_0(x) = \\mathbf{I}_{E[y|x] &gt; 0}\\]\nSo we should give treatment \\(t=1\\) whenever the expected outcome exceeds the reference cut-off. Let the true data generating mechanism be as following:\n\\[y = \\beta z t + x - z\\]\nAs \\(y\\) is a deterministic function of \\(t,x,z\\), we drop the expectation symbol in the following discussion. Let \\(x,z \\sim \\mathbf{U}(-1,1)\\) be independent variables following a uniform distribution between -1 and 1. We can new express the baseline policy as \\[\\pi_0(x) = \\mathbf{I}_{y|x &gt; 0} = \\mathbf{I}_{E_{z}[y|x,z]&gt;0} \\iff \\mathbf{I}_{x &gt; 0}\\]\nA naive implementation of the new prediction rule incorporating \\(z\\) would lead to the policy \\(\\pi_z(x,z) = \\mathbf{I}_{y|x,z &gt; 0}\\). Plugging in the data generating mechanism we can identify\n\\[\\begin{aligned}\n  y|x,z &= E_{t \\sim \\pi_0(x)}[y|t,x,z] \\\\\n    &= E_{t \\sim \\pi_0(x)}[\\beta z t + x - z] \\\\\n    &= \\beta z E_{t \\sim \\pi_0(x)} [t] + x - z \\\\\n    &= \\beta z E_x [\\mathbf{I}_{x &gt; 0}] + x - z \\\\\n    &= 0.5 \\beta z  + x - z \\\\\n    &= x - (1 - 0.5 \\beta)z\\end{aligned}\\]\nThus \\(\\pi_z(x,z) = \\mathbf{I}_{x + (0.5 \\beta - 1)z &gt; 0}\\).\nFrom the data generating mechanism it is clear that the conditional average treatment effect reduces to\n\\[\\begin{aligned}\n  \\text{CATE(x,z)} &= E[y|\\text{do}(t=1),x,z] - E[y|\\text{do}(t=0),x,z] \\\\\n                   &= \\beta z - 0\\end{aligned}\\]\nThe policy that maximizes the outcome \\(y\\) is \\(\\pi_{\\text{max}(y)}(z) = \\mathbf{I}_{z &gt; 0}\\) as \\(\\text{do}(t=1)\\) leads to better outcomes if and only if \\(z&gt;0\\). To conform with the ethical consensus that the intensive treatment is justified when \\(y|\\text{do}(t),x,z&gt;0\\), we set\n\\[\\begin{aligned}\n    \\pi^*(x,z) &= \\mathbf{I}_{y|\\text{do}(t=1),x,z &gt; 0} \\\\\n           &= \\mathbf{I}_{\\beta z * 1 + x - z &gt; 0} \\\\\n           &= \\mathbf{I}_{x + (\\beta - 1) z &gt; 0}\\end{aligned}\\]"
  },
  {
    "objectID": "posts/210720-good_predictions_bad_decisions.html#expected-utility-of-different-policies",
    "href": "posts/210720-good_predictions_bad_decisions.html#expected-utility-of-different-policies",
    "title": "When good predictions lead to bad decisions",
    "section": "Expected utility of different policies",
    "text": "Expected utility of different policies\nWe can now calculate the expected utility of the different policies \\(\\pi \\in \\{\\pi_0,\\pi_z,\\pi_{\\text{max}(y)},\\pi^*\\}\\) as the expected outcome, \\(U(\\pi) = E_{x,z}E_{t\\sim \\pi(x,z)}y|t,x,z = E_{x,z}\\beta z \\pi(x,z) + x - z\\). The calculation of these expected utilities depends on the treatment effect, which will we assume to be \\(\\beta = 1.5\\). We will use the marginal indepence of \\(x\\) and \\(z\\) to equate \\(E_{x,z}[.] = E_z E_x [.]\\).\n\\[\\begin{aligned}\n    U(\\pi_0) &= E_z E_x [\\beta z \\mathbf{I}_{x &gt; 0} + x - z] \\\\\n         &= \\beta E_z z E_x \\mathbf{I}_{x&gt;0} \\\\\n         &= \\beta E_z z 0.5 \\\\\n         &= 0\\end{aligned}\\]\nWe have \\(\\pi_z(x,z) = \\mathbf{I}_{x - (1 - 0.5 \\beta)z &gt; 0} = \\mathbf{I}_{x &gt; 0.25z}\\)\n\\[\\begin{aligned}\n    U(\\pi_z) &= E_z E_x [\\beta z \\mathbf{I}_{x &gt; 0.25z} + x - z] \\\\\n         &= \\beta E_z z E_x \\mathbf{I}_{x &gt; 0.25z} \\\\\n         &= \\beta E_z z \\text{Pr}(x &gt; 0.25z) \\\\\n         &=^1 \\frac{\\beta}{2} \\int_{-1}^{1} z \\text{Pr}(x &gt; 0.25z) dz\\\\\n         &=^2 \\frac{\\beta}{2} \\int_{-1}^{1} z (1 - \\frac{0.25z+1}{2}) dz\\\\\n         &= \\frac{\\beta}{4} \\int_{-1}^{1} z (1 - 0.25z)dz \\\\\n         &= \\frac{\\beta}{4} \\left[ \\frac{1}{2} z^2 - \\frac{0.25}{3}z^3 + C \\right]_{-1}^1 \\\\\n         &= \\frac{\\beta}{4} \\left( ( \\frac{1}{2} - \\frac{0.25}{3}) - ( \\frac{1}{2} + \\frac{0.25}{3}) \\right) \\\\\n         &= - \\frac{\\beta}{2} \\frac{0.25}{3} \\\\\n         &= - 0.075\\end{aligned}\\]\nWhere in \\(^1\\) we used the \\(U(-1,1)\\) distribution of \\(z\\) and in \\(^2\\) we used the probability density function of \\(x\\) and the fact that \\(-1 &lt; 0.25z &lt; 1\\). This demonstrates that the policy following the (accurate!) prediction model \\(y|x,z\\) leads to worse clinical outcomes than the previous situation that relied on \\(x\\) only.\nThe reader may verify that \\[\\begin{aligned}\n    U(\\pi_{\\text{max}(y)}) &= 0.375 \\\\\n    U(\\pi^*) &= 0.125\\end{aligned}\\]"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Here is a selection of my talks.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nSubtitle\n\n\n\n\n\n\nMay 7, 2024\n\n\nAn intro to causal inference and its uses in radiotherapy\n\n\nESTRO - Understanding dose-effects: Can we go beyond association - symposium\n\n\n\n\nApr 18, 2024\n\n\nAI and its (mis)uses in medical research and practice\n\n\nInfection and Immunity spring meeting\n\n\n\n\nMar 12, 2024\n\n\nWouter’s twitter X-peri(ments/ences)\n\n\nData Science and Biostatistics department lunch talk\n\n\n\n\nAug 10, 2023\n\n\nThe value of observational causal inference for medical decision making\n\n\nMLHC causality pre-conference workshop\n\n\n\n\nJun 24, 2023\n\n\nMy risk model is super accurate so it will be useful for treatment decision making, right? Wrong!\n\n\nCHIL 2023 - lightning talk\n\n\n\n\nMar 1, 2023\n\n\nIndividual treatment effect estimation in the presence of unobserved confounding using proxies\n\n\nSeminar at Manchester University\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#what-is-ai-1",
    "href": "talks/240418-ii-spring-meeting/index.html#what-is-ai-1",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "What is AI?",
    "text": "What is AI?\n\n\n\n\n\n\nWhat is artificial intelligence?\n\n\ncomputers doing tasks that normally require intelligence 1\n\n\n\n\n\n\n\n\n\n\nWhat is artificial general intelligence?\n\n\nGeneral purpose AI that performs a range of tasks in different domains like humans\n\n\n\n\nthese are my own definitions"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#ai-subsumes-rule-based-systems-and-machine-learning",
    "href": "talks/240418-ii-spring-meeting/index.html#ai-subsumes-rule-based-systems-and-machine-learning",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "AI subsumes rule-based systems and machine learning",
    "text": "AI subsumes rule-based systems and machine learning\n\nRule-based AI: knowledge base of rules\nMachine learning: statistical learning from examples #- (traditional) machine learning (logistic regression, SVM, RF, #GBM) #- modern machine learning: deep learning and foundation models"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#rule-based-systems-are-ai",
    "href": "talks/240418-ii-spring-meeting/index.html#rule-based-systems-are-ai",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "Rule-based systems are AI",
    "text": "Rule-based systems are AI\n\nrule: all cows are animals\nobservation: this is a cow \\(\\to\\) it is an animal\napplications:\n\nmedication interaction checkers\nbedside patient monitors"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#ml-tasks",
    "href": "talks/240418-ii-spring-meeting/index.html#ml-tasks",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "ML tasks",
    "text": "ML tasks\n\n\n\n\n\n\n\n\ndata:\n\n\n\ni\nlength\nweight\nsex\n\n\n\n\n1\n137\n30\nboy\n\n\n2\n122\n24\ngirl\n\n\n3\n101\n18\ngirl\n\n\n…\n…\n…\n…\n\n\n\n\n\\[l_i,w_i,s_i \\sim p(l,w,s)\\]"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#ml-tasks-generation",
    "href": "talks/240418-ii-spring-meeting/index.html#ml-tasks-generation",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "ML tasks: generation",
    "text": "ML tasks: generation\n\n\n\n\n\n\n\n\nuse samples to learn model \\(p_{\\theta}\\) for joint distribution \\(p\\) \\[\n  l_j,w_j,s_j \\sim p_{\\theta}(l,w,s)\n\\]"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#ml-tasks-conditional-generation",
    "href": "talks/240418-ii-spring-meeting/index.html#ml-tasks-conditional-generation",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "ML tasks: conditional generation",
    "text": "ML tasks: conditional generation\n\n\n\n\n\n\n\n\nuse samples to learn model for conditional distribution \\(p\\) \\[\n  l_j,w_j \\sim p_{\\theta}(l,w|s=\\text{boy})\n\\]\n\n\n\n\n\ntask\n\n\n\n\n\ngeneration\n\\(l_j,w_j,s_j \\sim p_{\\theta}(l,w,s)\\)"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#ml-tasks-conditional-generation-2",
    "href": "talks/240418-ii-spring-meeting/index.html#ml-tasks-conditional-generation-2",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "ML tasks: conditional generation 2",
    "text": "ML tasks: conditional generation 2\n\n\n\n\n\n\n\n\nuse samples to learn model for conditional distribution \\(p\\) of one variable \\[\ns_j \\sim p_{\\theta}(s|l=l',w=w')\n\\]\n\n\n\n\n\ntask\n\n\n\n\n\ngeneration\n\\(l_j,w_j,s_j \\sim p_{\\theta}(l,w,s)\\)\n\n\nconditional generation\n\\(l_j,w_j \\sim p_{\\theta}(l,w|s=\\text{boy})\\)"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#ml-tasks-discrimination",
    "href": "talks/240418-ii-spring-meeting/index.html#ml-tasks-discrimination",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "ML tasks: discrimination",
    "text": "ML tasks: discrimination\n\n\n\n\n\n\n\n\ncall this one variable outcome and classify when expected value passes threshold (e.g. 0.5): \\[\ns_j = p_{\\theta}(s|l=l',w=w') &gt; 0.5\n\\]\n\n\n\n\n\ntask\n\n\n\n\n\ngeneration\n\\(l_j,w_j,s_j \\sim p_{\\theta}(l,w,s)\\)\n\n\nconditional generation\n\\(l_j,w_j \\sim p_{\\theta}(l,w|s=\\text{boy})\\)\n\n\ndiscrimination\n\\(p_{\\theta}(s|l=l_i,w=w_i) &gt; 0.5\\)"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#ml-tasks-reinforcement-learning",
    "href": "talks/240418-ii-spring-meeting/index.html#ml-tasks-reinforcement-learning",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "ML tasks: reinforcement learning",
    "text": "ML tasks: reinforcement learning\n\ne.g. computers playing games\nmaybe not so useful for clinical research as requires many experiments"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#machine-learning-is-statistical-learning-with-flexible-models",
    "href": "talks/240418-ii-spring-meeting/index.html#machine-learning-is-statistical-learning-with-flexible-models",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "Machine learning is statistical learning with flexible models",
    "text": "Machine learning is statistical learning with flexible models\n\n\n- There is no fundamental difference between statistics and machine learning\n- both optimize parameters to improve some criterion (loss / likelihood) that measures model fit to data\n- models used in machine learning are more flexible"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#ml-models-can-fit-more-functions-but-also-more-likely-to-overfit",
    "href": "talks/240418-ii-spring-meeting/index.html#ml-models-can-fit-more-functions-but-also-more-likely-to-overfit",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "ML models can fit more functions but also more likely to overfit",
    "text": "ML models can fit more functions but also more likely to overfit"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#should-pick-the-right-amount-of-model-complexity",
    "href": "talks/240418-ii-spring-meeting/index.html#should-pick-the-right-amount-of-model-complexity",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "Should pick the ‘right’ amount of model complexity",
    "text": "Should pick the ‘right’ amount of model complexity"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#what-is-a-large-language-model-like-chatgpt-1",
    "href": "talks/240418-ii-spring-meeting/index.html#what-is-a-large-language-model-like-chatgpt-1",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "What is a large-language model like chatGPT?",
    "text": "What is a large-language model like chatGPT?\n\n\n\n\n\n\nWhat is chatGPT?\n\n\na stochastic auto-regressive next-word predictor with a chatbot interface\n\n\n\n\ntrained by predicting the next &lt;…&gt;\n\nin a large corpus of text\nwith a large model\nfor a long time on expensive hardware"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#auto-regressive-conditional-generation",
    "href": "talks/240418-ii-spring-meeting/index.html#auto-regressive-conditional-generation",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "auto-regressive conditional generation:",
    "text": "auto-regressive conditional generation:\n\\[\\begin{align}\n    \\text{word}_1 &\\sim p_{\\text{chatGPT}}(\\text{word}|\\text{prompt})\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#auto-regressive-conditional-generation-1",
    "href": "talks/240418-ii-spring-meeting/index.html#auto-regressive-conditional-generation-1",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "auto-regressive conditional generation:",
    "text": "auto-regressive conditional generation:\n\\[\\begin{align}\n    \\text{word}_1 &\\sim p_{\\text{chatGPT}}(\\text{word}|\\text{prompt})\\\\\n    \\text{word}_2 &\\sim p_{\\text{chatGPT}}(\\text{word}|\\text{word}_1,\\text{prompt})\n\\end{align}\\]"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#auto-regressive-conditional-generation-2",
    "href": "talks/240418-ii-spring-meeting/index.html#auto-regressive-conditional-generation-2",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "auto-regressive conditional generation:",
    "text": "auto-regressive conditional generation:\n\\[\\begin{align}\n    \\text{word}_1 &\\sim p_{\\text{chatGPT}}(\\text{word}|\\text{prompt})\\\\\n    \\text{word}_2 &\\sim p_{\\text{chatGPT}}(\\text{word}|\\text{word}_1,\\text{prompt})\n\\end{align}\\]"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#auto-regressive-conditional-generation-3",
    "href": "talks/240418-ii-spring-meeting/index.html#auto-regressive-conditional-generation-3",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "auto-regressive conditional generation:",
    "text": "auto-regressive conditional generation:\n\\[\\begin{align}\n    \\text{word}_1 &\\sim p_{\\text{chatGPT}}(\\text{word}|\\text{prompt})\\\\\n    \\text{word}_2 &\\sim p_{\\text{chatGPT}}(\\text{word}|\\text{word}_1,\\text{prompt})\\\\\n    \\text{word}_n &\\sim p_{\\text{chatGPT}}(\\text{word}|\\text{word}_{n-1},\\ldots,\\text{word}_1,\\text{prompt})\n\\end{align}\\]"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#auto-regressive-conditional-generation-4",
    "href": "talks/240418-ii-spring-meeting/index.html#auto-regressive-conditional-generation-4",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "auto-regressive conditional generation:",
    "text": "auto-regressive conditional generation:\n\\[\\begin{align}\n    \\text{word}_1 &\\sim p_{\\text{chatGPT}}(\\text{word}|\\text{prompt})\\\\\n    \\text{word}_2 &\\sim p_{\\text{chatGPT}}(\\text{word}|\\text{word}_1,\\text{prompt})\\\\\n    \\text{word}_n &\\sim p_{\\text{chatGPT}}(\\text{word}|\\text{word}_{n-1},\\ldots,\\text{word}_1,\\text{prompt})\\\\\n    \\text{STOP}   &\\sim p_{\\text{chatGPT}}(\\text{word}|\\text{word}_{n-1},\\ldots,\\text{word}_1,\\text{prompt})\n\\end{align}\\]"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#gpt-4-scale",
    "href": "talks/240418-ii-spring-meeting/index.html#gpt-4-scale",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "GPT-4 scale",
    "text": "GPT-4 scale"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#rule-based-vs-llms",
    "href": "talks/240418-ii-spring-meeting/index.html#rule-based-vs-llms",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "rule-based vs LLMs",
    "text": "rule-based vs LLMs\n\n\n\ndeduction from explicit knowledge\nknowledge verifiable and fast\nconstrained to deducible\n\n\n\n\n\n\n\n\nextracted from observed data\nunverifiable and compute intensive\n“chatGPT seems to know(?) much”"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#ml-versus-statistics-when-to-use-what",
    "href": "talks/240418-ii-spring-meeting/index.html#ml-versus-statistics-when-to-use-what",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "ML versus statistics, when to use what",
    "text": "ML versus statistics, when to use what\n\n\nmachine learning\n\nhave more data\nmore complex functions (images)\n\n\nstatistics (e.g. GLMs)\n\nless data\nmore domain knowledge"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#a-sobering-note",
    "href": "talks/240418-ii-spring-meeting/index.html#a-sobering-note",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "A sobering note",
    "text": "A sobering note\n- ML in medicine has been ‘hot’ since at least the 90s (Cooper et al. 1997)\n- not much evidence that it outperforms regression on most tasks (Christodoulou et al. 2019)\n- though many poorly performed studies (Dhiman et al. 2022)"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#two-questions",
    "href": "talks/240418-ii-spring-meeting/index.html#two-questions",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "two questions",
    "text": "two questions\nQuestion 1\n\nprediction model of \\(Y|X\\) fits the data really well (AUC = 0.99 and perfect calibration)\nwill changing \\(X\\) induce a change \\(Y\\)?\n\nQuestion 2\n\nGive statins when risk of cardiovascular event in 10 years exceeds 10%\nML model based on age, medication history, cardiac CT-scan predicts this very well\nwill using this model for treatment decisions improve patient outcomes?"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#improving-the-world-is-a-causal-task",
    "href": "talks/240418-ii-spring-meeting/index.html#improving-the-world-is-a-causal-task",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "Improving the world is a causal task",
    "text": "Improving the world is a causal task\n\nstatistics / ML: what to expect when we passively observe the world\nnot how we can intervene to make things better, this requires causality\nQuestion 1\n\nyellowish fingers predict lung cancer, paint fingers to skin color?\nweight loss predicts death in lung cancer, send patients to couch with McDonalds?"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#when-accurate-prediction-models-yield-harmful-self-fulfilling-prophecies",
    "href": "talks/240418-ii-spring-meeting/index.html#when-accurate-prediction-models-yield-harmful-self-fulfilling-prophecies",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "When accurate prediction models yield harmful self-fulfilling prophecies",
    "text": "When accurate prediction models yield harmful self-fulfilling prophecies"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#prediction-modeling-is-very-popular-in-medical-research",
    "href": "talks/240418-ii-spring-meeting/index.html#prediction-modeling-is-very-popular-in-medical-research",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "Prediction modeling is very popular in medical research",
    "text": "Prediction modeling is very popular in medical research"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#treatment-naive-risk-models",
    "href": "talks/240418-ii-spring-meeting/index.html#treatment-naive-risk-models",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "Treatment-naive risk models",
    "text": "Treatment-naive risk models"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#is-this-obvious",
    "href": "talks/240418-ii-spring-meeting/index.html#is-this-obvious",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "Is this obvious?",
    "text": "Is this obvious?\n\n\n\n\n\n\nTip\n\n\nIt may seem obvious that you should not ignore historical treatments in your prediction models, if you want to improve treatment decisions, but many of these models are published daily, and some guidelines even allow for implementing these models based on predictve performance only"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#other-risk-models",
    "href": "talks/240418-ii-spring-meeting/index.html#other-risk-models",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "Other risk models:",
    "text": "Other risk models:\n- condition on given treatment and traits\n- unobserved confounding (hat type) leads to wrong treatment decisions"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#recommended-validation-practices-do-not-protect-against-harm",
    "href": "talks/240418-ii-spring-meeting/index.html#recommended-validation-practices-do-not-protect-against-harm",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "Recommended validation practices do not protect against harm",
    "text": "Recommended validation practices do not protect against harm\nbecause they do not evaluate the policy change"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#bigger-data-does-not-protect-against-harmful-risk-models",
    "href": "talks/240418-ii-spring-meeting/index.html#bigger-data-does-not-protect-against-harmful-risk-models",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "Bigger data does not protect against harmful risk models",
    "text": "Bigger data does not protect against harmful risk models"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#more-flexible-models-do-not-protect-against-harmful-risk-models",
    "href": "talks/240418-ii-spring-meeting/index.html#more-flexible-models-do-not-protect-against-harmful-risk-models",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "More flexible models do not protect against harmful risk models",
    "text": "More flexible models do not protect against harmful risk models"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#gap-between-prediction-accuracy-and-value-for-decision-making",
    "href": "talks/240418-ii-spring-meeting/index.html#gap-between-prediction-accuracy-and-value-for-decision-making",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "Gap between prediction accuracy and value for decision making",
    "text": "Gap between prediction accuracy and value for decision making"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#section",
    "href": "talks/240418-ii-spring-meeting/index.html#section",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "",
    "text": "What to do?"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#section-1",
    "href": "talks/240418-ii-spring-meeting/index.html#section-1",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "",
    "text": "What to do?\n\n\nEvaluate policy change (cluster randomized controlled trial)\nBuild models that are likely to have value for decision making"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#prediction-under-intervention-models",
    "href": "talks/240418-ii-spring-meeting/index.html#prediction-under-intervention-models",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "Prediction-under-intervention models",
    "text": "Prediction-under-intervention models\nPredict outcome under hypothetical intervention of giving certain treatment"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#when-developing-risk-models",
    "href": "talks/240418-ii-spring-meeting/index.html#when-developing-risk-models",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "When developing risk models,",
    "text": "When developing risk models,\nalways discuss:\n\n\n\n\n\n1. what is effect on treatment policy?\n2. what is effect on patient outcomes?"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#take-aways",
    "href": "talks/240418-ii-spring-meeting/index.html#take-aways",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "take-aways",
    "text": "take-aways\n\nAI subsumes rule-based programs and machine learning\nmachine learning is statistical learning from data with flexible models\nchatGPT does auto-regressive next-word prediction\nchatGPT produces beautiful mistakes: eloquently written logical fallacies\nprediction: what to expect when passively observing the world\ncausality: what happens when I change something?\nprediction models can cause harmful self-fulfilling prophecies when used for decision making\nwhen building prediction models for decision support, you cannot ignore decisions on the treatments in historic data\nmodels for prediction-under-intervention have foreseeable effects when used for decision making\nultimate test of model utility is determined by outcomes in (cluster) RCT"
  },
  {
    "objectID": "talks/240418-ii-spring-meeting/index.html#references",
    "href": "talks/240418-ii-spring-meeting/index.html#references",
    "title": "AI and its (mis)uses in medical research and practice",
    "section": "",
    "text": "thank you\n\n\n\n©Wouter van Amsterdam (: WvanAmsterdam)\n\n\n\n\nAmsterdam, Wouter A. C. van, Nan van Geloven, Jesse H. Krijthe, Rajesh Ranganath, and Giovanni Ciná. 2024. “When Accurate Prediction Models Yield Harmful Self-Fulfilling Prophecies.” arXiv. https://doi.org/10.48550/arXiv.2312.01210.\n\n\nAmsterdam, Wouter A. C. van, Pim A. de Jong, Joost J. C. Verhoeff, Tim Leiner, and Rajesh Ranganath. 2024. “From Algorithms to Action: Improving Patient Care Requires Causality.” arXiv. https://doi.org/10.48550/arXiv.2209.07397.\n\n\nChristodoulou, Evangelia, Jie Ma, Gary S. Collins, Ewout W. Steyerberg, Jan Y. Verbakel, and Ben Van Calster. 2019. “A Systematic Review Shows No Performance Benefit of Machine Learning over Logistic Regression for Clinical Prediction Models.” Journal of Clinical Epidemiology 110 (June): 12–22. https://doi.org/10.1016/j.jclinepi.2019.02.004.\n\n\nCooper, Gregory F., Constantin F. Aliferis, Richard Ambrosino, John Aronis, Bruce G. Buchanan, Richard Caruana, Michael J. Fine, et al. 1997. “An Evaluation of Machine-Learning Methods for Predicting Pneumonia Mortality.” Artificial Intelligence in Medicine 9 (2): 107–38. https://doi.org/10.1016/S0933-3657(96)00367-3.\n\n\nDhiman, Paula, Jie Ma, Constanza L. Andaur Navarro, Benjamin Speich, Garrett Bullock, Johanna A. A. Damen, Lotty Hooft, et al. 2022. “Methodological Conduct of Prognostic Prediction Models Developed Using Machine Learning in Oncology: A Systematic Review.” BMC Medical Research Methodology 22 (1): 101. https://doi.org/10.1186/s12874-022-01577-x."
  },
  {
    "objectID": "talks/230624-chill-lightning-talk/index.html#section-31",
    "href": "talks/230624-chill-lightning-talk/index.html#section-31",
    "title": "My risk model is super accurate so it will be useful for treatment decision making, right? Wrong!",
    "section": "",
    "text": "©Wouter van Amsterdam (: WvanAmsterdam)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wouter van Amsterdam, MD, PhD",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     Linkedin\n  \n  \n    \n     ORCID\n  \n  \n    \n     github\n  \n  \n    \n     pubmed\n  \n  \n    \n     google-scholar\n  \n\n  \n  \n\n\nWouter van Amsterdam is an assistant professor at the University Medical Center Utrecht, working on methods and applications of machine learning and causal inference for health care. His focus is the intersection of machine learning and causal inference. This intersection goes two ways: machine learning can improve typical causal inference tasks such as estimating individual treatment effects and doing prediction-under-intervention. On the other hand, causal inference provides a formal way to understanding and improving prediction model generalization and robustness. Wouter holds degrees in Physics (BSc), Medicine (MD), epidemiology (MSc) and a PhD on machine learning for healthcare.\nMaster students with a background in statistics or machine learning who want to work with me are welcome to contact me. Students enrolled at Utrecht University or UMC Utrecht can find open master projects on konjoin\n\n\n\nUniversity Medical Center Utrecht / Utrecht University\n2010 | BSc. Physics\n2017 | M.D.\n2021 | MSc. Epidemiology (med. statistics track)\n2022 | PhD. machine learning for healthcare | co-advised by Rajesh Ranganath from NYU\n\n\n\nUMC Utrecht| Assistant Professor | 2023 - now\nBabylon Health | Senior Research Scientist | 2021 - 2023\nCV (pdf)"
  },
  {
    "objectID": "index.html#selected-papers",
    "href": "index.html#selected-papers",
    "title": "Wouter van Amsterdam, MD, PhD",
    "section": "Selected papers",
    "text": "Selected papers\nWhen accurate prediction models yield harmful self-fulfilling prophecies (arXiv:2312.01210).\nvan Amsterdam, W. A. C., van Geloven, N., Krijthe, J. H., Ranganath, R., & Ciná, G. (2024). ML4H 2023 findings-track\npdf\nIndividual treatment effect estimation in the presence of unobserved confounding using proxies: A cohort study in stage III non-small cell lung cancer.  van Amsterdam, W. A. C., Verhoeff, J. J. C., Harlianto, N. I., Bartholomeus, G. A., Puli, A. M., de Jong, P. A., Leiner, T., van Lindert, A. S. R., Eijkemans, M. J. C., & Ranganath, R. (2022). Scientific Reports\npdf\nConditional average treatment effect estimation with marginally constrained models.\nvan Amsterdam, W. A. C., & Ranganath, R. (2023). Journal of Causal Inference\npdf\nDecision making in cancer: Causal questions require causal answers.\nvan Amsterdam, W. A. C., de Jong, P. A., Suijkerbuijk, K. P. M., Verhoeff, J. J. C., Leiner, T., & Ranganath, R. (2022). ArXiv pre-print\npdf\nEliminating biasing signals in lung cancer images for prognosis predictions with deep learning.\nvan Amsterdam, W. A. C., Verhoeff, J. J. C., de Jong, P. A., Leiner, T., & Eijkemans, M. J. C. (2019). Npj Digital Medicine\npdf\nMore papers at google scholar"
  },
  {
    "objectID": "index.html#talks",
    "href": "index.html#talks",
    "title": "Wouter van Amsterdam, MD, PhD",
    "section": "Talks",
    "text": "Talks\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nSubtitle\n\n\n\n\n\n\nMay 7, 2024\n\n\nAn intro to causal inference and its uses in radiotherapy\n\n\nESTRO - Understanding dose-effects: Can we go beyond association - symposium\n\n\n\n\nApr 18, 2024\n\n\nAI and its (mis)uses in medical research and practice\n\n\nInfection and Immunity spring meeting\n\n\n\n\nMar 12, 2024\n\n\nWouter’s twitter X-peri(ments/ences)\n\n\nData Science and Biostatistics department lunch talk\n\n\n\n\nAug 10, 2023\n\n\nThe value of observational causal inference for medical decision making\n\n\nMLHC causality pre-conference workshop\n\n\n\n\nJun 24, 2023\n\n\nMy risk model is super accurate so it will be useful for treatment decision making, right? Wrong!\n\n\nCHIL 2023 - lightning talk\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#teaching",
    "href": "index.html#teaching",
    "title": "Wouter van Amsterdam, MD, PhD",
    "section": "Teaching",
    "text": "Teaching\n\n2024 Introduction to Causal Inference and Causal Data Science summer school\n2023 Big Data summer school"
  },
  {
    "objectID": "index.html#posts",
    "href": "index.html#posts",
    "title": "Wouter van Amsterdam, MD, PhD",
    "section": "Posts",
    "text": "Posts\n\n\n\n\n\n\n\n\nFinding the functional form for multiple linear regression\n\n\n\n\n\n\nWouter van Amsterdam\n\n\nAug 16, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nPartial residual plots with multiply imputed data\n\n\n\n\n\n\nWouter van Amsterdam\n\n\n\n\n\n\n\n\n\n\n\n\nThe difference between intervention and counterfactuals\n\n\n\n\n\n\nWouter van Amsterdam\n\n\nJul 25, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#activities",
    "href": "index.html#activities",
    "title": "Wouter van Amsterdam, MD, PhD",
    "section": "Activities",
    "text": "Activities\n\nboard member BMS-ANed (Dutch biometrics society, board member)\ncoordinator of UMC Utrecht AI methods lab\nambassador for Applied Data Science of Utrecht University\nco-coordinator of Causal Data Science Special Interest Group of Utrecht University"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Wouter van Amsterdam, MD, PhD",
    "section": "Contact",
    "text": "Contact\nwamster3 at umcutrecht dot nl"
  }
]