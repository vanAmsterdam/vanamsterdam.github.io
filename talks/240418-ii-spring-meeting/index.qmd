---
title: AI and its (mis)uses in medical research and practice
date: 2024/04/18
draft: false
format:
    revealjs:
        incremental: true
        smaller: true
execution:
    echo: false
bibliography: references.bib
cls: apa.csl
---

# What is AI? 


## What is AI?

:::{.callout-tip}
## What is artificial intelligence?
computers doing tasks that normally require intelligence ^[these are my own definitions]
:::

:::{.fragment}
:::{.callout-tip}
## What is artificial *general* intelligence?
General purpose AI that performs a range of tasks in different domains like humans
:::
:::

## AI subsumes rule-based systems and machine learning

-   Rule-based AI: knowledge base of rules
-   Machine learning: statistical learning from examples
    -   (traditional) machine learning (logistic regression, SVM, RF,
    GBM)
    -   modern machine learning: deep learning and foundation models

## Rule-based systems are AI

-   rule: all cows are animals
-   observation: this is a cow $\to$ it is an animal
-   applications:
    - medication interaction checkers
    - bedside patient monitors


## machine learning: statistical learning from examples

-   observe examples from some distribution (age, sex, BMI, medication,
    side-effect)
-   *learn* "patterns" in the data
-   different tasks in hierarchical order:
    -   generation
    -   conditional generation
    -   discrimination
    -   reinforcement learning
        - (maybe not so useful for medicine as requires many experiments)
    
## ML tasks
::: {.columns}
:::: {.column width="50%"}
![](figs/gen_scatter.png){width="100%" fig-align="center"}
::::
:::: {.column width="50%"}
data:
$$l_i,w_i,s_i \sim p(l,w,s)$$
::::
:::

## ML tasks: generation
::: {.columns}
:::: {.column width="50%"}
![](figs/gen_full.png){width="100%" fig-align="center"}
::::
:::: {.column width="50%"}
use samples to learn model for *joint* distribution $p$
$$
  l_j,w_j,s_j \sim p_{\theta}(l,w,s)
$$
::::
:::

## ML tasks: conditional generation
::: {.columns}
:::: {.column width="50%"}
![](figs/gen_boy.png){width="100%" fig-align="center"}
::::
:::: {.column width="50%"}
use samples to learn model for *conditional* distribution $p$
$$
  l_j,w_j \sim p_{\theta}(l,w|s=\text{boy})
$$
::::
:::
|task| |
|---:|:---|
|generation|$l_j,w_j,s_j \sim p_{\theta}(l,w,s)$|

## ML tasks: conditional generation 2

::: {.columns}
:::: {.column width="50%"}
![](figs/gen_scatter.png){width="100%" fig-align="center"}
::::
:::: {.column width="50%"}
use samples to learn model for *conditional* distribution $p$
of one variable
$$
s_j \sim p_{\theta}(s|l=l_i,w=w_i)
$$
::::
:::

|task| |
|---:|:---|
|generation|$l_j,w_j,s_j \sim p_{\theta}(l,w,s)$|
|conditional generation|$l_j,w_j \sim p_{\theta}(l,w|s=\text{boy})$|

## ML tasks: discrimination
::: {.columns}
:::: {.column width="50%"}
![](figs/class_logistic.png){width="100%" fig-align="center"}
::::
:::: {.column width="50%"}
call this variable *outcome* and *classify* as when expected value passes threshold (e.g. 0.5):
$$
s_j = p_{\theta}(s|l=l_i,w=w_i) > 0.5
$$
::::
:::

|task| |
|---:|:---|
|generation|$l_j,w_j,s_j \sim p_{\theta}(l,w,s)$|
|conditional generation|$l_j,w_j \sim p_{\theta}(l,w|s=\text{boy})$|
|discrimination|$p_{\theta}(s|l=l_i,w=w_i) > 0.5$|

## ML tasks: reinforcement learning

Maybe not so useful for clinical research as requires many experiments

![](figs/reinforcement_learning.png){width="80%"}

## Machine learning is statistical learning with flexible models

:::: {.columns}
::: {.column width="50%"}

[-   There is no fundamental difference between statistics and machine
    learning]{.fragment fragment-index=1}

[-   both optimize parameters to improve some criterion (loss /
    likelihood) that measures model fit to data]{.fragment fragment-index=2}

[-   models used in machine learning are more flexible]{.fragment fragment-index=3}
:::

::: {.column width="50%"}

::: {.r-stack}

![](figs/ml_vs_stats.png){.fragment fragment-index=1}

![](figs/a_machine_learning.jpeg){.fragment fragment-index=2}

![](figs/curve_fitting_xkcd_ 2048.png){.fragment fragment-index=3}

:::
:::
:::::
::::::

## ML models can fit more functions but also more likely to overfit

::: {.r-stack}
![](figs/overfitting_underfitting1.png){.fragment}

![](figs/overfitting_underfitting2.png){.fragment}

![](figs/overfitting_underfitting3.png){.fragment}
:::

## Should pick the 'right' amount of model complexity

![](figs/bias_variance.png)

## A sobering note

[- ML in medicine has been 'hot' since at least the 90s [@cooperEvaluationMachinelearningMethods1997]]{.fragment fragment-index=1}

[- not much evidence that it outperforms regression on most tasks [@christodoulouSystematicReviewShows2019]]{.fragment fragment-index=2}

[- though many poorly performed studies [@dhimanMethodologicalConductPrognostic2022]]{.fragment fragment-index=3}

::: {.r-stack}
![](figs/ml_1997.png){.fragment fragment-index=1}

![](figs/ml_review_performance.png){.fragment fragment-index=2}

![](figs/ml_review_methods.png){.fragment fragment-index=3}
:::

---

## What is a large-language model like chatGPT?

:::{.callout-tip}
## What is chatGPT?
a stochastic auto-regressive next-word predictor with a chatbot interface
:::

- trained by predicting the next <...?>
   - in a *large* corpus of text
   - with a *large* model
   - for a *long* time on *expensive* hardware
- this is akin to conditional generation:


[$$
p_{\text{chatGPT}}(\text{next-word}|\text{previously-generated-word},\text{prompt})
$$]{.fragment}

## GPT-4 scale

::: {.r-stack}

![](figs/gpt4_size1.jpeg){.fragment fragment-index=1}

![](figs/gpt4_size2.jpeg){.fragment fragment-index=2}

![](figs/gpt4_size3.jpeg){.fragment fragment-index=3}

:::

## rule-based vs LLMs

:::: {.columns}
::: {.column width=0.5}

-   deduction from explicit knowledge

-   knowledge verifiable and fast

-   constrained to deducible

![](figs/library_bing.jpeg)

:::

::: {.column width=0.5}

-   extracted from observed data

-   unverifiable and compute intensive

-   "chatGPT seems to *know*(?) much"

![](figs/brain_bing.jpeg)

:::
::::

# Using AI in research and medical practice

## ML versus statistics, when to use what

## Safe use of AI models in medical practice

## When accurate prediction models yield harmful self-fulfilling prophecies

## The rest



:::::: frame
### Machine learning tasks

::::: columns
::: column
0.5

<figure>
<p><embed src="figs/gen_scatter.pdf" /> <embed src="figs/gen_iso.pdf" />
<embed src="figs/gen_girl.pdf" /> <embed
src="figs/gen_scatter.pdf" /></p>
</figure>
:::

::: column
0.5

<figure>
<p><embed src="figs/gen_full.pdf" /> <embed src="figs/gen_boy.pdf" />
<embed src="figs/class_logistic.pdf" /></p>
</figure>
:::
:::::
::::::

::: frame
### Machine learning task: reinforcement learning

#### Maybe not so useful for clinical research as requires many experiments

![image](figs/reinforcement_learning.png){width="80%"}
:::

:::::: frame
### Machine learning is statistical learning with flexible models

::::: columns
::: column
0.5

-   There is no fundamental difference between statistics and machine
    learning

-   both optimize parameters to improve some criterion (loss /
    likelihood) that measures model fit to data

-   models used in machine learning are more flexible
:::

::: column
0.5

<figure>
<p><img src="figs/ml_vs_stats.png" style="width:90.0%" alt="image" />
<img src="figs/a_machine_learning.jpeg" style="width:90.0%"
alt="image" /> <img src="figs/curve_fitting_xkcd_ 2048.png"
style="width:90.0%" alt="image" /></p>
</figure>
:::
:::::
::::::

::: frame
### machine learning and statistics differences

-   whereas statistics is more concered with *inference* regarding
    parameters of a distribution (e.g. difference in survival time
    between treatments) in which the bespoke model is an integral part,
    machine learning focusses more on prediction accuracy, treating the
    model as a black box

-   can learn more complicated relationships (non-linearities,
    interactions)

-   also: can learn feature representations

-   nothing comes for free: need (much) more data
:::

::: frame
### models with much learning capacity can overfit

#### meaning they fit the noise in the training data and fail to genearlize to new data

<figure>
<p><img src="figs/overfitting_underfitting.png" style="width:80.0%"
alt="image" /> <img src="figs/bias_variance.png" style="width:80.0%"
alt="image" /></p>
</figure>
:::

::: frame
### Modern machine learning (neural networks a.k.a. deep learning)

#### computational models with multiple layers loosely inspired by mammalian brain

<figure>
<p><img src="figs/nn.png" style="width:90.0%" alt="image" /> <img
src="figs/cnn_overview.jpg" style="width:90.0%" alt="image" /></p>
</figure>
:::

::: frame
### Modern machine learning (neural networks a.k.a. deep learning)

-   For complex tasks, neural networks keep getting better with:

    -   more compute resources

    -   bigger data

    -   bigger models (enabled by data and compute)
:::

::: frame
### modern machine learning: deep learning and foundation models

-   build models to (extreme) scale in data, compute and number of
    parameters ![image](figs/biggerdata.png){width="70%"}

-   classical learning theory does not seem to apply (huge models that
    don't overfit)

-   e.g. convolutional neural networks
    ![image](figs/breastcancersweden.png){width="70%"}

-   large language models like chatGPT
:::

::: frame
### Large language models

#### auto-regressive next word predictors

-   large language models learn from large datasets with self-supervised
    objectives

-   fine-tune for tasks

-   not well-understood but very impressive
:::

::: frame
### Training large language models like chatGPT

#### masked-word prediction

![image](figs/nextwordpred.jpeg){width="60%"}

-   On a large corpus: predict the most likely \[mask\] in this sentence
    (answer = word)

-   Next text generation: start with 'prompt' (=context) and predict
    next likely word

-   one-word at a time, each time updating the *context* (=
    *auto-regressive*)

[^1]
:::

::: frame
### Does this create 'good answers'? Internet is full of harmful text

#### Need a filter

-   problem: no single good answer, some are unwanted (e.g. abusive)

-   let pre-trained model generate 2 responses to many prompts

-   let human judge which of the 2 is better

-   reinforcement-learning with human feedback (RLHF)

-   huge difference in user experience
:::

::: frame
### writing a good prompt

#### tasks that look like language modeling

-   provide context

    -   not: "describe vancomycin"

    -   but: "I'm in training as an hospital pharmacist, learning more
        about antiobiotic treatments in the intensive care, what is
        important for me to know about vancomycin?"

-   tasks that look like natural language completion will tend to work
    well

    -   not: \[statement\] $\to$ TRUE/FALSE

    -   but: is the following statement true? \[statement\], please
        provide your answer below
:::

:::::: frame
### rule-based vs LLMs

::::: columns
::: column
0.5 rule-based

-   deduction from explicit knowledge

-   knowledge verifiable and fast

-   constrained to deducible

<figure>
<p><img src="figs/library_bing.jpeg" style="width:60.0%" alt="image" />
<img src="figs/clippit.png" style="width:60.0%" alt="image" /></p>
</figure>
:::

::: column
0.5

-   extracted from observed data

-   unverifiable and compute intensive

-   "chatGPT seems to *know*(?) much"

<figure>
<p><img src="figs/brain_bing.jpeg" style="width:60.0%" alt="image" />
<img src="figs/chatgpt.jpg" style="width:60.0%" alt="image" /></p>
</figure>
:::
:::::
::::::

::: frame
### new research in LLMs

#### presented at NeurIPS, december 2023 (toolformer, META-AI)

<figure>
<p><img src="figs/toolformer_motivation.png" style="width:80.0%"
alt="image" /> <img src="figs/toolformer.png" style="width:80.0%"
alt="image" /></p>
</figure>
:::

::: frame
### Take-away part 1:

#### overview of AI and mL

-   AI subsumes rule-based programs and machine learning

-   ML is statistical learning with flexible models

-   modern ML uses large models that seem to violate classical learning
    theory

-   chatGPT is based on auto-regressive next-token prediction and
    reinforcement learning with human feedback

-   chatGPT produces beautiful mistakes: eloquently written logical
    fallacies, and thus needs expert 'supervision'

-   chatGPT does well on natural-language-like tasks (i.e. not \"is this
    sentence true or false\")
:::

::: frame
but what to *do*?
:::

::: frame
### two questions

Question 1

-   prediction model of $Y|X$ fits the data really well (AUC = 0.99 and
    perfect calibration)

-   will changing $X$ induce a change $Y$?
:::

::: frame
### Improving the world is a causal task

-   statistics / ML: what to expect when we passively observe the world

-   *not* how we can *intervene* to make things better

-   Question 1

    -   yellowish fingers predict lung cancer, paint fingers to skin
        color?

    -   weight loss predicts death in lung cancer, send patients to
        couch with McDonalds?

    ![image](figs/couch_patato.jpeg){width="40%"}
:::

::: frame
### How to learn about the effects of interventions?

#### Do the interventions in (psuedo) experiments

-   Why not look back at data and compare outcomes of patients treated
    with different medications?

-   Patients who take anti-hypertensive medications have more heart
    attacks

-   confounding by indication
:::

::: frame
### Causal inference

#### Studies causal questions, answers that tell us what to do

-   studies questions on effects of interventions

-   statistics / ML: what to expect when we passively observe the world

-   causal inference: what if I change something?

-   field of causal inference provides a language to express causal
    quantities

-   and tell us how to anser them

-   these answers tell us what to do
:::

::: frame
### Causal effects defined with potential outcomes

#### The individual treatment effect

<figure>
<p><img src="figs/ite1.png" style="width:80.0%" alt="image" /> <img
src="figs/ite2.png" style="width:80.0%" alt="image" /> <embed
src="figs/ite3.pdf" style="width:80.0%" /> <embed src="figs/ite_pos.pdf"
style="width:80.0%" /></p>
</figure>
:::

::: frame
### Causal effects defined with potential outcomes

#### The average treatment effect

<figure>
<p><img src="figs/ate1.png" alt="image" /> <img src="figs/ate2.png"
alt="image" /> <img src="figs/ate3.png" alt="image" /> <img
src="figs/ate.png" alt="image" /></p>
</figure>
:::

::: frame
### Treatment effect estimation requires exchangeability

#### Which is hard to ensure in non-experimental (observational data)

<figure>
<div class="center">

</div>
</figure>
:::

::: frame
### Randomized controlled trials gaurantee exchangeability

#### Thus allow for causal inference

<figure>
<div class="center">

</div>
</figure>

$$\begin{aligned}
            \onslide<2->{Y_t &= Y|T=t}
            %\onslide<3->{Y|do(T),Z &= Y|T,Z \\}
            %\onslide<4->{Y|do(T) &= \sum_Z Y|T,Z}
        
\end{aligned}$$
:::

::: frame
### Causal inference

#### summary

-   defines causal quantities / questions

-   need exchangeability to estimate treatment effects

-   holds by design in RCTs

-   may hold outside of RCTs (=observational data), conditional on
    covariates

-   may be unobserved confounding

-   RCT for everything?
:::

:::: frame
### Problems with RCTs for medical decision making

#### RCTs solve confounding but have their own limitations

::: columns
:::
::::

:::::: frame
### Ways forward

-   make right assumptions with observational data

    -   known confounders

    -   known functional forms for phenomena

-   use observational and RCT data

-   *active area of research*

    -   ML x CI: more flexible causal inference

    -   other way around: make ML more robust with CI insights

        -   sources of variation (*domain shifts*)

        -   unwanted biases

        -   make ML more efficient by generating pre-training data

::::: columns
::: column
0.7

<figure>
<p><img src="figs/elimbias_title.png" style="width:80.0%" alt="image" />
<img src="figs/protect_title.png" style="width:80.0%" alt="image" />
<img src="figs/offset_title.png" style="width:80.0%" alt="image" /></p>
</figure>
:::

::: column
0.3

<figure>
<p><img src="figs/qr_elimbias.png" style="width:80.0%" alt="image" />
<img src="figs/qr_protect.png" style="width:80.0%" alt="image" /> <img
src="figs/qr_offset.png" style="width:80.0%" alt="image" /></p>
</figure>
:::
:::::
::::::

::: frame
ML and causal inference\
Question 2: risk prediction model used for treatment decisions\
accurate outcome prediction models can yield harmful self-fulfilling
prophecies\
:::

::: frame
#### accurate outcome prediction models can yield harmful self-fulfilling prophecies

<figure>
<p><embed src="figs/new_overview1a.pdf" style="width:70.0%" /> <embed
src="figs/new_overview1b.pdf" style="width:70.0%" /> <embed
src="figs/new_overview1c.pdf" style="width:70.0%" /> <embed
src="figs/new_overview2a.pdf" style="width:70.0%" /> <embed
src="figs/new_overview2b.pdf" style="width:70.0%" /> <embed
src="figs/new_overview3a.pdf" style="width:70.0%" /> <embed
src="figs/new_overview3b.pdf" style="width:70.0%" /></p>
</figure>
:::

::: frame
### Prediction modeling very popular in medical research

![image](figs/predmodelsoverview.pdf){width="70%"}
:::

::: frame
building models for decision support without regards for the historic
treatment policy is a bad idea
:::

::: frame
![image](figs/policy_changea1.png){width="\\textwidth"}
![image](figs/policy_changea3.png){width="\\textwidth"}
![image](figs/policy_changeax.png){width="\\textwidth"}
![image](figs/policy_changeb2.png){width="\\textwidth"}
![image](figs/policy_changebx.png){width="\\textwidth"}
:::

::: frame
the question is not "is my model accurate before / after deploytment",
but did deploying the model improve patient outcomes?
:::

::: frame
Treatment-naive risk models
![image](figs/txnaive1.png){width="\\textwidth"}
![image](figs/txnaive2.png){width="\\textwidth"}
:::

::: frame
Other risk models: condition on given treatment and traits
![image](figs/postdecision1.png){width=".9\\textwidth"}
:::

::: frame
Other risk models: condition on given treatment and traits
![image](figs/postdecision2.png){width=".9\\textwidth"} unobserved
confounding (hat type) leads to wrong treatment decisions
:::

::: frame
Recommended validation practices do not protect against harmbecause they
do not evaluate the policy change
![image](figs/tripod_title.png){width=".5\\textwidth"}
![image](figs/ajcc_title.png){width=".5\\textwidth"}
:::

::: frame
Bigger data does not protect against harmful risk models
![image](figs/biggerdata.png){width="80%"}
:::

::: frame
More flexible models do not protect against harmful risk models
![image](figs/morelayers.png){width="50%"}
:::

::: frame
### Gap between prediction accuracy and value for decision making

![image](figs/mindthegap.pdf){width="80%"}
:::

::: frame
[what to do?]{.alert}

1.  Evaluate policy change (cluster randomized controlled trial)

2.  Build models that are likely to have value for decision making
:::

::: frame
Prediction-under-intervention modelsPredict outcome *under hypothetical
intervention* of giving certain treatment
![image](figs/predictionunderintervention.png){width="\\textwidth"}
:::

::: frame
When developing risk models, always discuss:
![image](figs/policy_changeb1.png){height=".6\\textheight"}
![image](figs/policy_changeb2.png){height=".6\\textheight"}
![image](figs/policy_changebx.png){height=".6\\textheight"}

1.  what is effect on treatment policy?

2.  what is effect on patient outcomes?
:::

:::::: frame
Takeaway Don't assume predicting well leads to good decisions, think
about the policy change\
 \

::::: columns
::: column
0.5 ![image](figs/comment_qr.png){width=".5\\textwidth"}\
Decision making in cancer: causal questions require causal answers
:::

::: column
0.5 ![image](figs/qr_selffulfilling.png){width=".5\\textwidth"}\
When accurate prediction models yield harmful sel-fulfilling prophecies
:::
:::::
::::::

::: frame
### take-aways

-   AI subsumes rule-based programs and machine learning

-   machine learning is statistical learning from data with flexible
    mdoels

-   modern machine learning does well with scale

-   chatGPT is based on auto-regressive next-token prediction and
    reinforcement learning with human feedback

-   chatGPT produces beautiful mistakes: eloquently written logical
    fallacies

-   chatGPT does well on natural-language-like tasks

-   prediction: what to expect when passively observing the world

-   causal inference: what happens when I change something?

-   prediction models can cause harmful self-fulfilling prophecies when
    used for decision making

-   when building prediction models for decision support, you cannot
    ignore decisions on the treatments in historic data

-   models for prediction-under-intervention have foreseeable effects
    when used for decision making

-   ultimate test of model utility is determined by outcomes in
    (cluster) RCT
:::

::: frame
thank you
:::

[^1]: image by akshay pachaar

