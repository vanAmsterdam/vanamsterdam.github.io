
@misc{georgeVisualizingSizeLarge2024,
	title = {Visualizing size of {Large} {Language} {Models} ðŸ’»},
	url = {https://medium.com/@georgeanil/visualizing-size-of-large-language-models-ec576caa5557},
	abstract = {The Important Factors that determine the size of Language model are: Model SizeÂ , Training Size and Compute Size.},
	language = {en},
	urldate = {2024-04-17},
	journal = {Medium},
	author = {George, Anil},
	month = jan,
	year = {2024},
	file = {gpt4_size.jpeg:/Users/wamster3/Zotero/storage/K4X2LU7B/gpt4_size.jpeg:image/jpeg},
}

@article{dhimanMethodologicalConductPrognostic2022,
	title = {Methodological conduct of prognostic prediction models developed using machine learning in oncology: a systematic review},
	volume = {22},
	issn = {1471-2288},
	shorttitle = {Methodological conduct of prognostic prediction models developed using machine learning in oncology},
	doi = {10.1186/s12874-022-01577-x},
	abstract = {BACKGROUND: Describe and evaluate the methodological conduct of prognostic prediction models developed using machine learning methods in oncology.
METHODS: We conducted a systematic review in MEDLINE and Embase between 01/01/2019 and 05/09/2019, for studies developing a prognostic prediction model using machine learning methods in oncology. We used the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) statement, Prediction model Risk Of Bias ASsessment Tool (PROBAST) and CHecklist for critical Appraisal and data extraction for systematic Reviews of prediction Modelling Studies (CHARMS) to assess the methodological conduct of included publications. Results were summarised by modelling type: regression-, non-regression-based and ensemble machine learning models.
RESULTS: Sixty-two publications met inclusion criteria developing 152 models across all publications. Forty-two models were regression-based, 71 were non-regression-based and 39 were ensemble models. A median of 647 individuals (IQR: 203 to 4059) and 195 events (IQR: 38 to 1269) were used for model development, and 553 individuals (IQR: 69 to 3069) and 50 events (IQR: 17.5 to 326.5) for model validation. A higher number of events per predictor was used for developing regression-based models (median: 8, IQR: 7.1 to 23.5), compared to alternative machine learning (median: 3.4, IQR: 1.1 to 19.1) and ensemble models (median: 1.7, IQR: 1.1 to 6). Sample size was rarely justified (nâ€‰=â€‰5/62; 8\%). Some or all continuous predictors were categorised before modelling in 24 studies (39\%). 46\% (nâ€‰=â€‰24/62) of models reporting predictor selection before modelling used univariable analyses, and common method across all modelling types. Ten out of 24 models for time-to-event outcomes accounted for censoring (42\%). A split sample approach was the most popular method for internal validation (nâ€‰=â€‰25/62, 40\%). Calibration was reported in 11 studies. Less than half of models were reported or made available.
CONCLUSIONS: The methodological conduct of machine learning based clinical prediction models is poor. Guidance is urgently needed, with increased awareness and education of minimum prediction modelling standards. Particular focus is needed on sample size estimation, development and validation analysis methods, and ensuring the model is available for independent validation, to improve quality of machine learning based clinical prediction models.},
	language = {eng},
	number = {1},
	journal = {BMC medical research methodology},
	author = {Dhiman, Paula and Ma, Jie and Andaur Navarro, Constanza L. and Speich, Benjamin and Bullock, Garrett and Damen, Johanna A. A. and Hooft, Lotty and Kirtley, Shona and Riley, Richard D. and Van Calster, Ben and Moons, Karel G. M. and Collins, Gary S.},
	month = apr,
	year = {2022},
	pmid = {35395724},
	pmcid = {PMC8991704},
	keywords = {Bias, Humans, Machine learning, Machine Learning, Medical Oncology, Methodology, Prediction, Prognosis, Research Design},
	pages = {101},
	file = {Dhiman et al_2022_Methodological conduct of prognostic prediction models developed using machine.pdf:/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Dhiman et al_2022_Methodological conduct of prognostic prediction models developed using machine.pdf:application/pdf;ml_review_performance.png:/Users/wamster3/Zotero/storage/T94N2WGL/ml_review_performance.png:image/png},
}


@article{christodoulouSystematicReviewShows2019,
	title = {A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models},
	volume = {110},
	issn = {1878-5921},
	doi = {10.1016/j.jclinepi.2019.02.004},
	abstract = {OBJECTIVES: The objective of this study was to compare performance of logistic regression (LR) with machine learning (ML) for clinical prediction modeling in the literature.
STUDY DESIGN AND SETTING: We conducted a Medline literature search (1/2016 to 8/2017) and extracted comparisons between LR and ML models for binary outcomes.
RESULTS: We included 71 of 927 studies. The median sample size was 1,250 (range 72-3,994,872), with 19 predictors considered (range 5-563) and eight events per predictor (range 0.3-6,697). The most common ML methods were classification trees, random forests, artificial neural networks, and support vector machines. In 48 (68\%) studies, we observed potential bias in the validation procedures. Sixty-four (90\%) studies used the area under the receiver operating characteristic curve (AUC) to assess discrimination. Calibration was not addressed in 56 (79\%) studies. We identified 282 comparisons between an LR and ML model (AUC range, 0.52-0.99). For 145 comparisons at low risk of bias, the difference in logit(AUC) between LR and ML was 0.00 (95\% confidence interval, -0.18 to 0.18). For 137 comparisons at high risk of bias, logit(AUC) was 0.34 (0.20-0.47) higher for ML.
CONCLUSION: We found no evidence of superior performance of ML over LR. Improvements in methodology and reporting are needed for studies that compare modeling algorithms.},
	language = {eng},
	journal = {Journal of Clinical Epidemiology},
	author = {Christodoulou, Evangelia and Ma, Jie and Collins, Gary S. and Steyerberg, Ewout W. and Verbakel, Jan Y. and Van Calster, Ben},
	month = jun,
	year = {2019},
	pmid = {30763612},
	keywords = {Algorithms, Area Under Curve, AUC, Calibration, Clinical prediction models, Humans, Logistic Models, Logistic regression, Machine learning, Models, Theoretical, Outcome Assessment, Health Care, Predictive Value of Tests, Reporting, Sensitivity and Specificity, Supervised Machine Learning},
	pages = {12--22},
	file = {Christodoulou et al_2019_A systematic review shows no performance benefit of machine learning over.pdf:/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Christodoulou et al_2019_A systematic review shows no performance benefit of machine learning over.pdf:application/pdf;ml_review_performance.png:/Users/wamster3/Zotero/storage/T94N2WGL/ml_review_performance.png:image/png},
}
@article{cooperEvaluationMachinelearningMethods1997,
	title = {An evaluation of machine-learning methods for predicting pneumonia mortality},
	volume = {9},
	issn = {0933-3657},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365796003673},
	doi = {10.1016/S0933-3657(96)00367-3},
	abstract = {This paper describes the application of eight statistical and machine-learning methods to derive computer models for predicting mortality of hospital patients with pneumonia from their findings at initial presentation. The eight models were each constructed based on 9847 patient cases and they were each evaluated on 4352 additional cases. The primary evaluation metric was the error in predicted survival as a function of the fraction of patients predicted to survive. This metric is useful in assessing a model's potential to assist a clinician in deciding whether to treat a given patient in the hospital or at home. We examined the error rates of the models when predicting that a given fraction of patients will survive. We examined survival fractions between 0.1 and 0.6. Over this range, each model's predictive error rate was within 1\% of the error rate of every other model. When predicting that approximately 30\% of the patients will survive, all the models have an error rate of less than 1.5\%. The models are distinguished more by the number of variables and parameters that they contain than by their error rates; these differences suggest which models may be the most amenable to future implementation as paper-based guidelines.},
	number = {2},
	urldate = {2024-04-05},
	journal = {Artificial Intelligence in Medicine},
	author = {Cooper, Gregory F. and Aliferis, Constantin F. and Ambrosino, Richard and Aronis, John and Buchanan, Bruce G. and Caruana, Richard and Fine, Michael J. and Glymour, Clark and Gordon, Geoffrey and Hanusa, Barbara H. and Janosky, Janine E. and Meek, Christopher and Mitchell, Tom and Richardson, Thomas and Spirtes, Peter},
	month = feb,
	year = {1997},
	keywords = {Machine learning, Clinical databases, Computer-based prediction, Pneumonia},
	pages = {107--138},
	annote = {pneumonia mortality prediction, asthma
},
	file = {Cooper et al_1997_An evaluation of machine-learning methods for predicting pneumonia mortality.pdf:/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Cooper et al_1997_An evaluation of machine-learning methods for predicting pneumonia mortality.pdf:application/pdf;ScienceDirect Snapshot:/Users/wamster3/Zotero/storage/QJRMCGFX/S0933365796003673.html:text/html},
}

