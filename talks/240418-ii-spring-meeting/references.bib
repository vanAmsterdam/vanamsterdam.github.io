
@misc{vanamsterdamWhenAccuratePrediction2024a,
	title = {When accurate prediction models yield harmful self-fulfilling prophecies},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2312.01210},
	doi = {10.48550/arXiv.2312.01210},
	abstract = {Objective: Prediction models are popular in medical research and practice. By predicting an outcome of interest for specific patients, these models may help inform difficult treatment decisions, and are often hailed as the poster children for personalized, data-driven healthcare. Many prediction models are deployed for decision support based on their prediction accuracy in validation studies. We investigate whether this is a safe and valid approach. Materials and Methods: We show that using prediction models for decision making can lead to harmful decisions, even when the predictions exhibit good discrimination after deployment. These models are harmful self-fulfilling prophecies: their deployment harms a group of patients but the worse outcome of these patients does not invalidate the predictive power of the model. Results: Our main result is a formal characterization of a set of such prediction models. Next we show that models that are well calibrated before and after deployment are useless for decision making as they made no change in the data distribution. Discussion: Our results point to the need to revise standard practices for validation, deployment and evaluation of prediction models that are used in medical decisions. Conclusion: Outcome prediction models can yield harmful self-fulfilling prophecies when used for decision making, a new perspective on prediction model development, deployment and monitoring is needed.},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {van Amsterdam, Wouter A. C. and van Geloven, Nan and Krijthe, Jesse H. and Ranganath, Rajesh and CinÃ¡, Giovanni},
	month = feb,
	year = {2024},
	note = {arXiv:2312.01210 [cs, stat]},
	keywords = {Statistics - Methodology, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/wamster3/Zotero/storage/TK4LYI8J/van Amsterdam et al. - 2024 - When accurate prediction models yield harmful self.pdf:application/pdf;arXiv.org Snapshot:/Users/wamster3/Zotero/storage/XJDVFTAH/2312.html:text/html},
}


@misc{vanamsterdamAlgorithmsActionImproving2024,
	title = {From algorithms to action: improving patient care requires causality},
	shorttitle = {From algorithms to action},
	url = {http://arxiv.org/abs/2209.07397},
	doi = {10.48550/arXiv.2209.07397},
	abstract = {In cancer research there is much interest in building and validating outcome predicting outcomes to support treatment decisions. However, because most outcome prediction models are developed and validated without regard to the causal aspects of treatment decision making, many published outcome prediction models may cause harm when used for decision making, despite being found accurate in validation studies. Guidelines on prediction model validation and the checklist for risk model endorsement by the American Joint Committee on Cancer do not protect against prediction models that are accurate during development and validation but harmful when used for decision making. We explain why this is the case and how to build and validate models that are useful for decision making.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {van Amsterdam, Wouter A. C. and de Jong, Pim A. and Verhoeff, Joost J. C. and Leiner, Tim and Ranganath, Rajesh},
	month = apr,
	year = {2024},
	note = {arXiv:2209.07397 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computers and Society},
	file = {arXiv.org Snapshot:/Users/wamster3/Zotero/storage/2DKBP4AF/2209.html:text/html;van Amsterdam et al_2024_From algorithms to action.pdf:/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/van Amsterdam et al_2024_From algorithms to action.pdf:application/pdf},
}

@misc{georgeVisualizingSizeLarge2024,
	title = {Visualizing size of {Large} {Language} {Models} ðŸ’»},
	url = {https://medium.com/@georgeanil/visualizing-size-of-large-language-models-ec576caa5557},
	abstract = {The Important Factors that determine the size of Language model are: Model SizeÂ , Training Size and Compute Size.},
	language = {en},
	urldate = {2024-04-17},
	journal = {Medium},
	author = {George, Anil},
	month = jan,
	year = {2024},
	file = {gpt4_size.jpeg:/Users/wamster3/Zotero/storage/K4X2LU7B/gpt4_size.jpeg:image/jpeg},
}

@article{dhimanMethodologicalConductPrognostic2022,
	title = {Methodological conduct of prognostic prediction models developed using machine learning in oncology: a systematic review},
	volume = {22},
	issn = {1471-2288},
	shorttitle = {Methodological conduct of prognostic prediction models developed using machine learning in oncology},
	doi = {10.1186/s12874-022-01577-x},
	abstract = {BACKGROUND: Describe and evaluate the methodological conduct of prognostic prediction models developed using machine learning methods in oncology.
METHODS: We conducted a systematic review in MEDLINE and Embase between 01/01/2019 and 05/09/2019, for studies developing a prognostic prediction model using machine learning methods in oncology. We used the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) statement, Prediction model Risk Of Bias ASsessment Tool (PROBAST) and CHecklist for critical Appraisal and data extraction for systematic Reviews of prediction Modelling Studies (CHARMS) to assess the methodological conduct of included publications. Results were summarised by modelling type: regression-, non-regression-based and ensemble machine learning models.
RESULTS: Sixty-two publications met inclusion criteria developing 152 models across all publications. Forty-two models were regression-based, 71 were non-regression-based and 39 were ensemble models. A median of 647 individuals (IQR: 203 to 4059) and 195 events (IQR: 38 to 1269) were used for model development, and 553 individuals (IQR: 69 to 3069) and 50 events (IQR: 17.5 to 326.5) for model validation. A higher number of events per predictor was used for developing regression-based models (median: 8, IQR: 7.1 to 23.5), compared to alternative machine learning (median: 3.4, IQR: 1.1 to 19.1) and ensemble models (median: 1.7, IQR: 1.1 to 6). Sample size was rarely justified (nâ€‰=â€‰5/62; 8\%). Some or all continuous predictors were categorised before modelling in 24 studies (39\%). 46\% (nâ€‰=â€‰24/62) of models reporting predictor selection before modelling used univariable analyses, and common method across all modelling types. Ten out of 24 models for time-to-event outcomes accounted for censoring (42\%). A split sample approach was the most popular method for internal validation (nâ€‰=â€‰25/62, 40\%). Calibration was reported in 11 studies. Less than half of models were reported or made available.
CONCLUSIONS: The methodological conduct of machine learning based clinical prediction models is poor. Guidance is urgently needed, with increased awareness and education of minimum prediction modelling standards. Particular focus is needed on sample size estimation, development and validation analysis methods, and ensuring the model is available for independent validation, to improve quality of machine learning based clinical prediction models.},
	language = {eng},
	number = {1},
	journal = {BMC medical research methodology},
	author = {Dhiman, Paula and Ma, Jie and Andaur Navarro, Constanza L. and Speich, Benjamin and Bullock, Garrett and Damen, Johanna A. A. and Hooft, Lotty and Kirtley, Shona and Riley, Richard D. and Van Calster, Ben and Moons, Karel G. M. and Collins, Gary S.},
	month = apr,
	year = {2022},
	pmid = {35395724},
	pmcid = {PMC8991704},
	keywords = {Bias, Humans, Machine learning, Machine Learning, Medical Oncology, Methodology, Prediction, Prognosis, Research Design},
	pages = {101},
	file = {Dhiman et al_2022_Methodological conduct of prognostic prediction models developed using machine.pdf:/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Dhiman et al_2022_Methodological conduct of prognostic prediction models developed using machine.pdf:application/pdf;ml_review_performance.png:/Users/wamster3/Zotero/storage/T94N2WGL/ml_review_performance.png:image/png},
}


@article{christodoulouSystematicReviewShows2019,
	title = {A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models},
	volume = {110},
	issn = {1878-5921},
	doi = {10.1016/j.jclinepi.2019.02.004},
	abstract = {OBJECTIVES: The objective of this study was to compare performance of logistic regression (LR) with machine learning (ML) for clinical prediction modeling in the literature.
STUDY DESIGN AND SETTING: We conducted a Medline literature search (1/2016 to 8/2017) and extracted comparisons between LR and ML models for binary outcomes.
RESULTS: We included 71 of 927 studies. The median sample size was 1,250 (range 72-3,994,872), with 19 predictors considered (range 5-563) and eight events per predictor (range 0.3-6,697). The most common ML methods were classification trees, random forests, artificial neural networks, and support vector machines. In 48 (68\%) studies, we observed potential bias in the validation procedures. Sixty-four (90\%) studies used the area under the receiver operating characteristic curve (AUC) to assess discrimination. Calibration was not addressed in 56 (79\%) studies. We identified 282 comparisons between an LR and ML model (AUC range, 0.52-0.99). For 145 comparisons at low risk of bias, the difference in logit(AUC) between LR and ML was 0.00 (95\% confidence interval, -0.18 to 0.18). For 137 comparisons at high risk of bias, logit(AUC) was 0.34 (0.20-0.47) higher for ML.
CONCLUSION: We found no evidence of superior performance of ML over LR. Improvements in methodology and reporting are needed for studies that compare modeling algorithms.},
	language = {eng},
	journal = {Journal of Clinical Epidemiology},
	author = {Christodoulou, Evangelia and Ma, Jie and Collins, Gary S. and Steyerberg, Ewout W. and Verbakel, Jan Y. and Van Calster, Ben},
	month = jun,
	year = {2019},
	pmid = {30763612},
	keywords = {Algorithms, Area Under Curve, AUC, Calibration, Clinical prediction models, Humans, Logistic Models, Logistic regression, Machine learning, Models, Theoretical, Outcome Assessment, Health Care, Predictive Value of Tests, Reporting, Sensitivity and Specificity, Supervised Machine Learning},
	pages = {12--22},
	file = {Christodoulou et al_2019_A systematic review shows no performance benefit of machine learning over.pdf:/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Christodoulou et al_2019_A systematic review shows no performance benefit of machine learning over.pdf:application/pdf;ml_review_performance.png:/Users/wamster3/Zotero/storage/T94N2WGL/ml_review_performance.png:image/png},
}
@article{cooperEvaluationMachinelearningMethods1997,
	title = {An evaluation of machine-learning methods for predicting pneumonia mortality},
	volume = {9},
	issn = {0933-3657},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365796003673},
	doi = {10.1016/S0933-3657(96)00367-3},
	abstract = {This paper describes the application of eight statistical and machine-learning methods to derive computer models for predicting mortality of hospital patients with pneumonia from their findings at initial presentation. The eight models were each constructed based on 9847 patient cases and they were each evaluated on 4352 additional cases. The primary evaluation metric was the error in predicted survival as a function of the fraction of patients predicted to survive. This metric is useful in assessing a model's potential to assist a clinician in deciding whether to treat a given patient in the hospital or at home. We examined the error rates of the models when predicting that a given fraction of patients will survive. We examined survival fractions between 0.1 and 0.6. Over this range, each model's predictive error rate was within 1\% of the error rate of every other model. When predicting that approximately 30\% of the patients will survive, all the models have an error rate of less than 1.5\%. The models are distinguished more by the number of variables and parameters that they contain than by their error rates; these differences suggest which models may be the most amenable to future implementation as paper-based guidelines.},
	number = {2},
	urldate = {2024-04-05},
	journal = {Artificial Intelligence in Medicine},
	author = {Cooper, Gregory F. and Aliferis, Constantin F. and Ambrosino, Richard and Aronis, John and Buchanan, Bruce G. and Caruana, Richard and Fine, Michael J. and Glymour, Clark and Gordon, Geoffrey and Hanusa, Barbara H. and Janosky, Janine E. and Meek, Christopher and Mitchell, Tom and Richardson, Thomas and Spirtes, Peter},
	month = feb,
	year = {1997},
	keywords = {Machine learning, Clinical databases, Computer-based prediction, Pneumonia},
	pages = {107--138},
	annote = {pneumonia mortality prediction, asthma
},
	file = {Cooper et al_1997_An evaluation of machine-learning methods for predicting pneumonia mortality.pdf:/Users/wamster3/Library/CloudStorage/GoogleDrive-w.a.c.vanamsterdam@gmail.com/My Drive/boox/zoteropdfs/Cooper et al_1997_An evaluation of machine-learning methods for predicting pneumonia mortality.pdf:application/pdf;ScienceDirect Snapshot:/Users/wamster3/Zotero/storage/QJRMCGFX/S0933365796003673.html:text/html},
}

