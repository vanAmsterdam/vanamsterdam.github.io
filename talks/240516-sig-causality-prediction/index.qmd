---
title: "Causality and prediction: developing and validating models for decision making"
subtitle: "Causal Data Science Special Interest Group - Utrecht"
date: 2024-05-16
format:
    revealjs:
        toc: true
        incremental: true
        #theme: custom.scss
        auto-stretch: true
        center: true
        fig-align: center
        width: 1600
        height: 900
---

# Prediction versus causal inference

## Prediction

1. have some features $X$ (patient characteristics, medical images, lab results)
2. define relevant outcome $Y$ (e.g. 1-year survival, blood pressure, treatment complication)
3. build prediction model $f: \mathbb{X} \to \mathbb{Y}$ that *predicts* $Y$ from $X$, e.g.:

. . . 

$$ \theta^* = \arg \min_{\theta} \sum_i^n ( f_{\theta}(x_i) - y_i )^2 $$

. . . 

Hoping that

$$ \lim_{n \to \infty} f_{\theta^*} = E[Y|X] $$

## Prediction: typical approach

1. define population, start a (prospective) longitudinal cohort
2. measure $X$ at *prediction baseline*
3. follow-up patients to measure $Y$
4. fit model $f$ to $\{x_i,y_i\}$
5. evaluate prediction performance with e.g. discrimination, calibration, $R^2$

## Causal inference

$y^0:=$ survival time **if I don't treat the patient**

. . . 

$y^1:=$ survival time **if I do treat**

. . .

```{r}
#| fig-height: 2.5
#| fig-width: 13
#| fig-align: center
library(ggplot2); theme_set(theme_void())
library(dplyr)
set.seed(123456)
n=1e3
mu = 0 
b = 1 
s = 1
t = rbinom(n, 1, .5)
s = rnorm(n, 0, s)
y = b*t  + s
mu0 = mean(y[t==0])
mu1 = mean(y[t==1])
delta = mu1 - mu0

df = data.frame(t=factor(t),y)
xmin = -3
xmax = 4
ymin = 0
ymax = 0.5

df %>%
    filter(t==0) %>%
    ggplot(aes(x=y, fill=t)) + geom_density(alpha=.5) + 
    geom_vline(aes(xintercept=mu0), linetype=2) + 
    labs(x="survival time", y="") + 
    scale_x_continuous(breaks = mu0, labels="mu0", limits=c(xmin, xmax)) + 
    scale_y_continuous(limits=c(ymin, ymax)) + 
    theme(
          axis.ticks.x = element_line(), axis.text.x = element_text(),
          legend.position="none"
          )

```

\begin{align}
y^0 &= \mu_0 + \epsilon, \quad \epsilon \overset{\mathrm{iid}}{\sim} N(0,\sigma)\\
\end{align}

. . . 

this formula for together with distribution over error term gives rise to a distribution over the outcome when intervening on treatment (i.e. an *interventional distribution*)

$$
 P(Y=y|\text{do}(T=0))
$$

## Causal inference

$y^0:=$ survival time **if I don't treat the patient**

$y^1:=$ survival time **if I do treat**

```{r}
#| fig-height: 2.5
#| fig-width: 13
#| fig-align: center
df %>%
    ggplot(aes(x=y, fill=t)) + geom_density(alpha=.5) + 
    geom_vline(aes(xintercept=mu0), linetype=2) + 
    geom_vline(aes(xintercept=mu1), linetype=2) + 
    labs(x="survival time", y="") + 
    scale_x_continuous(breaks = c(mu0,mu1), labels=c("mu0","mu1"), limits=c(xmin, xmax)) + 
    scale_y_continuous(limits=c(ymin, ymax)) + 
    theme(
          axis.ticks.x = element_line(), axis.text.x = element_text(),
          legend.position="none"
          )
```

\begin{align}
y^0 &= \mu_0 + \epsilon, \quad \epsilon \overset{\mathrm{iid}}{\sim} N(0,\sigma) \to &P(Y=y|\text{do}(T=0))\\
y^1 &= \mu_1 + \epsilon, \quad \epsilon \overset{\mathrm{iid}}{\sim} N(0,\sigma) \to &P(Y=y|\text{do}(T=1))
\end{align}

. . . 

\begin{align}
\text{treatment effect} &:= E[y^1] - E[y^0] = \mu_1 - \mu_0 \\
                        &:= E[P(Y|\text{do}(T=1))] - E[P(Y|\text{do}(T=0))]
\end{align}

## Causal inference: typical approach

1. define target population and targeted treatment comparison
2. run randomized controlled trial, randomizing treatment allocation
3. measure patient outcomes
4. estimate parameter that summarizes *average treatment effect* (ATE)

. . . 

::: {.callout-tip icon=false}
## What if you cannot do a (big enough) RCT?

Emulate / approximate the ideal trial in observational data you do have, using **causal inference** techniques

**(which rely on untestable assumptions)**

:::

## Causal inference versus prediction

:::{.columns}

::::{.column width="50%"}

prediction

- typical estimand $E[Y|X]$
- typical study: longitudinal cohort
- typical interpretation: $X$ predicts $Y$
- primary use: know what $Y$ to expect when observing $X$ *assuming no change in joint distribution*

::::

::::{.column width="50%"}

causal inference

- typical estimand $E[y^1] - E[y^0]$
- typical study: RCT
- typical interpretation: *causal effect* of $T$ on $Y$
- primary use: know what treatment to give

::::

:::

## The in-between: using prediction models for (medical) decision making

![](figs/8q4cne_meme_crossover.jpg){fig-align="center"}

## Using prediction models for decision making is commonly thought of as a good idea

For example:

1. give chemotherapy to cancer patients with high predicted risk of recurrence
2. give statins to patients with a high risk of a heart attack

. . . 

::: {.callout-note icon=false}
## TRIPOD+AI on prediction models [@collinsTRIPODAIStatement2024]

“Their primary use is to support clinical decision making, such as ... **initiate treatment or lifestyle changes.**” 

:::

---

:::{.callout-warning}

## This may lead to bad situations when:

1. ignoring the treatments patients may have had during training / validation
2. only considering measures of predictive accuracy as sufficient evidence for safe deployment
3. predictive accuracy (AUC) may be measured pre- or post-deployment of the model

:::

# When accurate prediction models yield harmful self-fulfilling prophecies{#selffulfilling}

---

:::{.r-stack}

![](figs/new_overview1a.png){.fragment height=18cm}

![](figs/new_overview1b.png){.fragment height=18cm}

![](figs/new_overview1c.png){.fragment height=18cm}

![](figs/new_overview2a.png){.fragment height=18cm}

![](figs/new_overview2b.png){.fragment height=18cm}

![](figs/new_overview3a.png){.fragment height=18cm}

![](figs/new_overview3b.png){.fragment height=18cm}

:::


## Prediction modeling is very popular in medical research

![](figs/predmodelsoverview.png){fig-align='center'}

---

:::{.callout-tip}
building models for decision support without regards for the historic treatment policy is a bad idea
:::

:::{.r-stack}

![](figs/policy_changea1.png){.fragment width="100%"}

![](figs/policy_changea3.png){.fragment width="100%"}

![](figs/policy_changeax.png){.fragment width="100%"}

![](figs/policy_changeb2.png){.fragment width="100%"}

![](figs/policy_changebx.png){.fragment width="100%"}

:::

--- 

:::{.callout-note}
The question is not "is my model accurate before / after deploytment", but did deploying the model improve patient outcomes?
:::


## Treatment-naive risk models

:::{.r-stack}

![](figs/txnaive1.png){.fragment}

![](figs/txnaive2b.png){.fragment}

:::


## Is this obvious?

::: {.callout-tip}

It may seem obvious that you should not ignore historical treatments in your prediction models, if you want to improve treatment decisions, but many of these models are published daily, and some guidelines even allow for implementing these models based on predictve performance only

:::

## Other risk models:

[- condition on given treatment and traits]{.fragment fragment-index=1}

[- unobserved confounding (hat type) leads to wrong treatment decisions]{.fragment fragment-index=2}

:::{.r-stack}

![](figs/postdecision1.png){.fragment fragment-index=1}

![](figs/postdecision2.png){.fragment fragment-index=2} 

:::

##  Recommended validation practices do not protect against harm

because they do not evaluate the policy change

![](figs/ajcc_title.png){height="5cm"}
![](figs/tripod_title.png){height="5cm"}
![](figs/tripod_ai.png){height="5cm"}

## Bigger data does not protect against harmful risk models

![](figs/biggerdata.png){fig-align="center"}

## More flexible models do not protect against harmful risk models

![](figs/morelayers.png){fig-align="center"}

## Gap between prediction accuracy and value for decision making

![](figs/mindthegap.png){fig-align="center"}

## {auto-animate=true}

::: {style="margin-top: 200px; font-size: 3em; color: red;"}
What to do?
:::

## {auto-animate=true}

::: {style="margin-top: 100px"}
What to do?
:::

1.  Evaluate policy change (cluster randomized controlled trial)
2.  Build models that are likely to have value for decision making

# Building and validating models for decision support

## Deploying a model is an intervention that changes the way treatment decisions are made

![](figs/policy_changebx.png){fig-align="center"}

## How do we learn about the effect of an intervention?

With causal inference!

- for using a decision support model, the unit of intervention is usually *the doctor*
- randomly assign *doctors* to have access to the model or not
- measure differences in **treatment decisions** and **patient outcomes**
- if using model improves outcomes, use that one

. . . 

:::{.callout-tip icon="false"}

## This is not a new idea [@cooperEvaluationMachinelearningMethods1997]

“As one possibility, suppose that a trial is performed in which clinicians are randomized either to have or not to have access to such a decision aid in making decisions about where to treat patients who present with pneumonia.” 

:::

. . .

:::{.callout-warning}
## What we don't learn
was the model predicting anything sensible?
:::

## So build prediction models and trial them?

Not a good idea

- baking a cake without a recipe
- hoping it turns into something nice
- not pleasant to people that need to taste the experiment
  - (i.e. patients may have side-effects / die)

## Models that are likely to be valuable for decision making

- prediction under hypothetical interventions (prediction-under-intervention) models predict expected outcomes under the *hypothetical intervention* of giving a certain treatment
- this is not a new idea

. . . 

::: {.callout-tip icon=false}
## Hilden and Habbema on prognosis [@hildenPrognosisMedicineAnalysis1987]
"Prognosis cannot be divorced from contemplated medical action, nor from action to be taken by the patient in response to prognostication.” 
:::

- whereas *treatment-naive* prediction models average out over the historic treatment policy, prediction-under-intervention allows the user to select a treatment option

## Estimand for prediction-under-intervention models

What is the estimand?

- prediction: $E[Y|X]$
- treatment effect: $E[Y|\text{do}(T=1)] - E[Y|\text{do}(T=0)]$
- prediction-under-intervention: $E[Y|\text{do}(T=t),X]$

--- 

:::{.columns}

::::{.column width="50%"}

using *treatment naive* prediction models for decision support

![](figs/8q4cne_meme_crossover.jpg)

::::

::::{.column width="50%"}

prediction-under-intervention

![](figs/peanutbutter_chocolatesprinkles.jpg){.fragment}

::::
:::

## Estimating prediction-under-intervention models

- the estimand $E[Y|\text{do}(T=t),X]$ is an interventional distribution
- RCTs randomly sample from interventional distributions
- prediction-under-intervention models may be estimated and evaluated in RCT data
- however, RCTs are typically designed to estimate a single parameter
- prediction models need more data
- in comes causal inference from observational data?

## Challenges with observational data

- assumption of no unobserved confounding may be hard to justify
- but there's more between heaven (RCT) and earth (confounder adjustment)
  - instrumental variable analysis (high variance estimates)
  - proxy-variable methods
  - constant relative treatment effect assumption
  - diff-in-diff
  - front-door analysis
- these come with their own assumptions and trade-offs
- do sensitivity analysis
- may not have treatment information
- may be many decision time-points, hard to formulate estimand over long time-horizon

## How to proceed?

- build prediction-under-intervention model with best data + assumptions
- test policy value in historical RCT data
  - for each patient in RCT, determine recommended treatment according to model
  - if actual (randomly allocated) treatment is concordant, keep the patient
  - if not, drop observation
  - calculate expected outcomes in the subpopulation
- then do a cluster RCT

## Take-aways

- Prediction and causal inference come together neatly by declaring $P(Y|\text{do},X)$ as the estimand
- (mis)using prediction models for treatment decisions without causal thinking and evaluation is a bad idea
- building and evaluating prediction-under-intervention models requires unconfoundedness
- prediction-under-intervention models can be evaluated with standard prediction accuracy metrics on RCT data

. . .

:::: {.columns}
::: {.column width="50%"}
![](figs/qr_comment.png){height=10cm fig-align='center'}

From algorithms to action: improving patient care requires causality [@amsterdamAlgorithmsActionImproving2024]
:::

::: {.column width="50%"}
![](figs/qr_selffulfilling.png){height=10cm fig-align='center'}

When accurate prediction models yield harmful sel-fulfilling prophecies [@vanamsterdamWhenAccuratePrediction2024a]
:::
::::


## References



