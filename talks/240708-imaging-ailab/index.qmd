---
title: "Medical imaging and AI for decision support"
subtitle: "Medical Imaging AI lab meeting"
date: 2024-07-08
categories:
  - invited
format:
    revealjs:
        toc: true
        incremental: false
        theme: umcu.scss
        auto-stretch: true
        center: true
        fig-align: center
        width: 1600
        height: 900
        logo: umcu_blue.png
---

# Uses of AI in medical imaging

## Use AI for medical imaging

### to make healthcare easier or more efficient

1. Acquisition ($S \to X$)

   :::{.fragment}
   ::::{.nonincremental}
   - k-space to MRI image
   - raw projection data to CT image
   ::::
   :::

2. detection / segmentation ($X \to X$)

   :::{.fragment}
   ::::{.nonincremental}
   - segmenting organs at risk in radiotherapy
   ::::
   :::

3. inference / diagnosis ($X \to D$, both at prediction time)

   :::{.fragment}
   ::::{.nonincremental}
   - medical diagnosis
   - psuedo CT from MRI
   ::::
   :::

## Use AI for medical imaging

### to make healthcare better (improve decisions)

4. prognosis ($X \to Y$, $Y$ in the future)

   :::{.fragment}
   ::::{.nonincremental}
   - expected survival time given CT-scan
   ::::
   :::

5. treatment effect ($X$ determines effect of a treatment $T$ on outcome $Y$ in the future)

<!--
   :::{.fragment}
   ::::{.nonincremental}
   - effect of immunotherapy given T-cell distribution around tumor
   - heterogeneity of tumor on CT may predict response to radiotherapy
   - patient 
   ::::
   :::
-->

## Why would you estimate treatment effects based on images? {#ai-treatment-effect}

<!-- TODO: make examples figures instead of text -->

- treatments have different effects on patients based on their (disease) characteristics
- for example, whether tamoxifen increases survival for breast cancer patients depends on whether their tumor is hormone sensitive
- some characteristics may be well captured in medical imaging:
  - T-cell distributions around tumors related to effect of immunotherapy in cancer
   - heterogeneity of tumor on CT may predict response to radiotherapy
   - holistic view of 'body composition' on CT-scans determines whether patient can tolerate chemotherapy


## How to estimate treatment effects based on images?

<!-- TODO: have mulitple steps in point 4 to build it up -->

In principle the same as estimating a subgroup treatment effect (e.g. male vs female)

1. Conduct a randomized controlled trial where the treatments of interest are randomly allocated
2. Collect (imaging) data at randomization timepoint
3. Use a statistical learning technique like TARnet [@shalitEstimatingIndividualTreatment2017] to estimate outcomes conditional on image and treatment
4. conditional treatment effect $= f(X,T=1) - f(X,T=0)$

. . .

::: {.callout-tip icon=false}
## What if you cannot do a (big enough) RCT?

Emulate / approximate the ideal trial in observational data you do have, using **causal inference** techniques

**(which rely on untestable assumptions)**

:::

## Improving decisions with AI

::: {.nonincremental}
4. prognosis ($X \to Y$, $Y$ in the future)
5. treatment effect ($X$ determines effect of a treatment $T$ on outcome $Y$ in the future)
:::

- Whereas treatment effect estimation is typically thought of as a *causal* task requiring *causal* approaches (e.g. randomized controllerd trials)
- Prognosis models are often developed without any causal thinking (if it predicts it predicts)
- but then advertised for making treatment decisions.

## The in-between: using prediction models for (medical) decision making {auto-animate=true}

::: {.nonincremental}
- prognosis (e.g. survival given medical image)
:::

![](figs/8q8oy7_meme_crossover.jpg){fig-align="center"}

## Using prediction models for decision making is often thought of as a good idea

For example:

1. give chemotherapy to cancer patients with high predicted risk of recurrence
2. give statins to patients with a high risk of a heart attack

. . . 

::: {.callout-note icon=false}
## TRIPOD+AI on prediction models [@collinsTRIPODAIStatement2024]

“Their primary use is to support clinical decision making, such as ... **initiate treatment or lifestyle changes.**” 

:::

---

:::{.callout-warning}

## This may lead to bad situations when:

1. ignoring the treatments patients may have had during training / validation of (AI) prediction model
2. only considering measures of predictive accuracy as sufficient evidence for safe deployment


:::

# When accurate prediction models yield harmful self-fulfilling prophecies{#selffulfilling}

---

:::{.r-stack}

![](figs/rt_example1.png){.fragment height=24cm}

![](figs/rt_example2.png){.fragment height=24cm}

![](figs/rt_example3.png){.fragment height=24cm}

![](figs/rt_example4.png){.fragment height=24cm}

![](figs/rt_example5.png){.fragment height=24cm}

![](figs/rt_example6.png){.fragment height=24cm}

![](figs/rt_example.png){.fragment height=24cm}

:::


## Building models for decision support without regards for the historic treatment policy is a bad idea

:::{.r-stack}

![](figs/policy_changea1.png){.fragment width="100%"}

![](figs/policy_changea3.png){.fragment width="100%"}

![](figs/policy_changeax.png){.fragment width="100%"}

![](figs/policy_changeb2.png){.fragment width="100%"}

![](figs/policy_changebx.png){.fragment width="100%"}

:::

--- 

[The question is not "is my model accurate before / after deployment",]{.r-fit-text}

[but did deploying the model improve patient outcomes?]{.r-fit-text .fragment}


## Treatment-naive prediction models

:::{.r-stack}

![](figs/txnaive1.png)

![](figs/txnaive2b.png){.fragment}

:::

\begin{align}
    E[Y|X] \class{fragment}{= E[E_{t~\sim \pi_0(X)}[Y|X,t]]}
\end{align}

## Treatment-naive prediction models

[Results from @vanamsterdamWhenAccuratePrediction2024]

1. good or bad discrimination post deployment may be a sign of a harmful or a beneficial policy change
2. models that are perfectly calibrated before and after deployment are certainly not useful for decision making because they didn't change the distribution

---

[Is this obvious?]{.r-fit-text}

<!--::: {.callout-tip}-->

<!--It may seem obvious that you should not ignore historical treatments in your prediction models, if you want to improve treatment decisions, but many of these models are published daily, and some guidelines even allow for implementing these models based on predictve performance only-->

<!--:::-->

## Prediction modeling is very popular in medical research

![](figs/predmodelsoverview.png){fig-align='center'}

## Recommended validation practices and reporting guidelines do not protect against harm

### because they do not evaluate the policy change

:::{.columns}

::::{.column width="50%"}

![](figs/ajcc_title.png){fig-align="center"}

::::

::::{.column width="50%"}

![](figs/tripod_ai.png){fig-align="center"}

::::

:::



## Bigger data does not protect against harmful prediction models

![](figs/biggerdata.png){fig-align="center"}

## More flexible models do not protect against harmful prediction models

![](figs/morelayers.png){fig-align="center"}

<!-- TODO make mindthegap background image -->

---

::: {.r-stretch}

![](figs/mindthegap.png){fig-align="center"}

:::

## {auto-animate=true}

::: {style="margin-top: 200px; font-size: 3em; color: red;"}
What to do?
:::

## {auto-animate=true}

::: {style="margin-top: 100px"}
What to do?
:::

1.  Evaluate policy change (cluster randomized controlled trial)
2.  Build models that are likely to have value for decision making

# Building and validating models for decision support

## Deploying a model is an intervention that changes the way treatment decisions are made

![](figs/policy_changebx.png){fig-align="center"}

## How do we learn about the effect of an intervention?

With a randomized experiment

<!--With causal inference!-->

- for using a decision support model, the unit of intervention is usually *the doctor*
- randomly assign *doctors* to have access to the model or not
- measure differences in **treatment decisions** and **patient outcomes**
- this called a cluster RCT
- if using model improves outcomes, use that one

. . . 

:::{.callout-tip icon="false"}

## Using cluster RCTs to evaluated models for decision making is not a new idea [@cooperEvaluationMachinelearningMethods1997]

“As one possibility, suppose that a trial is performed in which clinicians are randomized either to have or not to have access to such a decision aid in making decisions about where to treat patients who present with pneumonia.” 

:::

. . .

:::{.callout-warning}
## What we don't learn
was the model predicting anything sensible?
:::

## So build treatment-naive prediction models and trial them for decision support?

Not a good idea

- baking a cake without a recipe
- hoping it turns into something nice
- not pleasant to people that need to taste result of the experiment
  - (i.e. patients may have side-effects / die)

## We should build models that are likely to be valuable for decision making

- Build models that predict expected outcomes under *hypothetical interventions* (*prediction-under-intervention models*)
- doctor / patient can pick the treatment with *best* expected outcomes, depending on patient's values and preferences
- whereas *treatment-naive* prediction models average out over the historic treatment policy, prediction-under-intervention allows the user to select a treatment option

. . . 

::: {.callout-tip icon=false}
## Hilden and Habbema on prognosis [@hildenPrognosisMedicineAnalysis1987]
"Prognosis cannot be divorced from contemplated medical action, nor from action to be taken by the patient in response to prognostication.” 
:::

- prediction-under-intervention is not a new idea, but language and methods on causality have come a long way since [@hildenPrognosisMedicineAnalysis1987].

## Estimand for prediction-under-intervention models

What is the estimand?

- prediction: $E[Y|X]$
- average treatment effect: $E[Y|\text{do}(T=1)] - E[Y|\text{do}(T=0)]$
- conditional average treatment effect: $E[Y|\text{do}(T=1),X] - E[Y|\text{do}(T=0),X]$
- prediction-under-intervention: $E[Y|\text{do}(T=t),X]$

--- 

:::{.columns}

::::{.column width="50%"}

using *treatment naive* prediction models for decision support

![](figs/8q8oy7_meme_crossover.jpg){fig-align="center"}

::::

::::{.column width="50%"}

prediction-under-intervention

![](figs/peanutbutter_chocolatesprinkles.jpg){.fragment fig-align="center"}

::::
:::

## More on prediction-under-intervention models

development:

- ideally estimated from RCTs, but these are often too small or don't measure the right data
- alternatively can use observational data and causal inference methods
- assumption of no unobserved confounding often hard to justify in observational data
- but there's more between heaven (RCT) and earth (confounder adjustment)
  - proxy-variable methods [e.g. @miaoIdentifyingCausalEffects2018;@vanamsterdamIndividualTreatmentEffect2022]
  - constant relative treatment effect assumption [e.g. @alaaMachineLearningGuide2021; @vanamsterdamConditionalAverageTreatment2023; @candidodosreisUpdatedPREDICTBreast2017]
  - diff-in-diff
  - instrumental variable analysis [@waldFittingStraightLines1940; @puliGeneralControlFunctions2021;@hartfordDeepIVFlexible2017]
  - front-door analysis
- many of these have potential new applications with AI and medical imaging

## Evaluation of prediction-under-intervention models

- prediction accuracy can be tested in RCTs, or in observational data with specialized methods accounting for confounding [e.g. @keoghPredictionInterventionsEvaluation2024]
- in confounded observational data, typical metrics (e.g. AUC or calibration) are not sufficient as we want to predict well in data from *other distribution than observed data* (i.e. other treatment decisions)
- a new *policy* can be evaluated in historic RCTs [e.g. @karmaliBloodPressureloweringTreatment2018]
- ultimate test is cluster RCT
- if not perfect, likely a better recipe than *treatment-naive* models

## Take-aways

- deploying models for decision support is an intervention and should be evaluated as such
- when developing or evaluating (AI) prediction models for medical decisions, think about
  - what is the effect of using this model on medical decisions?
  - what is the effect of this policy change on patient outcomes?
- prediction-under-intervention models have a foreseeable effect on patient oucomes when used for decision making

. . .

:::: {.columns}
::: {.column width="50%"}
![](figs/qr_comment.png){height=6cm fig-align='center'}

From algorithms to action: improving patient care requires causality [@vanamsterdamAlgorithmsActionImproving2024]
:::

::: {.column width="50%"}
![](figs/qr_selffulfilling.png){height=6cm fig-align='center'}

When accurate prediction models yield harmful sel-fulfilling prophecies [@vanamsterdamWhenAccuratePrediction2024]
:::
::::

## References

