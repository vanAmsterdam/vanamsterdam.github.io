---
title: AI in Health Care
subtitle: BMS-Aned seminar
date: 2024-09-26
bibliography: ../../library.bib
format:
    revealjs:
        incremental: true
#        css: ../../colors.css
        theme: ../custom.scss
        auto-stretch: true
        center: true
        fig-align: center
        width: 1600
        height: 900
        pdf-separate-fragments: true
        embed-resources: false
---

## Disclaimer

- of course, I used AI to help with the slides
  - generate tikz diagrams
  - generate vector illustrations in adobe illustrator
  - write text and equations with github copilot

# What is AI?

## Definition

:::{.callout-tip}

## What is AI?

Artificial Intelligence is the branch of computer science that focuses on creating systems capable of performing tasks that typically require human intelligence. [@russellArtificialIntelligenceModern2020]

:::

- These tasks include: learning, reasoning, problem-solving, perception, natural language understanding, and decision-making.
- AI systems can be designed to operate autonomously, adapt to new inputs, and improve their performance over time.

## Early Milestones
- **1940s**: Concept of AI emerged with Alan Turing's work on computation and intelligence.
- **1956**: The term "Artificial Intelligence" was coined at the Dartmouth Conference by John McCarthy.
- **1960s-1970s**: Early AI programs focused on solving algebra, proving theorems, and playing games (e.g., Chess).

## Key Developments
- **1980s**: Introduction of expert systems (rule-based systems for decision making).
- **1990s**: Machine learning began gaining traction, allowing AI systems to improve with experience.
- **2010s**: Deep learning and neural networks revolutionized AI, enabling breakthroughs in areas like image recognition, natural language processing, and more.

## AI Landscape: what is AI?

---

:::{.r-stack}

![](figs/ai-landscape-1-bare.png){.fragment .fade-in-then-out}

![](figs/ai-landscape-2-rules.png){.fragment .fade-in-then-out}
:::

## Rule-based systems are AI

-   rule: all cows are animals
-   observation: this is a cow $\to$ it is an animal
-   applications:
    - medication interaction checkers
    - bedside patient monitors

---

:::{.r-stack}

![](figs/ai-landscape-2-rules.png){.fragment .fade-in-then-out}

![](figs/ai-landscape-3-ml.png){.fragment .fade-in-then-out}

:::

## What is Machine Learning?

::: {.center .large}
Machine Learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to perform tasks without explicit instructions. Instead, they rely on patterns and inference from data. [@samuelStudiesMachineLearning1959]
:::

## Assume we have this data

::: {.columns}
:::: {.column width="50%"}
![](figs/gen_scatter.png){width="100%" fig-align="center"}
::::
:::: {.column width="50%"}

|i |length|weight|sex|
|-:|-----:|-----:|--:|
|1|137|30|boy|
|2|122|24|girl|
|3|101|18|girl|
|...|...|...|...|

::: {.fragment}
We typically assume these data are (i.i.d.) samples from some unknown distribution $p(l,w,s)$:

$$l_i,w_i,s_i \sim p(l,w,s)$$
:::

::::
:::

## ML tasks: generation
::: {.columns}
:::: {.column width="50%"}
![](figs/gen_full.png){width="100%" fig-align="center"}
::::
:::: {.column width="50%"}
1. formulate a model for *joint* distribution $p_{\theta}$
2. use samples to optimize $\theta$
3. generate new samples

[$$l_j,w_j,s_j \sim p_{\theta}(l,w,s)$$]{.fragment}

:::{.fragment}
|task| |
|---:|:---|
|generation|$l_j,w_j,s_j \sim p_{\theta}(l,w,s)$|
:::

::::
:::

## ML tasks: conditional generation
::: {.columns}
:::: {.column width="50%"}
![](figs/gen_boy.png){width="100%" fig-align="center"}
::::
:::: {.column width="50%"}
use samples to learn model for *conditional* distribution $p$
$$
  l_j,w_j \sim p_{\theta}(l,w|s=\text{boy})
$$

:::{.fragment}
|task| |
|---:|:---|
|generation|$l_j,w_j,s_j \sim p_{\theta}(l,w,s)$|
|conditional generation|$l_j,w_j \sim p_{\theta}(l,w|s=\text{boy})$|
:::

::::
:::

## ML tasks: conditional generation 2

::: {.columns}
:::: {.column width="50%"}
![](figs/gen_scatter.png){width="100%" fig-align="center"}
::::
:::: {.column width="50%"}
use samples to learn model for *conditional* distribution $p$
of one variable
$$
s_j \sim p_{\theta}(s|l=l',w=w')
$$

|task| |
|---:|:---|
|generation|$l_j,w_j,s_j \sim p_{\theta}(l,w,s)$|
|conditional generation|$l_j,w_j \sim p_{\theta}(l,w|s=\text{boy})$|
::::
:::

## ML tasks: discrimination
::: {.columns}
:::: {.column width="50%"}
![](figs/class_logistic.png){width="100%" fig-align="center"}
::::
:::: {.column width="50%"}
call this one variable *outcome* and *classify* when expected value[^1] passes threshold (e.g. 0.5)
$$
s_j = p_{\theta}(s|l=l',w=w') > 0.5
$$

|task| |
|---:|:---|
|generation|$l_j,w_j,s_j \sim p_{\theta}(l,w,s)$|
|conditional generation|$l_j,w_j \sim p_{\theta}(l,w|s=\text{boy})$|
|discrimination|$p_{\theta}(s|l=l_i,w=w_i) > 0.5$|
::::
:::
[^1]: either calculated directly or estimated by simulation

## ML tasks: reinforcement learning

- e.g. computers playing games
- typically requires many experiments, maybe not too useful in health care

![](figs/reinforcement_learning.png){width="60%", fig-align="center"}

---

:::{.r-stack}

![](figs/ai-landscape-2-rules.png){.fragment .fade-in-then-out}

![](figs/ai-landscape-3-ml.png){.fragment .fade-in-then-out}

![](figs/ai-landscape-4-rf-svm.png){.fragment .fade-in-then-out}

![](figs/ai-landscape-5-dl.png){.fragment .fade-in-then-out}

![](figs/ai-landscape-6-llm.png){.fragment .fade-in}

:::

## Neural Networks and Deep Learning

### From Linear Regression ...

:::{layout-ncol=2}

![](./tikzs/linear-regression.png)

::::{.column}

$$y = \sum_{i=0}^5 x_i \beta_i$$

- optimize $\beta_i$ to minimize mean squared error, e.g. using second-order methods

::::

:::

## Neural Networks and Deep Learning

### ... to 'Deep' Learning

:::{layout-ncol=2}

![](./tikzs/neural-network.png){#fig-mlp}

::::{.column}

\begin{align}
    h_i &= w_{0i} + w_{1i} x_1 + \ldots \\
    h_i &= g(h_i) \\
      y &= \sum_{i=1}^3 h_i w_i
\end{align}

- sticked $h_i$ between input and output
- $g$ is a non-linear function: each $h_i$ is a non-linear transformation of the input
- renamed $\beta_i$ ('coefficients') to $w_{0i}$ and $w_i$ ('weights')

::::

:::

## Why would neural networks work?

- universal approximation theorem: a neural network with one hidden layer of sufficient width can approximate any continuous function
- problems:
  - no longer convex optimization
  - typical optimizers scale quadratically or worse with number of parameters
- deep learning: initialize parameters randomly, use gradient descent (first-order) methods
- big networks require much memory and computation: do parallel on graphics processing units (GPUs) with mini-batches of data (i.e. stochastic gradient descent)
- new optimizers keep coming up (e.g. Adam)

## There are specialized neural network architectures for different types of data

- Convolutional Neural Networks (CNNs): images
- Recurrent Neural Networks (RNNs): time series
- Transformers: natural language processing

## Convolutional Neural Networks

![](tikzs/cnn-pooling.png){width="100%" fig-align="center"}

## Convolutional Neural Networks

![LeNet-5 [@lecunGradientbasedLearningApplied1998]](figs/lenet-5.png){width="100%" fig-align="center"}

## Why convolutional neural networks for images?

- images exhibit much *local* structure, e.g. edges and textures
- this local structure is hierarchical: edges form shapes, shapes form objects
- convolutional neural networks: learn local structure, then combine these to learn higher-level structure
- local structure is location invariant: exploited by convolution operation

## Training (convolutional) neural networks

- regularization:
  - L1, L2: add penalty to weights
  - early stopping: stop training when validation error starts increasing

## Early stopping

![](tikzs/loss-curve.png){width="100%" fig-align="center"}


## Training (convolutional) neural networks

- regularization:
  - L1, L2: add penalty to weights
  - early stopping: stop training when validation error starts increasing
  - random initialization: initialize weights randomly
  - dropout: randomly set some weights to zero

:::{.callout-note .fragment}

## Parameter counting is a bad proxy for model complexity in neural networks

Whereas in regression models, model complexity is well-captured by the number of parameters, this is not the case for neural networks. 

:::

## For complex tasks, neural networks keep getting better with:

  [- more compute resources]{.fragment fragment-index=1}

  [- bigger data]{.fragment fragment-index=2}

  [- bigger models (enabled by data and compute)]{.fragment fragment-index=3}

:::{.r-stack}

![[@kaplanScalingLawsNeural2020]](figs/kaplan2020_scaling1.png){.fragment fragment-index=1 .fade-in-then-out width=100%}

![[@kaplanScalingLawsNeural2020]](figs/kaplan2020_scaling2.png){.fragment fragment-index=2 .fade-in-then-out width=100%}

![[@kaplanScalingLawsNeural2020]](figs/kaplan2020_scaling3.png){.fragment fragment-index=3 .fade-in-then-out width=100%}

:::

## scaling over time

![](figs/modelsize_over_time.png){width=100%}

## Neural Network Architectures

- common theme:
 - use an architecture that fits the data type well 
   - images with local structure: CNNs
   - time series: RNNs
 - scale! 



## Scaling of neural networks


## Neural Network Architectures

- Multilayer Perceptrons (MLPs): general purpose (as @fig-mlp))

- hierarchical structure (e.g. images):
    - Convolutional Neural Networks (CNNs): image recognition
- sequence modeling (e.g. time series, natural language):
    - Recurrent Neural Networks (RNNs): time series
    - Transformers: natural language processing

---

:::{.r-stack}

![](figs/ai-landscape-5-dl.png){.fragment .fade-in-then-out}

![](figs/ai-landscape-6-llm.png){.fragment .fade-in}

:::


## AI versus stats

- scaling
- emerging
- continual learning

# What is Deep Learning?

# What is an LLM?

# Pitfalls

## Biases

## Can we lose control?

## Accuracy does not imply value for decision making

## References



