---
title: "Aligning development, deployment and monitoring for AI: a causal perspective"
subtitle: "MLHC pre-conference workshop 2025"
date: 2025/08/14
bibliography: ../../library.bib
format:
    revealjs:
        incremental: true
        theme: [../custom.scss]
        center: true
        fig-align: center
        width: 1600
        height: 900
categories:
  - prediction
  - causal inference
---

# Today

## Welcome to the workshop!

### Organizers - Workshop Chairs MLHC 2025

:::{layout-ncol=2}

::::{.column}
![Uri Shalit](figs/uri.png){width=4in}

Visiting Researcher, Google DeepMind, London

Associate Professor, Tel Aviv University
::::

::::{.column}
![Wouter van Amsterdam](figs/wouter.png){width=4in}

Assistant Professor, University Medical Center Utrecht, Netherlands

::::

:::

## Today's workshop

9-12am: focus talks, 30 minutes each

:::{.nonincremental}
- Wouter van Amsterdam
- David Kent
- Mohammad Mamdani
- Jean Feng - Addressing performance drift: Over time and space
:::

12-12.30: Plenary Discussion

13:30: group work:work on questions, draft manuscripts

16:00-16:45: groups present ideas 

16:45-17:00: wrap-up

# much of AI is 'predict predict predict'

---

predict sepsis

---

predict diagnosis of structural heart disease from ECG

---

predict next token in a sequence ...

---

these models take input data, model its statistical properties (e.g. what to expect for outcome $Y$ given observation $X$)

## evaluated on predictive *performance* (e.g. accuracy, AUC, calibration)

:::{layout="[[1,1],[4,4]]"}

![](figs/tripod_ai_short.png){height=2in}

![](figs/probast_ai_short.png){height=2in}

![](figs/auc1.png){height=4in}

![https://doi.org/10.1371/journal.pmed.1004566.g004](figs/pmed.1004566.g004.png){height=4in}

:::

---

:::{.r-stack}

![](figs/hc1.png){.fragment}

![](figs/hc2.png){.fragment}

:::


## AI models tend to break under data shifts

[@finlaysonClinicianDatasetShift2021]

# deploying AI in healthcare is an intervention

![[@joshiAIInterventionImproving2025]](figs/aiasintervention-paper.png)

---

the entire purpose of an AI is to *break* (improve) a system, meaning to **cause** a data shift

see a potential issue here?

## Medical Interventions based on (AI) prediction models

- prevention: reduce risk of heart attacks with cholesterol lowering medication, based on predicted risk of a heart attack [@hippisley-coxDevelopmentValidationNew2024]
- give chemotherapy to breast cancer patients with a high risk of recurrence [@alaaMachineLearningGuide2021]
- triage / early warning: patients at high risk of sepsis [@henryTargetedRealtimeEarly2015]
- early detection: structural heart disease from ECGs performed in primary care
<!-- todo: add ref -->

# What can go wrong when AI development and deployment are misaligned?

# Where causal inference may help

## What is causal inference?

- 'classical' prediction / statistics / AI: model **associations**; know what to expect for outcome $Y$ given observation $X$, **when we keep our hands behind our back and do not change the system**
- causal inference: answers questions **what if ...** - we changed the system (i.e. intervened by giving a certain treatment $T$) - what would be the outcome $Y$?
- association: representative (observational) data
- causal inference: experimental data (randomized controlled trials), when not available, approximate with observational data using assumptions (unconfoundedness, positivity, consistency) 

## Also causal inference: predicting outcomes under hypothetical interventions

- expected outcome $Y$ if we give treatment $T$ to patient with features $X$

. . . 

::: {.callout-tip icon=false}
## Hilden and Habbema on prognosis [@hildenPrognosisMedicineAnalysis1987]

"Prognosis cannot be divorced from contemplated medical action, nor from action to be taken by the patient in response to prognostication.” 

:::

## What can go wrong if predictions have unclear relationship with treatment decisions?

1. treatment-naive: simply ignore treatment in the predictions
2. include treatment but there are important confounders that are not included in the model

## Treatment-naive prediction models: predict outcomes based on features, ignoring treatments

![](figs/txnaive1.png)

## Treatment-naive prediction models: predict outcomes based on features, ignoring treatments

### ... that are important to the outcomes

![](figs/txnaive2.png)

## When accurate prediction models yield harmful self-fulfilling prophecies

How this can go wrong if we misalign the AI evaluation metric and the patient oucome [@vanamsterdamWhenAccuratePrediction2025]

---

:::{.r-stack}

![](figs/rt_example1.png){.fragment height=24cm}

![](figs/rt_example2.png){.fragment height=24cm}

![](figs/rt_example3.png){.fragment height=24cm}

![](figs/rt_example4.png){.fragment height=24cm}

![](figs/rt_example5.png){.fragment height=24cm}

![](figs/rt_example6.png){.fragment height=24cm}

![](figs/rt_example.png){.fragment height=24cm}

:::

# Big hurdles in AI deployment from an intervention / policy perspective


1. prediction model outputs do not automatically align with value for intervention decision making (except when clearly defined treatment strategy (i.e. 'prediction under intervention'))
2. evaluting the effects of deploying a model **is** causal inference
3. monitoring under shifts in treatment policy is hard

## How are causal models special

1. clear alignment between prediciton accuracy and value for treatment policy:

. . . 

:::{.callout-tip icon=false}

## Pick the treatment that maximizes the expected outcome

:::

2. stable predictions under shifts in treatment policy, conditional on the model's features [@fengMonitoringMachineLearningbased2024]

## What to measure

### For 'non-causal' prediction models that don't factor in treatment decisions but predict post-treatment outcomes

|               |                      | pre-deploy | deployment study | monitoring |
|---------------|----------------------|------------|------------------|------------|
|               | **metric**           |            |                  |            |
| model         | discrimination (AUC) |            |       🔁           |     🔁       |
|               | calibration          |            |   🔻          |  🔻      |
| health system | treatments / interventions    |            |       🔁           |      🔁      |
|               | patient outcomes     |            |       🔁           |      🔁      |

**Legend**  
🔁 changes ✅ stable 🔻 worsens


## What to measure

### Prediction under intervention preserves calibration under shifts in policy conditional on the model's features

|               |                      | pre-deploy | deployment study | monitoring |
|---------------|----------------------|------------|------------------|------------|
|               | **metric**               |            |                  |            |
| model         | discrimination (AUC) |            |       🔁           |     🔁       |
|               | calibration          |            |   🔻      ✅    |  🔻    ✅  |
| health system | interventions        |            |       🔁           |      🔁      |
|               | patient outcomes     |            |       🔁           |      🔁      |

**Legend**  
🔁 changes ✅ stable 🔻 worsens


# Setup

## Healthcare as a system

- takes inputs (clinical data)
- makes decisions ('treatment')
- produces outcomes (health status, costs)

:::{.callout-tip}

## Fundamental hypothesis:

system is suboptimal in its use of prior data
can improve decision process by using data

:::

## Prediction models used for decision making: applications


## The elefant in the room

- issue: deploying a prediction model is an intervention that changes treatment decisions and outcomes [@joshiAIInterventionImproving2025]; 
- this was the very purpose of deploying the model
- standard prediction model evaluation does not properly account for this

## Prediction model evaluation

- reporting: TRIPOD+AI [@collinsTRIPOD+AIStatementUpdated2024]
  - measures: discrimination, calibration, clinical utility (net benefit)

## Two (three) views on performance / utility

- 0: prediction model accuracy on historic data [@vanamsterdamWhenAccuratePrediction2025]
- 1a: accuracy of predicting under intervention
- 1b: accuracy of estimating CATE 
- 2: accuracy of estimating counterfactual outcomes (c-for benefit?)
- 3: policy utility
- 4: post-deployment monitoring

## When all stars align

- point treatment
- prediction under intervention / CATE estimation
- evaluation in unconfounded data (e.g. historical RCTs)
- change in policy restriced to subset of features $X$
- then:
  - 'treat when tau > t' improves utility
  - calibration stable under shift in policy (monitoring)

# current status

## not good enough
