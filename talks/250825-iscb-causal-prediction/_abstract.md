# From prediction to treatment decision: aligning development, evaluation and monitoring

From sepsis prediction to heart-attack risk and cancer prognosis, the medical literature is full of models predicting future patient health. Many of these models are motivated by the goal of supporting clinical decisions, yet few are designed or evaluated with this goal in mind.

Prediction models are typically assessed based on their predictive accuracy, but strong predictive performance does not automatically translate into better clinical decision-making. To truly optimize patient outcomes, we must align model development, evaluation, and monitoring with the goal of informing treatment decisions.

In this talk, I will outline a framework for this alignment, discussing key pitfalls in traditional predictive model evaluation and how to assess real-world decision impact. I will also address the challenge of monitoring predictive models in clinical practice—an area where EMA and FDA regulations demand action but offer little guidance. Standard metrics like discrimination and calibration can be misleading when applied to deployed models, particularly when not recognizing the effect of the deployed model on treatment decisions and patient outcomes.

Finally, I will discuss prediction-under-intervention models—models designed to estimate patient outcomes under different treatment scenarios. These models directly connect predictions to treatment decisions, making evaluation and monitoring conceptually straightforward. Though conceptually appealing, these models come with their own challenges—both methodological and practical.

Mapping out approaches to predictive model evaluation and monitoring—and addressing the lack of clear guidance—will be essential to ensuring that predictive models truly support clinical decision-making.
