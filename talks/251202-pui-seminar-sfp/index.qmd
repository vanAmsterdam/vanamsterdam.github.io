---
title: "When accurate prediction models yield harmful self-fulfilling prophecies"
subtitle: "Seminar on Prediction Under Intervention(s), Leiden"
date: 2025/12/02
bibliography: ../../library.bib
format:
    revealjs:
        incremental: true
        theme: [../custom.scss]
        center: true
        fig-align: center
        width: 1600
        height: 900
        toc: false
categories:
  - prediction
  - causal inference
  - invited
---

## Model building and evaluation versus healthcare impact

- many prediction models exist that predict risk of a certain outcome $Y$ given features $X$
- e.g. predict 10-year risk of a heart attack, given age, cholesterol, sex
- evaluated on *predictive performance*: calibration and discrimination (AUC)
- then used for decision support, e.g.: give statins if risk > 10%
- the hope is: better predictive performance $\implies$ better impact

#

[what could possibly go wrong?]{.r-fit-text}

# When accurate prediction models yield harmful self-fulfilling prophecies [@vanamsterdamWhenAccuratePrediction2025]

van Amsterdam, van Geloven, Krijthe, Ranganath, Cina

---

:::{.r-stack}

![](figs/rt_example1.png){.fragment height=22cm}

![](figs/rt_example2.png){.fragment height=22cm}

![](figs/rt_example3.png){.fragment height=22cm}

![](figs/rt_example4.png){.fragment height=22cm}

![](figs/rt_example5.png){.fragment height=22cm}

![](figs/rt_example6.png){.fragment height=22cm}

:::

## What happened here?

- had a 'good' model, got a bad policy
- model predicted outcome (survival) *under historic treatment policy* (always radiation)
- did not predict what outcomes would be under *alternative policy* (no radiation)
- in this case, unmodeled *treatment effect heterogeneity* (aka treatment effect modification, interation, differing conditional average treatment effects)

## Regulation to the rescue: we need to monitor (AI) models

:::{.r-stack}

![](figs/ema-monitor.png){.fragment height=700px}

![](figs/fda-gp-monitor.png){.fragment}

:::

## Let's monitor the model performance over time 

:::{.r-stack}

![](figs/rt_example6.png){height=22cm}

![](figs/rt_example.png){.fragment height=22cm}

:::

## What happened in monitoring?

- the model re-inforced its own predictions (self-fulfilling prophecy)
- took a measure of predictive performance (AUC)
- mistook it for a measure of (good) impact
- many potential examples (e.g. ICU stop treatment [@balcarcelFeedbackLoopsIntensive2025], others [@sciencemediacenterExpertReactionStudy])

---

:::{.r-stretch}

![](figs/mindthegap.png)

:::

## Our paper

We formalized the simplest general case

- before: everyone treated (or untreated)
- new binary feature $X$
- outcome prediction model: $\mu(x) = P(Y=1|X=x)$
- *new policy* based on OPM: treat when $\mu(x) > \lambda$ (assume non-constant policy)
- for a grid of values of:
  - treatment effect for $X=0$
  - treatment effect interaction (different effect when $X=1$)
  - prevalence of $X$
  - historic policy and interpretation of $Y$
- exected values of outcomes
- pre- and post-deployment AUC


## Harmful self-fulfilling prophecies occur in without extreme treatment (interaction) effects

![](figs/figure1.png)

## Deterministic evaluation of usefulness of policy

![](figs/table1.png)

## Calibration result

![](figs/definition-calibration.png){width=100%}

![](figs/thm-calibration.png){.fragment width=100%}

# Reception

## Editorial

![](figs/patterns-editorial.png)

---

![](figs/ldh-feedbackloops.png)

## Press coverage - urgent mail

![](figs/storm.png)

## Press coverage

![](figs/theindependent.png)

## Press coverage

![](figs/pharmaphorum.png)

## Science Media Center Roundup (7 experts)

- **Withholding lifesaving treatments:** When AI predicts low survival for certain patients, clinicians may deny treatment, causing worse outcomes that falsely validate the model.

- **Rehabilitation triage bias:** AI tools predicting poor recovery after surgery can lead hospitals to allocate fewer rehab resources to those patients, directly causing the poor outcomes the model anticipated.

- **Post-deployment performance paradox:** If real-world care improves outcomes for certain patients, models trained on historical data may appear to “fail,” encouraging withdrawal of beneficial changes and reinforcing the old, harmful patterns.

- **Perpetuating historical under-treatment:** Models trained on biased historical data may predict poor outcomes for groups who were previously under-treated, and clinicians acting on these predictions can continue the cycle, worsening outcomes and deepening disparities.

- **Generalisation beyond healthcare:** Predictive models used in policing can label historically over-policed demographics as “high risk,” triggering intensified surveillance that produces the very outcomes used to justify the predictions.

## Current status

- reporting guidelines (e.g. TRIPOD+AI / PROBAST+AI [@collinsTRIPOD+AIStatementUpdated2024; @moonsPROBAST+AIUpdatedQuality2025]) do not provide enough evidence for safe deployment [@PrognosticModelsDecision2024]
- some acceptance criteria lists (AJCC) even allow for harmful self-fulfilling prophecies [@kattanAmericanJointCommittee2016]
- EMA and FDA are developing monitoring guidelines, mostly emphasis on **predictive performance**, but **good performance** $\neq$ **postive impact**

## Takeaways

- when predicting prognosis, need well defined relation between prediction and potential treatment decisions (e.g. prediction under intervention)
- evaluate and monitor prediction models based on what we care about: impact on healthcare
- guidelines require updates

## References
